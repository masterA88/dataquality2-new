{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script for data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRICARE:\n",
    "\n",
    "BRICARE consists of 2 different types of files by year:\n",
    "\n",
    "a. File after 2022 (2023-2024) = 79 kolom\n",
    "\n",
    "\n",
    "b. File before 2022 (2019-2022) = 27 kolom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type A\n",
    "\n",
    "\n",
    "Data Extraction for File Type A must be 2 Files:\n",
    "\n",
    "\n",
    "A.1 Columns (without \"Details\")\n",
    "\n",
    "\n",
    "A.2 Details only \n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['Create_Date','TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\"\n",
    "\n",
    "- Columns shoud be mapped based on their Call_Type_ID:\n",
    "\n",
    "['Produk','Jenis_Produk','Jenis_Laporan']\n",
    "\n",
    "- PLEASE ADD CIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_uat20230101_20230101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('NULL', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('None', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_file(file_path):\n",
    "\n",
    "    data = []\n",
    "    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "\n",
    "            date_index = next(i for i, part in enumerate(parts) if date_pattern.match(part))\n",
    "\n",
    "            ticket_id = parts[0] \n",
    "            call_type_id = parts[1]  \n",
    "            description = ';'.join(parts[2:date_index])  \n",
    "            create_date = parts[date_index]  \n",
    "\n",
    "      \n",
    "            data.append([ticket_id, call_type_id, description, create_date] + parts[date_index + 1:])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.txt\"\n",
    "\n",
    "df = parse_file(file_path)\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "   \n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_uat{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bricare_20230101_20230101.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if len(parts) > 78:\n",
    "                description = ';'.join(parts[2:-75])\n",
    "                new_parts = parts[:2] + [description] + parts[-75:]\n",
    "                data.append(new_parts)\n",
    "            else:\n",
    "                data.append(parts)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    df = df.replace(['0', 0], '')\n",
    "\n",
    "    columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "    for column in columns_to_convert:\n",
    "        df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "    \n",
    "    df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "    return df\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.csv\"\n",
    "df = parse_csv(file_path)\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleasing the master Call Type file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_24036\\2288781540.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "master_df_path = r\"C:\\Users\\maste\\Downloads\\bricare\\(REVISED) SLA-OLA_NewUserGrouping_Ringkasan Kirim ME Versi 1.6.csv\"\n",
    "df = pd.read_csv(master_df_path, sep=';')\n",
    "\n",
    "\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "df = df.dropna(how='all')\n",
    "df.iloc[:450]\n",
    "df.to_csv(\"master_calltype.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Type di tes MMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Data Mapping - Case_Type.csv\"\n",
    "df=pd.read_csv(path)\n",
    "# case_types = ['8634', '8635', '8636', '8623', '8624', '8628', '8629', '8630', '8412', '8625', '8615', '8616', '8617', '8618', '8619', '8638', '8620']\n",
    "\n",
    "case_types = ['8412']\n",
    "# Filter the dataframe\n",
    "df = df[df['Case Types'].isin(case_types)]\n",
    "df['Segment'] = df['Segment'].replace('All', '')\n",
    "df\n",
    "df.to_csv(r'C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Case_Type_MMS2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call type final add external ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "1     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "2     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "3     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "4                                                                 bad value for restricted picklist field: Giro\n",
       "5                                                Customer Segment: bad value for restricted picklist field: All\n",
       "6                                                Customer Segment: bad value for restricted picklist field: All\n",
       "7                                                         Product: bad value for restricted picklist field: SBN\n",
       "8                                                        Product: bad value for restricted picklist field: DPLK\n",
       "9                                                Customer Segment: bad value for restricted picklist field: All\n",
       "10                                               Customer Segment: bad value for restricted picklist field: All\n",
       "11                                               Customer Segment: bad value for restricted picklist field: All\n",
       "12                                               Customer Segment: bad value for restricted picklist field: All\n",
       "13    Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "14                                               Customer Segment: bad value for restricted picklist field: All\n",
       "15                                               Customer Segment: bad value for restricted picklist field: All\n",
       "16             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "17                                               Customer Segment: bad value for restricted picklist field: All\n",
       "18                                               Customer Segment: bad value for restricted picklist field: All\n",
       "19                                               Customer Segment: bad value for restricted picklist field: All\n",
       "20                                               Customer Segment: bad value for restricted picklist field: All\n",
       "21                                               Customer Segment: bad value for restricted picklist field: All\n",
       "22                                               Customer Segment: bad value for restricted picklist field: All\n",
       "23                                               Customer Segment: bad value for restricted picklist field: All\n",
       "24                                                bad value for restricted picklist field: Transaction Solution\n",
       "25                                                bad value for restricted picklist field: Transaction Solution\n",
       "26                                               Customer Segment: bad value for restricted picklist field: All\n",
       "27                                               Customer Segment: bad value for restricted picklist field: All\n",
       "28                                               Customer Segment: bad value for restricted picklist field: All\n",
       "29                                               Customer Segment: bad value for restricted picklist field: All\n",
       "30                                               Customer Segment: bad value for restricted picklist field: All\n",
       "31                                               Customer Segment: bad value for restricted picklist field: All\n",
       "32                                               Customer Segment: bad value for restricted picklist field: All\n",
       "33                                               Customer Segment: bad value for restricted picklist field: All\n",
       "34                                               Customer Segment: bad value for restricted picklist field: All\n",
       "35                                               Customer Segment: bad value for restricted picklist field: All\n",
       "36                                               Customer Segment: bad value for restricted picklist field: All\n",
       "37                                               Customer Segment: bad value for restricted picklist field: All\n",
       "38                                               Customer Segment: bad value for restricted picklist field: All\n",
       "39                                               Customer Segment: bad value for restricted picklist field: All\n",
       "40                                               Customer Segment: bad value for restricted picklist field: All\n",
       "41    Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "42             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "43                                                bad value for restricted picklist field: Transaction Solution\n",
       "44                                                bad value for restricted picklist field: Transaction Solution\n",
       "45                                               Customer Segment: bad value for restricted picklist field: All\n",
       "46                                               Customer Segment: bad value for restricted picklist field: All\n",
       "47                                               Customer Segment: bad value for restricted picklist field: All\n",
       "48                                               Customer Segment: bad value for restricted picklist field: All\n",
       "49                                                        Product: bad value for restricted picklist field: KTA\n",
       "50                                               Customer Segment: bad value for restricted picklist field: All\n",
       "51                                               Customer Segment: bad value for restricted picklist field: All\n",
       "52                                          Customer Segment: bad value for restricted picklist field: Individu\n",
       "53                                          Customer Segment: bad value for restricted picklist field: Individu\n",
       "54             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "55                                               Customer Segment: bad value for restricted picklist field: All\n",
       "56                                               Customer Segment: bad value for restricted picklist field: All\n",
       "57                                                        Product: bad value for restricted picklist field: KTA\n",
       "58                                                                bad value for restricted picklist field: Giro\n",
       "59                                                                bad value for restricted picklist field: Giro\n",
       "60                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "61                                                                bad value for restricted picklist field: Giro\n",
       "62                                                       Product: bad value for restricted picklist field: DPLK\n",
       "63                                                       Product: bad value for restricted picklist field: DPLK\n",
       "64                                                       Product: bad value for restricted picklist field: DPLK\n",
       "65                                                       Product: bad value for restricted picklist field: DPLK\n",
       "66                                                       Product: bad value for restricted picklist field: DPLK\n",
       "67                                                       Product: bad value for restricted picklist field: DPLK\n",
       "68                               Customer Segment: bad value for restricted picklist field: Individu Umum, UMKM\n",
       "69                                               Customer Segment: bad value for restricted picklist field: All\n",
       "70                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "71                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "72                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "73                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "74             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "75             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "Name: ERROR, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"D:\\dataquality2\\new as per 5 June\\error060824083047752_SIT.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "df.loc[df['Case Types'].isin([8418, 8419]), 'PRODUCT'] = 'Transaction Banking'\n",
    "df.loc[df['Case Types'].isin([8708, 8711]), 'PRODUCT'] = 'Transaction Banking'\n",
    "df['CASE CATEGORY'] = df['CASE CATEGORY'].replace('Customer Feedback / Sentiment Monitoring', 'Customer Feedback')\n",
    "\n",
    "segment_mapping = {\n",
    "    'Individu': 'Individu Umum',\n",
    "    'Individu Umum, Individu Priority &amp; Private' : 'Individu Umum',\n",
    "    'All' : 'Individu Umum',\n",
    "    'Individu Umum, UMKM' : 'Individu Umum',\n",
    "    \n",
    "}\n",
    "\n",
    "product_mapping = {\n",
    "    'loans': 'Loans',\n",
    "    'savings': 'Savings',\n",
    "    'KTA': ''\n",
    "}\n",
    "\n",
    "subproduk_mapping = {\n",
    "    'Giro': 'Giro (CA)',\n",
    "    'KPR, KUR, KMK, KI': '',\n",
    "    'SBN': '',\n",
    "    'DPLK': ''\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "data[\"ERROR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\new as per 5 June\\error060824061742698_uatcalltype.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df=df.iloc[:50]\n",
    "df['SEGMENT']='Individu Umum'\n",
    "df.to_csv('error060824061742698_uatcalltype_resolved.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the error in Segment Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'Individu umum']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_40000\\1757643033.py:45: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Data Mapping - Case_Type.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# df['Product'].unique()\n",
    "\n",
    "# I have a dataset, I want in column Segment you map\n",
    "# Individu=Individu umum\n",
    "# All becomes empty\n",
    "# and column Product\n",
    "\n",
    "# loans = Loans\n",
    "# savings = Savings\n",
    "\n",
    "\n",
    "\n",
    "# mapping\n",
    "\n",
    "segment_mapping = {\n",
    "    'Individu': 'Individu umum',\n",
    "}\n",
    "\n",
    "product_mapping = {\n",
    "    'loans': 'Loans',\n",
    "    'savings': 'Savings',\n",
    "    'KTA' : ''\n",
    "}\n",
    "\n",
    "subproduk_mapping = {\n",
    "    'Giro' : 'Giro (CA)',\n",
    "    'KPR, KUR, KMK, KI' : '',\n",
    "    'SBN' : '',\n",
    "    'DPLK' : ''\n",
    "}\n",
    "\n",
    "case_category = {\n",
    "    'Customer Feedback / Sentiment Monitoring' : 'Customer Feedback'\n",
    "}\n",
    "\n",
    "\n",
    "df['Segment'] = df['Segment'].apply(lambda x: segment_mapping.get(x, ''))\n",
    "df['Product'] = df['Product'].map(product_mapping).fillna(df['Product'])\n",
    "\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "print(df['Segment'].unique())\n",
    "\n",
    "# df['Product'].unique()\n",
    "\n",
    "# output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\master_casetype_cleaned.csv\"\n",
    "\n",
    "# See my code above I wnat another mapping in column Case Category and Sub Produk\n",
    "\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_40000\\3962062038.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# path= r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424015731096.csv\"\n",
    "# path2 = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424020750586.csv\"\n",
    "path3 = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424022013225.csv\"\n",
    "df = pd.read_csv(path3)\n",
    "\n",
    "# segment_mapping = {\n",
    "#     'Individu': 'Individu umum',\n",
    "# }\n",
    "\n",
    "# product_mapping = {\n",
    "#     'loans': 'Loans',\n",
    "#     'savings': 'Savings',\n",
    "#     'KTA': ''\n",
    "# }\n",
    "\n",
    "# subproduk_mapping = {\n",
    "#     'Giro': 'Giro (CA)',\n",
    "#     'KPR, KUR, KMK, KI': '',\n",
    "#     'SBN': '',\n",
    "#     'DPLK': ''\n",
    "# }\n",
    "\n",
    "# case_category = {\n",
    "#     'Customer Feedback / Sentiment Monitoring': 'Customer Feedback',\n",
    "#     # Add more mappings for other Case Category values here\n",
    "# }\n",
    "\n",
    "# # Apply mapping to columns\n",
    "# df['SEGMENT'] = df['SEGMENT'].apply(lambda x: segment_mapping.get(x, ''))\n",
    "# df['PRODUCT'] = df['PRODUCT'].map(product_mapping).fillna(df['PRODUCT'])\n",
    "# df['CASE CATEGORY'] = df['CASE CATEGORY'].apply(lambda x: case_category.get(x, x))  # Apply default value if not mapped\n",
    "# df['SUB PRODUCT'] = df['SUB PRODUCT'].map(subproduk_mapping).fillna(df['SUB PRODUCT'])\n",
    "df.fillna('', inplace=True)\n",
    "df.replace('Transaction Solution', '', inplace=True)\n",
    "output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\master_casetype_cleaned_error3.csv\"\n",
    "\n",
    "# df['SUB PRODUK'] = df.apply(lambda row: row['SUB PRODUK'].replace('Transaction Solution', '') if 'Servicing' in row['PRODUK'] else row['SUB PRODUK'], axis=1)\n",
    "# Copy values from NEW CASE DESCRIPTION to CASE DESCRIPTION if empty\n",
    "df['CASE DESCRIPTION']='Nasabah Mengajukan Unlock User Microsite DPLK'\n",
    "df\n",
    "\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call type mapping for columns 'Produk', 'Jenis Produk', 'Jenis Laporan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "user_dataset_path = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "user_df = pd.read_csv(user_dataset_path)\n",
    "master_df_path = r\"D:\\dataquality\\master_calltype.csv\"\n",
    "master_df = pd.read_csv(master_df_path)\n",
    "\n",
    "\n",
    "master_df = master_df.rename(columns={\n",
    "    'Case Types': 'Call_Type_ID', \n",
    "    'Product': 'Produk', \n",
    "    'Sub Product': 'Jenis_Produk', \n",
    "    'Case Category': 'Jenis_Laporan'\n",
    "})\n",
    "\n",
    "\n",
    "user_df['Call_Type_ID'] = user_df['Call_Type_ID'].astype(str)\n",
    "master_df['Call_Type_ID'] = master_df['Call_Type_ID'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(user_df, master_df[['Call_Type_ID', 'Produk', 'Jenis_Produk', 'Jenis_Laporan']], on='Call_Type_ID', how='left')\n",
    "\n",
    "user_df['Produk'] = merged_df['Produk_y']\n",
    "user_df['Jenis_Produk'] = merged_df['Jenis_Produk_y']\n",
    "user_df['Jenis_Laporan'] = merged_df['Jenis_Laporan_y']\n",
    "\n",
    "\n",
    "\n",
    "user_df.to_csv(user_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_text_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:  \n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]  \n",
    "                current_entry.append(parts[3].strip())  \n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def remove_bom_and_strip(df):\n",
    "    return df.applymap(lambda x: x.replace('\\ufeff', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.txt\"\n",
    "processed_data = process_text_data(file_path)\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "# df_final=df_final.iloc[:10]\n",
    "df_final.iloc[:10].to_csv('details_uat_20230101_20230101.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to details_20230101_20230101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:\n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]\n",
    "                current_entry.append(parts[3].strip())\n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.csv\"\n",
    "processed_data = process_csv_data(file_path)\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "\n",
    "df_final = df_final.iloc[:10]\n",
    "output_path = \"details_20230101_20230101.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the file A.1 and file A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#just take 10 lines for an example\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.iloc[:10].to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path_1 = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "file_path_2 = r\"D:\\dataquality2\\details_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "df_tenline_bricare = pd.read_csv(file_path_1)\n",
    "df_detail_bricare_10line = pd.read_csv(file_path_2)\n",
    "\n",
    "df_detail_bricare_10line.columns = ['Ticket_ID', 'Details']\n",
    "\n",
    "merged_df = pd.merge(df_tenline_bricare, df_detail_bricare_10line, on='Ticket_ID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "column_to_move=\"Details\"\n",
    "merged_df = merged_df[[col for col in merged_df if col != column_to_move][:3] + [column_to_move] + [col for col in merged_df if col != column_to_move][3:]] \n",
    "\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type B\n",
    "\n",
    "\n",
    "Data Extraction for File Type B (27 columns)\n",
    "\n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['TanggalClosed', 'tanggalTransaksi','Create_Date']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\" if any\n",
    "\n",
    "- PLEASE ADD CIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_33500\\3452762791.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20200101_20200101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\BRICARE_25042024 masking.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "\n",
    "data['Column1'] = data['Column1'].astype(str)\n",
    "data_cleaned = data[data['Column1'].str.match(r'TTB\\d+')]\n",
    "\n",
    "data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n",
    "data_cleaned = data_cleaned[data_cleaned['Column2'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "data_cleaned['Column4'] = pd.to_datetime(data_cleaned['Column4'], errors='coerce')\n",
    "data_cleaned = data_cleaned.dropna(subset=['Column4'])\n",
    "\n",
    "\n",
    "data_to_drop = ['Column28', 'Column29', 'Column30', 'Column31', 'Column32']\n",
    "data_cleaned = data_cleaned.drop(columns=data_to_drop)\n",
    "\n",
    "\n",
    "if len(data_cleaned.columns) <= len(column_list):\n",
    "    data_cleaned.columns = column_list[:len(data_cleaned.columns)]\n",
    "\n",
    "\n",
    "data_cleaned.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "data_cleaned.fillna('', inplace=True)\n",
    "data_cleaned = data_cleaned.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    data_cleaned[column] = pd.to_datetime(data_cleaned[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    data_cleaned[column] = data_cleaned[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "# Just take 10 lines for an example\n",
    "data_cleaned = data_cleaned.iloc[:10]\n",
    "\n",
    "\n",
    "data_cleaned.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(data_cleaned['Create_Date']))\n",
    "enddate = pd.Timestamp(max(data_cleaned['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "data_cleaned.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "27\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Details', 'Create_Date',\n",
      "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
      "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
      "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
      "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
      "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
      "       'user_login_name', 'Jenis_Produk', 'Last_Modified_By', 'Merchant_ID',\n",
      "       'Modified_Date', 'NOTAS', 'Produk', 'SLA_Status', 'TID',\n",
      "       'tanggalAttachmentDone', 'Tgl_Assigned', 'Tgl_Eskalasi', 'AnalisaSkils',\n",
      "       'Attachment_', 'Bank_BRI', 'Biaya_Admin', 'Suku_Bunga', 'Bunga',\n",
      "       'Butuh_Attachment', 'Cicilan', 'Hasil_Kunjungan', 'Log_Name',\n",
      "       'MMS_Ticket_Id', 'Mass_Ticket_Upload_Flag', 'Nama_Supervisor',\n",
      "       'Nama_TL', 'Nama_Wakabag', 'Nasabah_Prioritas', 'Notify_By',\n",
      "       'Organization', 'Output_Settlement', 'phone_survey', 'Return_Ticket',\n",
      "       'Settlement_By', 'Settlement_ID', 'Settlement', 'Site_User',\n",
      "       'Status_Return', 'Status_Transaksi', 'Submitter_Region',\n",
      "       'Submitter_SiteGroup', 'Submitter_User_group_ID', 'Tanggal_Settlement',\n",
      "       'Tgl_Foward', 'Tgl_In_Progress', 'Tgl_Returned', 'Ticket_Referensi',\n",
      "       'Tiket_Urgency', 'Tipe_Remark', 'UniqueID', 'users', 'Usergroup_ID'],\n",
      "      dtype='object')\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date', 'gateway',\n",
      "       'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal', 'status',\n",
      "       'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur', 'Nomor_Kartu',\n",
      "       'user_group', 'assgined_to', 'attachment_done', 'email', 'full_name',\n",
      "       'no_telepon', 'approver_login', 'approver_name', 'SLAResolution',\n",
      "       'submitter_login_id', 'submitter_user_group', 'user_login_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path1=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "path2=r\"D:\\dataquality\\bricare_20200101_20200101.csv\"\n",
    "\n",
    "df1=pd.read_csv(path1)\n",
    "df2=pd.read_csv(path2)\n",
    "print(len(df1.columns))\n",
    "print(len(df2.columns))\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Compare two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape_match': True,\n",
       " 'columns_match': True,\n",
       " 'column_differences': {},\n",
       " 'value_differences': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the newly uploaded files for detailed comparison\n",
    "processed_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "expected_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "# Read the files\n",
    "processed_df_newest = pd.read_csv(processed_file_path_newest)\n",
    "expected_df_newest = pd.read_csv(expected_file_path_newest)\n",
    "\n",
    "# Check for exact match first\n",
    "exact_match = processed_df_newest.equals(expected_df_newest)\n",
    "\n",
    "# Initialize a dictionary to store detailed comparison results\n",
    "comparison_details = {\n",
    "    'shape_match': processed_df_newest.shape == expected_df_newest.shape,\n",
    "    'columns_match': processed_df_newest.columns.equals(expected_df_newest.columns),\n",
    "    'column_differences': {},\n",
    "    'value_differences': {}\n",
    "}\n",
    "\n",
    "# Compare each aspect if exact match is not true\n",
    "if not exact_match:\n",
    "    # Check shape\n",
    "    if not comparison_details['shape_match']:\n",
    "        comparison_details['shape_details'] = {\n",
    "            'processed_shape': processed_df_newest.shape,\n",
    "            'expected_shape': expected_df_newest.shape\n",
    "        }\n",
    "    \n",
    "    # Check columns\n",
    "    if not comparison_details['columns_match']:\n",
    "        comparison_details['column_details'] = {\n",
    "            'processed_columns': processed_df_newest.columns.tolist(),\n",
    "            'expected_columns': expected_df_newest.columns.tolist()\n",
    "        }\n",
    "\n",
    "    # Check column-by-column values\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            comparison_details['column_differences'][column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "\n",
    "# Summarize value differences\n",
    "if not comparison_details['columns_match']:\n",
    "    value_differences = {}\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            value_differences[column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "    comparison_details['value_differences'] = value_differences\n",
    "\n",
    "comparison_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To collect all Error logs in a path\n",
    "\n",
    "directory_path is the error logs path as well as the location where the combined error log file will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined error logs saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_error_logs(directory_path):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.startswith(\"error\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Normalize the 'TICKET_ID' to ensure duplicates are identified\n",
    "    combined_df['TICKET_ID'] = combined_df['TICKET_ID'].str.upper()\n",
    "\n",
    "    combined_df = combined_df.drop_duplicates(subset='TICKET_ID')\n",
    "\n",
    "    output_path = os.path.join(directory_path, 'error_logs.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Combined error logs saved to: {output_path}\")\n",
    "    return output_path, combined_df\n",
    "\n",
    "directory_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\"\n",
    "output_path, combined_df = combine_error_logs(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To cleanse user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_43760\\4048578058.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import html\n",
    "\n",
    "\n",
    "# Function to decode HTML entities in a DataFrame\n",
    "def decode_html_entities(df):\n",
    "    df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "    return df_decoded\n",
    "\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\USER.txt\" \n",
    "\n",
    "# Read the raw file content\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_data = file.readlines()\n",
    "    \n",
    "# Split headers and data    \n",
    "header = raw_data[0].replace(\"&#124;\", \"|\").strip()\n",
    "data = [line.replace(\"&#124;\", \"|\").strip() for line in raw_data[1:]]\n",
    "\n",
    "# Read the cleaned data into a pandas DataFrame\n",
    "df_cleaned = pd.DataFrame([line.split('|') for line in data], columns=header.split('|'))\n",
    "\n",
    "# Decode HTML entities\n",
    "df_cleaned = decode_html_entities(df_cleaned)\n",
    "df_cleaned\n",
    "\n",
    "# Remove the quotes in Dataframe\n",
    "def remove_quotes(df):\n",
    "    df.columns = df.columns.str.replace('\"', '')\n",
    "    df = df.apply(lambda col: col.str.replace('\"', '', regex=True))\n",
    "    return df\n",
    "\n",
    "df_cleaned = remove_quotes(df_cleaned)\n",
    "df_cleaned\n",
    "df_cleaned.to_csv(\"user_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_15156\\3036651791.py:7: DtypeWarning: Columns (1,8,17,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep=',', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Option 1: Check for quoted fields or use different delimiter\n",
    "try:\n",
    "    df = pd.read_csv(path, sep=',', on_bad_lines='skip') \n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricaredatareal_20200101_20200201.csv\n",
      "Problematic data saved to bricaredatareal_problematic_20200101_20200201.csv\n",
      "Number of rows in dataframe: 463581\n",
      "Number of problematic lines: 881\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"\n",
    "]\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Initialize lists to store data and problematic lines\n",
    "data = []\n",
    "problematic_lines = []\n",
    "\n",
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    try:\n",
    "        # Identifying Ticket_ID\n",
    "        ticket_id_index = next(i for i, part in enumerate(parts) if part.startswith('TTB'))\n",
    "        ticket_id = parts[ticket_id_index]\n",
    "        \n",
    "        # Identifying Call_Type_ID\n",
    "        call_type_id_index = ticket_id_index + 1\n",
    "        call_type_id = parts[call_type_id_index]\n",
    "        \n",
    "        # Identifying Create_Date (the part that looks like a datetime string)\n",
    "        create_date_index = next(i for i, part in enumerate(parts) if '-' in part and ':' in part)\n",
    "        create_date = parts[create_date_index]\n",
    "        \n",
    "        # Call_Type is everything between Call_Type_ID and Create_Date\n",
    "        call_type = ' '.join(parts[call_type_id_index + 1:create_date_index])\n",
    "        \n",
    "        # The rest of the fields in order\n",
    "        rest = parts[create_date_index + 1:]\n",
    "        \n",
    "        # Combine into a single list in the correct order\n",
    "        structured_line = [ticket_id, call_type_id, call_type, create_date] + rest\n",
    "        \n",
    "        # If the length is correct, return it\n",
    "        if len(structured_line) == len(column_list):\n",
    "            return structured_line\n",
    "        # If there are extra fields, handle them (for example, by merging or ignoring)\n",
    "        elif len(structured_line) > len(column_list):\n",
    "            return structured_line[:len(column_list)]\n",
    "        else:\n",
    "            return None\n",
    "    except StopIteration:\n",
    "        # If any part of the parsing fails, consider the line problematic\n",
    "        return None\n",
    "\n",
    "# Read the file line by line and parse it\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        parsed_line = parse_line(lines[i])\n",
    "        if parsed_line:\n",
    "            data.append(parsed_line)\n",
    "            i += 1\n",
    "        else:\n",
    "            # Check if merging the next line solves the issue\n",
    "            if i + 1 < len(lines):\n",
    "                merged_line = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "                parsed_line = parse_line(merged_line)\n",
    "                if parsed_line:\n",
    "                    data.append(parsed_line)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    problematic_lines.append((i, lines[i]))\n",
    "                    i += 1\n",
    "            else:\n",
    "                problematic_lines.append((i, lines[i]))\n",
    "                i += 1\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=column_list)\n",
    "\n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].astype(str)\n",
    "df = df[df['Ticket_ID'].str.match(r'TTB\\d+')]\n",
    "\n",
    "df['Call_Type_ID'] = df['Call_Type_ID'].astype(str)\n",
    "df = df[df['Call_Type_ID'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "df = df.dropna(subset=['Create_Date'])\n",
    "\n",
    "df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricaredatareal_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n",
    "problematic_df = pd.DataFrame(problematic_lines, columns=['Line_Number', 'Data'])\n",
    "\n",
    "\n",
    "problematic_filename = f\"bricaredatareal_problematic_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "problematic_df.to_csv(problematic_filename, index=False)\n",
    "\n",
    "print(f\"Problematic data saved to {problematic_filename}\")\n",
    "\n",
    "\n",
    "print(f\"Number of rows in dataframe: {len(df)}\")\n",
    "print(f\"Number of problematic lines: {len(problematic_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create masked Data for UAT\n",
    "\n",
    "\n",
    "To Mask:\n",
    "\n",
    "Nama_Nasabah\n",
    "No_Rekening\n",
    "full_name\n",
    "no_telepon\n",
    "approver_name = beda dengan Nama_Nasabah\n",
    "user_login_name = beda \n",
    "Log_Name\n",
    "Nama_Supervisor\n",
    "Nama_TL\n",
    "Nama_Wakabag\n",
    "Remark\n",
    "\n",
    "Kolom Detail:\n",
    "\n",
    "Kode Cabang          : 0307 4 digits\n",
    "No Kartu             : 5221843130736932 16 digits\n",
    "No Rekening          : 030701098507501 15 digits\n",
    "Nama                 : SITI SHOLEHA\n",
    "No ID                : 3316022012770004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random \n",
    "\n",
    "fake = Faker('id_ID')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "def mask_data(df):\n",
    "    df['Nama_Nasabah'] = df['Nama_Nasabah'].apply(lambda x: fake.name())\n",
    "    df['No_Rekening'] = df['No_Rekening'].apply(lambda x: fake.bban())\n",
    "    df['full_name'] = df['full_name'].apply(lambda x: fake.name())\n",
    "    df['no_telepon'] = df['no_telepon'].apply(lambda x: fake.phone_number())\n",
    "    df['approver_name'] = df['approver_name'].apply(lambda x: fake.name())\n",
    "    df['user_login_name'] = df['user_login_name'].apply(lambda x: fake.user_name())\n",
    "    df['Log_Name'] = df['Log_Name'].apply(lambda x: fake.user_name())\n",
    "    df['Nama_Supervisor'] = df['Nama_Supervisor'].apply(lambda x: fake.name())\n",
    "    df['Nama_TL'] = df['Nama_TL'].apply(lambda x: fake.name())\n",
    "    df['Nama_Wakabag'] = df['Nama_Wakabag'].apply(lambda x: fake.name())\n",
    "    return df\n",
    "\n",
    "\n",
    "df_masked = mask_data(df)\n",
    "\n",
    "def generate_nik():\n",
    "    return f'{fake.random_number(digits=16)}'\n",
    "\n",
    "def mask_detail(detail):\n",
    "    if isinstance(detail, float) and pd.isna(detail):\n",
    "        return detail\n",
    "    lines = str(detail).split('\\n')\n",
    "    masked_lines = []\n",
    "    for line in lines:\n",
    "        if 'Kode Cabang' in line:\n",
    "            line = f'Kode Cabang          : {fake.random_number(digits=4)}'\n",
    "        elif 'No Kartu' in line:\n",
    "            line = f'No Kartu             : {fake.credit_card_number()}'\n",
    "        elif 'No Rekening' in line:\n",
    "            line = f'No Rekening          : {fake.bban()}'\n",
    "        elif 'Nama' in line:\n",
    "            line = f'Nama                 : {fake.name()}'\n",
    "        elif 'No ID' in line:\n",
    "            line = f'No ID                : {generate_nik()}'\n",
    "        masked_lines.append(line)\n",
    "    return '\\n'.join(masked_lines)\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "\n",
    "\n",
    "# Ensure 'Details' column is treated as string and apply mask_detail function\n",
    "df_masked['Details'] = df_masked['Details'].astype(str).apply(mask_detail)\n",
    "\n",
    "\n",
    "\n",
    "# gateway_options = [\n",
    "#     'Email', 'Phone', 'Instagram', 'Walk-In', 'MMS', \n",
    "#     'Twitter', 'Facebook', 'BRImo', 'BRILink', 'Sabrina', 'Ceria'\n",
    "# ]\n",
    "\n",
    "#UAT\n",
    "gateway_options = [\n",
    "    'Email', 'Phone', 'Web', 'Facebook', 'Twitter'\n",
    "]\n",
    "# Assign random values to the 'gateway' column\n",
    "df_masked['gateway'] = df_masked['gateway'].apply(lambda x: random.choice(gateway_options))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "df_masked\n",
    "output_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101_masking.csv\"\n",
    "df_masked.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"bricare_uat_20230101_20230101_masking.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['status']='New'\n",
    "df.to_csv(r\"bricare_uat_20230101_20230101_masking_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in SIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\SIT\\dummy_data_case1100000.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "# df['User*']='00GMR0000000dCf2AI'\n",
    "df['User*']='00GMR0000000dEH2AY'\n",
    "df['Status']='Escalated'\n",
    "df['Status'] = df['Status'].replace('New', 'Escalated')\n",
    "df['Case Origin']='Email'\n",
    "df['Case Type']='a1BMR00000010h72AA'\n",
    "df['Account']='001MR000004C7l2YAC'\n",
    "\n",
    "df.to_csv('dummy_data_uat60k.csv', index=False)\n",
    "\n",
    "# df['Merchant ID']\n",
    "# df['TID']\n",
    "# df['User*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in UAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "df.to_csv('dummy_data_case_uat60k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\CHM\\CHM\\Part2\\bricare_get_cleansing_77_kolom(1).csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "\n",
    "output_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\bricare_get_cleansing_77_kolom_fixed_return.csv\"\n",
    "df=df.iloc[:1]\n",
    "df['status']='Return'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# output_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "ref_fiturid_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\ref_fiturid.csv\"\n",
    "id_calltype_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_calltype_uat.csv\"\n",
    "\n",
    "ref_fiturid = pd.read_csv(ref_fiturid_path)\n",
    "id_calltype = pd.read_csv(id_calltype_path)\n",
    "\n",
    "# Convert calltype and NAME columns to string type\n",
    "ref_fiturid['calltype'] = ref_fiturid['calltype'].astype(str)\n",
    "id_calltype['NAME'] = id_calltype['NAME'].astype(str)\n",
    "\n",
    "# Merge the dataframes to add the Call_Type_ID column based on the calltype column\n",
    "ref_fiturid = ref_fiturid.merge(id_calltype[['NAME', 'ID']], left_on='calltype', right_on='NAME', how='left')\n",
    "\n",
    "# Rename the ID column to Call_Type_ID\n",
    "ref_fiturid.rename(columns={'ID': 'Call_Type_ID'}, inplace=True)\n",
    "\n",
    "# Drop the unnecessary NAME column from the merge\n",
    "ref_fiturid.drop(columns=['NAME'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\"\n",
    "# ref_fiturid=ref_fiturid.iloc[62:63]\n",
    "ref_fiturid.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Updated CSV file saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>Call Type Info</th>\n",
       "      <th>Type</th>\n",
       "      <th>User</th>\n",
       "      <th>Title</th>\n",
       "      <th>Detail Bricare</th>\n",
       "      <th>Deskripsi</th>\n",
       "      <th>Empati</th>\n",
       "      <th>Konfirmasi</th>\n",
       "      <th>...</th>\n",
       "      <th>Verifikasi</th>\n",
       "      <th>Gali Informasi</th>\n",
       "      <th>Pembuatan Laporan</th>\n",
       "      <th>Konfirmasi Ulang</th>\n",
       "      <th>Percepatan Komplain</th>\n",
       "      <th>Solusi, Informasi</th>\n",
       "      <th>Edukasi &amp; Cross Selling</th>\n",
       "      <th>Closing</th>\n",
       "      <th>Article Type</th>\n",
       "      <th>URL_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>Nasabah Menanyakan Informasi Pengajuan Terkait...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1000 - Nasabah Menanyakan Informasi Pengajuan ...</td>\n",
       "      <td>nomor ponsel nasabah yang bisa dihubungi\\nID C...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...</td>\n",
       "      <td>Jika Nasabah menanyakan Cara &amp; Syarat Pengajua...</td>\n",
       "      <td>...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nVerifikas...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nGali Info...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nBuat lapo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nInformasi...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1002</td>\n",
       "      <td>Nasabah Menyakan Terkait Promo dan Program CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1002 - Nasabah Menyakan Terkait Promo dan Prog...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\nJika...</td>\n",
       "      <td>Konfirmasi :\\nJika Nasabah menanyakan promo ce...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi\\nTidak dilakukan Verifikasi\\n</td>\n",
       "      <td>Gali Informasi\\t\\n\\t i. Jenis promo/...</td>\n",
       "      <td>Pembuatan Laporan\\t\\n\\ti. Nomor laporan ti...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\t\\n\\t  i.              \\...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1003</td>\n",
       "      <td>Nasabah Mengajukan Pelunasan Awal Cicilan CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1003 - Nasabah Mengajukan Pelunasan Awal Cicil...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\n\"Bap...</td>\n",
       "      <td>Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...</td>\n",
       "      <td>Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi\\n1.\\tCicilan telah berjalan...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1005</td>\n",
       "      <td>Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1005 - Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>Nasabah mengajukan pemblokiran Sementara Akun ...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati\\nJika nasabah infokan ingin melakukan p...</td>\n",
       "      <td>Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...</td>\n",
       "      <td>Pembuatan Laporan\\nNasabah mengajukan pembloki...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\n1.\\tAgent Konfirmasi ke...</td>\n",
       "      <td>Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...</td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1005-Nasabah-Mengajukan-Pemblokiran-CERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1008</td>\n",
       "      <td>Nasabah Mengajukan Pengaktifan Akun Ceria Terb...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1008 - Nasabah Mengajukan Pengaktifan Akun Cer...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 Call Type                                     Call Type Info  \\\n",
       "0            1      1000  Nasabah Menanyakan Informasi Pengajuan Terkait...   \n",
       "1            2      1002   Nasabah Menyakan Terkait Promo dan Program CERIA   \n",
       "2            3      1003    Nasabah Mengajukan Pelunasan Awal Cicilan CERIA   \n",
       "3            4      1005               Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4            5      1008  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...   \n",
       "..         ...       ...                                                ...   \n",
       "437                                                                           \n",
       "438                                                                           \n",
       "439                                                                           \n",
       "440                                                                           \n",
       "441                                                                           \n",
       "\n",
       "      Type                  User  \\\n",
       "0    CERIA  Agent Contact Center   \n",
       "1    CERIA  Agent Contact Center   \n",
       "2    CERIA  Agent Contact Center   \n",
       "3    CERIA  Agent Contact Center   \n",
       "4    CERIA  Agent Contact Center   \n",
       "..     ...                   ...   \n",
       "437                                \n",
       "438                                \n",
       "439                                \n",
       "440                                \n",
       "441                                \n",
       "\n",
       "                                                 Title  \\\n",
       "0    1000 - Nasabah Menanyakan Informasi Pengajuan ...   \n",
       "1    1002 - Nasabah Menyakan Terkait Promo dan Prog...   \n",
       "2    1003 - Nasabah Mengajukan Pelunasan Awal Cicil...   \n",
       "3          1005 - Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4    1008 - Nasabah Mengajukan Pengaktifan Akun Cer...   \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Detail Bricare  \\\n",
       "0    nomor ponsel nasabah yang bisa dihubungi\\nID C...   \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Nasabah mengajukan pemblokiran Sementara Akun ...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                             Deskripsi  \\\n",
       "0    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "1    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "2    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "3    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                                Empati  \\\n",
       "0    Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...   \n",
       "1    Empati sesuai kondisi (pilih salah satu)\\nJika...   \n",
       "2    Empati sesuai kondisi (pilih salah satu)\\n\"Bap...   \n",
       "3    Empati\\nJika nasabah infokan ingin melakukan p...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                            Konfirmasi  ...  \\\n",
       "0    Jika Nasabah menanyakan Cara & Syarat Pengajua...  ...   \n",
       "1    Konfirmasi :\\nJika Nasabah menanyakan promo ce...  ...   \n",
       "2    Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...  ...   \n",
       "3    Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...  ...   \n",
       "4                                                       ...   \n",
       "..                                                 ...  ...   \n",
       "437                                                     ...   \n",
       "438                                                     ...   \n",
       "439                                                     ...   \n",
       "440                                                     ...   \n",
       "441                                                     ...   \n",
       "\n",
       "                                            Verifikasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nVerifikas...   \n",
       "1             Verifikasi\\nTidak dilakukan Verifikasi\\n   \n",
       "2    Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...   \n",
       "3    Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Gali Informasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nGali Info...   \n",
       "1    Gali Informasi\\t\\n\\t i. Jenis promo/...   \n",
       "2    Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...   \n",
       "3    Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                     Pembuatan Laporan Konfirmasi Ulang  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nBuat lapo...                    \n",
       "1    Pembuatan Laporan\\t\\n\\ti. Nomor laporan ti...                    \n",
       "2    Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...                    \n",
       "3    Pembuatan Laporan\\nNasabah mengajukan pembloki...                    \n",
       "4                                                                         \n",
       "..                                                 ...              ...   \n",
       "437                                                                       \n",
       "438                                                                       \n",
       "439                                                                       \n",
       "440                                                                       \n",
       "441                                                                       \n",
       "\n",
       "    Percepatan Komplain                                  Solusi, Informasi  \\\n",
       "0                        1\\tCara & Syarat pengajuan Aplikasi\\nInformasi...   \n",
       "1                        Informasi & Solusi \\t\\t\\n\\t  i.              \\...   \n",
       "2                        Informasi & Solusi\\n1.\\tCicilan telah berjalan...   \n",
       "3                        Informasi & Solusi \\t\\n1.\\tAgent Konfirmasi ke...   \n",
       "4                                                                            \n",
       "..                  ...                                                ...   \n",
       "437                                                                          \n",
       "438                                                                          \n",
       "439                                                                          \n",
       "440                                                                          \n",
       "441                                                                          \n",
       "\n",
       "                               Edukasi & Cross Selling  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                               Closing         Article Type  \\\n",
       "0    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "1    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "2    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "3    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "4                                                       Working Instruction   \n",
       "..                                                 ...                  ...   \n",
       "437                                                     Working Instruction   \n",
       "438                                                     Working Instruction   \n",
       "439                                                     Working Instruction   \n",
       "440                                                     Working Instruction   \n",
       "441                                                     Working Instruction   \n",
       "\n",
       "                                              URL_name  \n",
       "0    1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...  \n",
       "1    1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...  \n",
       "2    1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...  \n",
       "3            1005-Nasabah-Mengajukan-Pemblokiran-CERIA  \n",
       "4    1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...  \n",
       "..                                                 ...  \n",
       "437                                                     \n",
       "438                                                     \n",
       "439                                                     \n",
       "440                                                     \n",
       "441                                                     \n",
       "\n",
       "[375 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare(WI).csv\"\n",
    "df=pd.read_csv(path, encoding='ISO-8859-1')\n",
    "df = df.fillna('')\n",
    "df['Article Type']='Working Instruction'\n",
    "\n",
    "output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\artikel_all.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Change 'Brilink' to 'BRILink' in 'Type' column\n",
    "df['Type'] = df['Type'].replace('Brilink', 'BRILink')\n",
    "\n",
    "# Remove rows with 'Wholesale' in 'Type' column\n",
    "df = df[df['Type'] != 'Wholesale']\n",
    "\n",
    "# df['Type'].unique()\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('artikel2.csv', index= False)\n",
    "# df.iloc[:1].to_csv(output, index= False)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df= pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424041601915.csv\")\n",
    "df.to_csv('error.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare 2.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path)\n",
    "df=df.iloc[:1]\n",
    "# df['Gali Informasi']\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "df.to_csv('artikel_bullet.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate 1k roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/extract_role.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the uploaded CSV file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/extract_role.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m roles_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the content of the CSV file to understand its structure\u001b[39;00m\n\u001b[0;32m      8\u001b[0m roles_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/extract_role.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '/mnt/data/extract_role.csv'\n",
    "roles_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the content of the CSV file to understand its structure\n",
    "roles_df.head()\n",
    "\n",
    "\n",
    "# Number of additional roles needed to reach a total of 1000\n",
    "num_additional_roles_needed = 1000 - len(expanded_roles_df)\n",
    "\n",
    "# Duplicate the existing roles to meet the required number of roles\n",
    "additional_roles_more = expanded_roles_df.sample(num_additional_roles_needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "# Generate unique names for the additional roles\n",
    "additional_roles_more['NAME'] = additional_roles_more['NAME'] + \"_more_\" + (additional_roles_more.index + 1).astype(str)\n",
    "\n",
    "# Combine the original roles with the additional roles\n",
    "expanded_roles_df_more = pd.concat([expanded_roles_df, additional_roles_more], ignore_index=True)\n",
    "\n",
    "# Ensure we have a total of 1000 roles\n",
    "expanded_roles_df_more = expanded_roles_df_more.head(1000)\n",
    "\n",
    "# Save the expanded roles to a new CSV file\n",
    "output_file_path_more = '/mnt/data/expanded_roles_1000.csv'\n",
    "expanded_roles_df_more.to_csv(output_file_path_more, index=False)\n",
    "\n",
    "output_file_path_more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Extraction Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check data before 2023 = 27 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns in DataFrame: {'Tgl_Assigned'}\n",
      "Extra columns in DataFrame: {'TglAssigned', 'cifno', 'Details'}\n"
     ]
    }
   ],
   "source": [
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_27_kolom.csv\"\n",
    "path2=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_79_kolom.csv\"\n",
    "# df= pd.read_csv(path, delimiter=';')\n",
    "df= pd.read_csv(path2, delimiter=';')\n",
    "\n",
    "\n",
    "# column_list_set = set(column_list)\n",
    "column_list_set = set(column_names)\n",
    "df_columns_set = set(df.columns)\n",
    "\n",
    "missing_columns = column_list_set - df_columns_set\n",
    "extra_columns = df_columns_set - column_list_set\n",
    "\n",
    "if not missing_columns and not extra_columns:\n",
    "    print(\"The column names match.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "    if extra_columns:\n",
    "        print(f\"Extra columns in DataFrame: {extra_columns}\")\n",
    "\n",
    "################################################\n",
    "# column_list_set2 = set(column_names)\n",
    "# df_columns_set2 = set(df2.columns)\n",
    "\n",
    "# missing_columns2 = column_list_set2 - df_columns_set2\n",
    "# extra_columns2 = df_columns_set2 - column_list_set2\n",
    "\n",
    "# if not missing_columns and not extra_columns:\n",
    "#     print(\"The column names match.\")\n",
    "# else:\n",
    "#     if missing_columns2:\n",
    "#         print(f\"Missing columns in DataFrame: {missing_columns2}\")\n",
    "#     if extra_columns2:\n",
    "#         print(f\"Extra columns in DataFrame: {extra_columns2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data dengan 27 kolom:\n",
    "- jumlah kolom sudah ok\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- attachment\n",
    "- penambahan nama file diakhir \"27\"\n",
    "\n",
    "\n",
    "\n",
    "Data dengan 79 kolom:\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- penambahan nama file diakhir \"79\"\n",
    "\n",
    "\n",
    "\n",
    "Data Zendesk:\n",
    "- harus hilangkan double quotes \"\"\n",
    "- delimiternya ;\n",
    "- nama file zendesk\n",
    "\n",
    "\n",
    "Data Omni:\n",
    "- SMS: harus format csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Unable to create/update fields: LastModifiedDa...\n",
       "1    Unable to create/update fields: LastModifiedDa...\n",
       "2    Unable to create/update fields: LastModifiedDa...\n",
       "3    Unable to create/update fields: LastModifiedDa...\n",
       "4    Unable to create/update fields: LastModifiedDa...\n",
       "5    Unable to create/update fields: LastModifiedDa...\n",
       "6    Unable to create/update fields: LastModifiedDa...\n",
       "7    Unable to create/update fields: LastModifiedDa...\n",
       "8    Unable to create/update fields: LastModifiedDa...\n",
       "9    Unable to create/update fields: LastModifiedDa...\n",
       "Name: ERROR, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060524083553095.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['ERROR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zendesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
      "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
      "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
      "\n",
      "     Requester name                                     Ticket subject  \\\n",
      "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
      "1  Masruroh Sodikin                                                  P   \n",
      "\n",
      "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
      "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
      "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
      "\n",
      "  Ticket solved - Timestamp                 Tickets  \n",
      "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
      "1       2024-01-01T00:46:27  1.00000000000000000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket ID</th>\n",
       "      <th>Ticket channel</th>\n",
       "      <th>Assignee ID</th>\n",
       "      <th>Assignee name</th>\n",
       "      <th>Requester ID</th>\n",
       "      <th>Requester name</th>\n",
       "      <th>Ticket subject</th>\n",
       "      <th>Requester created - Timestamp</th>\n",
       "      <th>Ticket created - Timestamp</th>\n",
       "      <th>Ticket solved - Timestamp</th>\n",
       "      <th>Tickets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3435272</td>\n",
       "      <td>Any channel</td>\n",
       "      <td>405257525413</td>\n",
       "      <td>Agent Sosmed 4</td>\n",
       "      <td>27064719563033</td>\n",
       "      <td>vianovia94</td>\n",
       "      <td>[IGC] Minimal opo boloo... ......#relsvideo #r...</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:30:56</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3435273</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>405257335633</td>\n",
       "      <td>Agent Sosmed 3</td>\n",
       "      <td>27064768687769</td>\n",
       "      <td>Masruroh Sodikin</td>\n",
       "      <td>P</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:46:27</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
       "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
       "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
       "\n",
       "     Requester name                                     Ticket subject  \\\n",
       "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
       "1  Masruroh Sodikin                                                  P   \n",
       "\n",
       "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
       "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
       "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
       "\n",
       "  Ticket solved - Timestamp                 Tickets  \n",
       "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
       "1       2024-01-01T00:46:27  1.00000000000000000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\Data Zendesk 1-15 Januari 2024 - sampel.csv\"\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "# Read the CSV file with proper handling of quotes and custom separator\n",
    "with open(file_path, 'r', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "    rows = [row for row in reader]\n",
    "\n",
    "# Split the combined columns that have commas within them\n",
    "split_rows = []\n",
    "for row in rows:\n",
    "    split_row = row[0].split(',') + row[1:]  # Split the first column and add the rest as they are\n",
    "    split_rows.append(split_row)\n",
    "\n",
    "# Create DataFrame from the processed rows\n",
    "df = pd.DataFrame(split_rows[1:], columns=split_rows[0])\n",
    "\n",
    "# Remove double quotes from column names\n",
    "df.columns = df.columns.str.replace('\"', '')\n",
    "\n",
    "# Remove double quotes from all data in the DataFrame\n",
    "df = df.replace('\"', '', regex=True)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "print(df.head())\n",
    "df\n",
    "\n",
    "# If you need to save the cleaned DataFrame to a new CSV file\n",
    "# cleaned_file_path = '/mnt/data/Cleaned_Zendesk_Data.csv'\n",
    "# df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# df=pd.read_csv(path)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# df\u001b[39;00m\n\u001b[0;32m     10\u001b[0m path2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaste\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataloader_v60.0.2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata check\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnew as per 5 June\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcase.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\bricare_20221213_20240604_case_account.csv\"\n",
    "# df=pd.read_csv(path)\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "path2=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\case.csv\"\n",
    "df=pd.read_csv(path2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy data for Alex7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Account Alex7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for account with quoting (DONE)\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_dummy_data(file_path, file_name, num_rows):\n",
    "    # Define possible values for each column\n",
    "    account_names = ['John Doe', 'Jane Smith', 'Mike Brown', 'Lisa Green', 'Mark Taylor']\n",
    "    account_owners = ['Owner A', 'Owner B', 'Owner C', 'Owner D']\n",
    "    nasabah_types = ['Nasabah', 'Non Nasabah']\n",
    "    account_record_types = ['Personal', 'Non Personal']\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Writing to the CSV file\n",
    "    with open(os.path.join(file_path, file_name), mode='w', newline='') as file:\n",
    "        writer = csv.writer(file, quotechar='\"', quoting=csv.QUOTE_ALL)  # Enforce quoting for all fields\n",
    "        writer.writerow(['Account Name', 'CIF No', 'Account Owner', 'No Telp', 'Email', 'Nasabah Type', 'Account Record Type'])\n",
    "        \n",
    "        for _ in range(num_rows):\n",
    "            account_name = random.choice(account_names)\n",
    "            email = f\"{account_name.split(' ')[0].lower()}.{account_name.split(' ')[1].lower()}@example.com\"\n",
    "            writer.writerow([\n",
    "                account_name,\n",
    "                ''.join([\"{}\".format(random.randint(0, 9)) for _ in range(10)]),  # 10-digit CIF No\n",
    "                random.choice(account_owners),\n",
    "                f'+62{random.randint(1000000000, 9999999999)}',  # Phone number\n",
    "                email,\n",
    "                random.choice(nasabah_types),\n",
    "                random.choice(account_record_types)\n",
    "            ])\n",
    "\n",
    "\n",
    "file_path = 'D:\\dataquality2'  # Adjust the path as needed\n",
    "file_name = 'dummy_data_account.csv'\n",
    "num_rows = 1  # Adjust the number of rows as needed\n",
    "\n",
    "generate_dummy_data(file_path, file_name, num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Case ALex 7\n",
    "\n",
    "\n",
    "num of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dummy_casefor_alex5.csv'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_salesforce_case_dummy_data(num_rows=100):\n",
    "    import pandas as pd\n",
    "    import random\n",
    "\n",
    "    # Define the columns and possible values\n",
    "    statuses = [\"New\", \"Working\", \"Escalated\"]\n",
    "    types = [\"Electronic\", \"Electrical\", \"Mechanical\"]\n",
    "    case_reasons = [\"Performance\", \"Breakdown\"]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        \"Status\": [random.choice(statuses) for _ in range(num_rows)],\n",
    "        \"Type\": [random.choice(types) for _ in range(num_rows)],\n",
    "        \"Case Reason\": [random.choice(case_reasons) for _ in range(num_rows)],\n",
    "        \"Subject\": [f\"Subject {i+1}\" for i in range(num_rows)],\n",
    "        \"Description\": [f\"Description of issue {i+1}\" for i in range(num_rows)],\n",
    "        \"Legacy_Ticket_ID\": [f\"TTB{i+1:06d}\" for i in range(num_rows)],  # Generating 6-digit random numbers in order\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = 'dummy_casefor_alex5.csv'\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "# Example usage\n",
    "file_path = create_salesforce_case_dummy_data(num_rows=5)\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_5108\\2256201397.py:5: DtypeWarning: Columns (14,16,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\bricaredatareal_20200101_20200201.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df.iloc[:100]\n",
    "df.to_csv('bricare_100_pakpondah.csv',index=False)\n",
    "\n",
    "# data dummy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_data.csv with 150 rows created successfully.\n"
     ]
    }
   ],
   "source": [
    "#To create dummy data to maintain the case number\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dummy_file(filename, num_rows):\n",
    "  \n",
    "    data = {\n",
    "        'Status': ['Cancelled'] * num_rows,\n",
    "        'Priority': ['Low'] * num_rows,\n",
    "        'Call Type' : ['1000'] * num_rows\n",
    "    }\n",
    "    \n",
    "  \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "   \n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"{filename} with {num_rows} rows created successfully.\")\n",
    "\n",
    "\n",
    "filename = 'dummy_data.csv'\n",
    "num_rows = 150 \n",
    "generate_dummy_file(filename, num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real data to be imported\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# path=r\"C:\\Users\\maste\\Downloads\\Test Data Dummy.csv\"\n",
    "# path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_150_casenumberandID.csv\"\n",
    "path=r\"C:\\Users\\maste\\Downloads\\Test Data Dummy.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Case ALex 7\n",
    "Join the two files based on Ticket ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Ticket_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m extract_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCASENUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicket_ID\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Merge the dataframes based on 'Ticket_ID'\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data_dummy_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTicket_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Reorder the columns to place 'ID' as the first column\u001b[39;00m\n\u001b[0;32m     28\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m merged_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:794\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_left_right_on(left_on, right_on)\n\u001b[0;32m    788\u001b[0m (\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[0;32m    792\u001b[0m     left_drop,\n\u001b[0;32m    793\u001b[0m     right_drop,\n\u001b[1;32m--> 794\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_drop_labels_or_levels(left_drop)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1310\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m     lk \u001b[38;5;241m=\u001b[39m cast(Hashable, lk)\n\u001b[1;32m-> 1310\u001b[0m     left_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1311\u001b[0m     join_names\u001b[38;5;241m.\u001b[39mappend(lk)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:1911\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1909\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1911\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Ticket_ID'"
     ]
    }
   ],
   "source": [
    "# from the real data to join the ID or Case Number and update the records in Case \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded files\n",
    "extract_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_150_casenumberandID.csv\")\n",
    "# test_data_dummy_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\Test Data Dummy.csv\")\n",
    "test_data_dummy_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error062124052713145.csv\")\n",
    "\n",
    "\n",
    "# Split the 'ID;\"CASENUMBER\"' column into two separate columns\n",
    "extract_df[['ID', 'CASENUMBER']] = extract_df['ID;\"CASENUMBER\"'].str.split(';', expand=True)\n",
    "\n",
    "# Drop the original combined column\n",
    "extract_df.drop(columns=['ID;\"CASENUMBER\"'], inplace=True)\n",
    "\n",
    "# Remove leading and trailing quotes from the columns\n",
    "extract_df['ID'] = extract_df['ID'].str.strip('\"')\n",
    "extract_df['CASENUMBER'] = extract_df['CASENUMBER'].str.strip('\"')\n",
    "\n",
    "# Rename columns for easier merging\n",
    "extract_df.rename(columns={'CASENUMBER': 'Ticket_ID'}, inplace=True)\n",
    "\n",
    "# Merge the dataframes based on 'Ticket_ID'\n",
    "merged_df = test_data_dummy_df.merge(extract_df, on='Ticket_ID', how='left')\n",
    "\n",
    "# Reorder the columns to place 'ID' as the first column\n",
    "columns = ['ID'] + [col for col in merged_df.columns if col != 'ID']\n",
    "merged_df = merged_df[columns]\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "# merged_df=merged_df.iloc[:1]\n",
    "merged_df['Call_Type_ID']='1000'\n",
    "merged_df=merged_df.drop('ID', axis=1)\n",
    "merged_df.to_csv('Updated_Test_Data_Dummy_2row.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>CASENUMBER</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500MR000004NtSvYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988612</td>\n",
       "      <td>TICKET000988612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500MR000004NtSwYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988613</td>\n",
       "      <td>TICKET000988613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500MR000004NtSxYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988614</td>\n",
       "      <td>TICKET000988614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500MR000004NtSyYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500MR000004NtSzYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988616</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500MR000004NtT0YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988617</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500MR000004NtT1YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988618</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500MR000004NtT2YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988619</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500MR000004NtT3YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500MR000004NtT4YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988621</td>\n",
       "      <td>TICKET000988621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500MR000004NtT5YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988622</td>\n",
       "      <td>TICKET000988622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>500MR000004NtT6YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988623</td>\n",
       "      <td>TICKET000988623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>500MR000004NtT7YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988624</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>500MR000004NtT8YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988625</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>500MR000004NtT9YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988626</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>500MR000004NtTAYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988627</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>500MR000004NtTBYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988628</td>\n",
       "      <td>TICKET000988628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>500MR000004NtTCYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988629</td>\n",
       "      <td>TICKET000988629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>500MR000004NtTDYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988630</td>\n",
       "      <td>TICKET000988630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>500MR000004NtTEYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988631</td>\n",
       "      <td>TICKET000988631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>500MR000004NtTFYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988632</td>\n",
       "      <td>TICKET000988632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>500MR000004NtTGYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988633</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>500MR000004NtTHYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>500MR000004NtTIYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988635</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>500MR000004NtTJYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>500MR000004NtTKYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>500MR000004NtTLYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988638</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>500MR000004NtTMYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988639</td>\n",
       "      <td>TICKET000988639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>500MR000004NtTNYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988640</td>\n",
       "      <td>TICKET000988640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>500MR000004NtTOYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988641</td>\n",
       "      <td>TICKET000988641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>500MR000004NtTPYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988642</td>\n",
       "      <td>TICKET000988642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>500MR000004NtTQYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988643</td>\n",
       "      <td>TICKET000988643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>500MR000004NtTRYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988644</td>\n",
       "      <td>TICKET000988644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>500MR000004NtTSYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988645</td>\n",
       "      <td>TICKET000988645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>500MR000004NtTTYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988646</td>\n",
       "      <td>TICKET000988646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>500MR000004NtTUYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988647</td>\n",
       "      <td>TICKET000988647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>500MR000004NtTVYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988648</td>\n",
       "      <td>TICKET000988648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>500MR000004NtTWYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988649</td>\n",
       "      <td>TICKET000988649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>500MR000004NtTXYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988650</td>\n",
       "      <td>TICKET000988650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>500MR000004NtTYYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988651</td>\n",
       "      <td>TICKET000988651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>500MR000004NtTZYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988652</td>\n",
       "      <td>TICKET000988652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>500MR000004NtTaYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988653</td>\n",
       "      <td>TICKET000988653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>500MR000004NtTbYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988654</td>\n",
       "      <td>TICKET000988654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>500MR000004NtTcYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988655</td>\n",
       "      <td>TICKET000988655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>500MR000004NtTdYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988656</td>\n",
       "      <td>TICKET000988656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>500MR000004NtTeYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988657</td>\n",
       "      <td>TICKET000988657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>500MR000004NtTfYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988658</td>\n",
       "      <td>TICKET000988658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>500MR000004NtTgYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988659</td>\n",
       "      <td>TICKET000988659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>500MR000004NtThYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988660</td>\n",
       "      <td>TICKET000988660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>500MR000004NtTiYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988661</td>\n",
       "      <td>TICKET000988661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>500MR000004NtTjYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988662</td>\n",
       "      <td>TICKET000988662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>500MR000004NtTkYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988663</td>\n",
       "      <td>TICKET000988663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>500MR000004NtTlYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988664</td>\n",
       "      <td>TICKET000988664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>500MR000004NtTmYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988665</td>\n",
       "      <td>TICKET000988665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>500MR000004NtTnYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988666</td>\n",
       "      <td>TICKET000988666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>500MR000004NtToYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988667</td>\n",
       "      <td>TICKET000988667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>500MR000004NtTpYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988668</td>\n",
       "      <td>TICKET000988668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>500MR000004NtTqYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988669</td>\n",
       "      <td>TICKET000988669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>500MR000004NtTrYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988670</td>\n",
       "      <td>TICKET000988670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>500MR000004NtTsYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988671</td>\n",
       "      <td>TICKET000988671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>500MR000004NtTtYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988672</td>\n",
       "      <td>TICKET000988672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>500MR000004NtTuYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988673</td>\n",
       "      <td>TICKET000988673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>500MR000004NtTvYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988674</td>\n",
       "      <td>TICKET000988674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>500MR000004NtTwYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988675</td>\n",
       "      <td>TICKET000988675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>500MR000004NtTxYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988676</td>\n",
       "      <td>TICKET000988676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>500MR000004NtTyYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988677</td>\n",
       "      <td>TICKET000988677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>500MR000004NtTzYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988678</td>\n",
       "      <td>TICKET000988678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>500MR000004NtU0YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988679</td>\n",
       "      <td>TICKET000988679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>500MR000004NtU1YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988680</td>\n",
       "      <td>TICKET000988680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>500MR000004NtU2YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988681</td>\n",
       "      <td>TICKET000988681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>500MR000004NtU3YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988682</td>\n",
       "      <td>TICKET000988682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>500MR000004NtU4YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988683</td>\n",
       "      <td>TICKET000988683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>500MR000004NtU5YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988684</td>\n",
       "      <td>TICKET000988684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>500MR000004NtU6YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988685</td>\n",
       "      <td>TICKET000988685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>500MR000004NtU7YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988686</td>\n",
       "      <td>TICKET000988686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>500MR000004NtU8YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988687</td>\n",
       "      <td>TICKET000988687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>500MR000004NtU9YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988688</td>\n",
       "      <td>TICKET000988688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>500MR000004NtUAYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988689</td>\n",
       "      <td>TICKET000988689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>500MR000004NtUBYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988690</td>\n",
       "      <td>TICKET000988690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>500MR000004NtUCYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988691</td>\n",
       "      <td>TICKET000988691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>500MR000004NtUDYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988692</td>\n",
       "      <td>TICKET000988692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>500MR000004NtUEYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988693</td>\n",
       "      <td>TICKET000988693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>500MR000004NtUFYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988694</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>500MR000004NtUGYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988695</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>500MR000004NtUHYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988696</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>500MR000004NtUIYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988697</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>500MR000004NtUJYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988698</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>500MR000004NtUKYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988699</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>500MR000004NtULYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988700</td>\n",
       "      <td>TICKET000988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>500MR000004NtUMYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988701</td>\n",
       "      <td>TICKET000988701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>500MR000004NtUNYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988702</td>\n",
       "      <td>TICKET000988702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>500MR000004NtUOYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988703</td>\n",
       "      <td>TICKET000988703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>500MR000004NtUPYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988704</td>\n",
       "      <td>TICKET000988704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>500MR000004NtUQYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988705</td>\n",
       "      <td>TICKET000988705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>500MR000004NtURYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988706</td>\n",
       "      <td>TICKET000988706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>500MR000004NtUSYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988707</td>\n",
       "      <td>TICKET000988707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>500MR000004NtUTYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988708</td>\n",
       "      <td>TICKET000988708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>500MR000004NtUUYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988709</td>\n",
       "      <td>TICKET000988709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>500MR000004NtUVYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988710</td>\n",
       "      <td>TICKET000988710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>500MR000004NtUWYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988711</td>\n",
       "      <td>TICKET000988711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>500MR000004NtUXYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988712</td>\n",
       "      <td>TICKET000988712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>500MR000004NtUYYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988713</td>\n",
       "      <td>TICKET000988713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>500MR000004NtUZYA0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988714</td>\n",
       "      <td>TICKET000988714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>500MR000004NtUaYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988715</td>\n",
       "      <td>TICKET000988715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>500MR000004NtUbYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988716</td>\n",
       "      <td>TICKET000988716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>500MR000004NtUcYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988717</td>\n",
       "      <td>TICKET000988717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>500MR000004NtUdYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988718</td>\n",
       "      <td>TICKET000988718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>500MR000004NtUeYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988719</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>500MR000004NtUfYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988720</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>500MR000004NtUgYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988721</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>500MR000004NtUhYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>500MR000004NtUiYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988723</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>500MR000004NtUjYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988724</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>500MR000004NtUkYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988725</td>\n",
       "      <td>TICKET000988725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>500MR000004NtUlYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988726</td>\n",
       "      <td>TICKET000988726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>500MR000004NtUmYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988727</td>\n",
       "      <td>TICKET000988727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>500MR000004NtUnYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988728</td>\n",
       "      <td>TICKET000988728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>500MR000004NtUoYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988729</td>\n",
       "      <td>TICKET000988729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>500MR000004NtUpYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988730</td>\n",
       "      <td>TICKET000988730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>500MR000004NtUqYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988731</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>500MR000004NtUrYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988732</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>500MR000004NtUsYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988733</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>500MR000004NtUtYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988734</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>500MR000004NtUuYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988735</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>500MR000004NtUvYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988736</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>500MR000004NtUwYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>500MR000004NtUxYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988738</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>500MR000004NtUyYAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988739</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>500MR000004NtUzYAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988740</td>\n",
       "      <td>TICKET000988740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>500MR000004NtV0YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988741</td>\n",
       "      <td>TICKET000988741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>500MR000004NtV1YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988742</td>\n",
       "      <td>TICKET000988742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>500MR000004NtV2YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988743</td>\n",
       "      <td>TICKET000988743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>500MR000004NtV3YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988744</td>\n",
       "      <td>TICKET000988744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>500MR000004NtV4YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988745</td>\n",
       "      <td>TICKET000988745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>500MR000004NtV5YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988746</td>\n",
       "      <td>TICKET000988746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>500MR000004NtV6YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988747</td>\n",
       "      <td>TICKET000988747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>500MR000004NtV7YAK</td>\n",
       "      <td>Closed</td>\n",
       "      <td>TICKET000988748</td>\n",
       "      <td>TICKET000988748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>500MR000004NtV8YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988749</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>500MR000004NtV9YAK</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>500MR000004NtVAYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988751</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>500MR000004NtVBYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988752</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>500MR000004NtVCYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988753</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>500MR000004NtVDYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988754</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>500MR000004NtVEYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988755</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>500MR000004NtVFYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988756</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>500MR000004NtVGYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988757</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>500MR000004NtVHYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>500MR000004NtVIYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988759</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>500MR000004NtVJYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988760</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>500MR000004NtVKYA0</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>TICKET000988761</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID     STATUS       CASENUMBER SCC_LEGACY_TICKET_ID__C\n",
       "0    500MR000004NtSvYAK     Closed  TICKET000988612         TICKET000988612\n",
       "1    500MR000004NtSwYAK     Closed  TICKET000988613         TICKET000988613\n",
       "2    500MR000004NtSxYAK     Closed  TICKET000988614         TICKET000988614\n",
       "3    500MR000004NtSyYAK  Cancelled  TICKET000988615                     NaN\n",
       "4    500MR000004NtSzYAK  Cancelled  TICKET000988616                     NaN\n",
       "5    500MR000004NtT0YAK  Cancelled  TICKET000988617                     NaN\n",
       "6    500MR000004NtT1YAK  Cancelled  TICKET000988618                     NaN\n",
       "7    500MR000004NtT2YAK  Cancelled  TICKET000988619                     NaN\n",
       "8    500MR000004NtT3YAK  Cancelled  TICKET000988620                     NaN\n",
       "9    500MR000004NtT4YAK     Closed  TICKET000988621         TICKET000988621\n",
       "10   500MR000004NtT5YAK     Closed  TICKET000988622         TICKET000988622\n",
       "11   500MR000004NtT6YAK     Closed  TICKET000988623         TICKET000988623\n",
       "12   500MR000004NtT7YAK  Cancelled  TICKET000988624                     NaN\n",
       "13   500MR000004NtT8YAK  Cancelled  TICKET000988625                     NaN\n",
       "14   500MR000004NtT9YAK  Cancelled  TICKET000988626                     NaN\n",
       "15   500MR000004NtTAYA0  Cancelled  TICKET000988627                     NaN\n",
       "16   500MR000004NtTBYA0     Closed  TICKET000988628         TICKET000988628\n",
       "17   500MR000004NtTCYA0     Closed  TICKET000988629         TICKET000988629\n",
       "18   500MR000004NtTDYA0     Closed  TICKET000988630         TICKET000988630\n",
       "19   500MR000004NtTEYA0     Closed  TICKET000988631         TICKET000988631\n",
       "20   500MR000004NtTFYA0     Closed  TICKET000988632         TICKET000988632\n",
       "21   500MR000004NtTGYA0  Cancelled  TICKET000988633                     NaN\n",
       "22   500MR000004NtTHYA0  Cancelled  TICKET000988634                     NaN\n",
       "23   500MR000004NtTIYA0  Cancelled  TICKET000988635                     NaN\n",
       "24   500MR000004NtTJYA0  Cancelled  TICKET000988636                     NaN\n",
       "25   500MR000004NtTKYA0  Cancelled  TICKET000988637                     NaN\n",
       "26   500MR000004NtTLYA0  Cancelled  TICKET000988638                     NaN\n",
       "27   500MR000004NtTMYA0     Closed  TICKET000988639         TICKET000988639\n",
       "28   500MR000004NtTNYA0     Closed  TICKET000988640         TICKET000988640\n",
       "29   500MR000004NtTOYA0     Closed  TICKET000988641         TICKET000988641\n",
       "30   500MR000004NtTPYA0     Closed  TICKET000988642         TICKET000988642\n",
       "31   500MR000004NtTQYA0     Closed  TICKET000988643         TICKET000988643\n",
       "32   500MR000004NtTRYA0     Closed  TICKET000988644         TICKET000988644\n",
       "33   500MR000004NtTSYA0     Closed  TICKET000988645         TICKET000988645\n",
       "34   500MR000004NtTTYA0     Closed  TICKET000988646         TICKET000988646\n",
       "35   500MR000004NtTUYA0     Closed  TICKET000988647         TICKET000988647\n",
       "36   500MR000004NtTVYA0     Closed  TICKET000988648         TICKET000988648\n",
       "37   500MR000004NtTWYA0     Closed  TICKET000988649         TICKET000988649\n",
       "38   500MR000004NtTXYA0     Closed  TICKET000988650         TICKET000988650\n",
       "39   500MR000004NtTYYA0     Closed  TICKET000988651         TICKET000988651\n",
       "40   500MR000004NtTZYA0     Closed  TICKET000988652         TICKET000988652\n",
       "41   500MR000004NtTaYAK     Closed  TICKET000988653         TICKET000988653\n",
       "42   500MR000004NtTbYAK     Closed  TICKET000988654         TICKET000988654\n",
       "43   500MR000004NtTcYAK     Closed  TICKET000988655         TICKET000988655\n",
       "44   500MR000004NtTdYAK     Closed  TICKET000988656         TICKET000988656\n",
       "45   500MR000004NtTeYAK     Closed  TICKET000988657         TICKET000988657\n",
       "46   500MR000004NtTfYAK     Closed  TICKET000988658         TICKET000988658\n",
       "47   500MR000004NtTgYAK     Closed  TICKET000988659         TICKET000988659\n",
       "48   500MR000004NtThYAK     Closed  TICKET000988660         TICKET000988660\n",
       "49   500MR000004NtTiYAK     Closed  TICKET000988661         TICKET000988661\n",
       "50   500MR000004NtTjYAK     Closed  TICKET000988662         TICKET000988662\n",
       "51   500MR000004NtTkYAK     Closed  TICKET000988663         TICKET000988663\n",
       "52   500MR000004NtTlYAK     Closed  TICKET000988664         TICKET000988664\n",
       "53   500MR000004NtTmYAK     Closed  TICKET000988665         TICKET000988665\n",
       "54   500MR000004NtTnYAK     Closed  TICKET000988666         TICKET000988666\n",
       "55   500MR000004NtToYAK     Closed  TICKET000988667         TICKET000988667\n",
       "56   500MR000004NtTpYAK     Closed  TICKET000988668         TICKET000988668\n",
       "57   500MR000004NtTqYAK     Closed  TICKET000988669         TICKET000988669\n",
       "58   500MR000004NtTrYAK     Closed  TICKET000988670         TICKET000988670\n",
       "59   500MR000004NtTsYAK     Closed  TICKET000988671         TICKET000988671\n",
       "60   500MR000004NtTtYAK     Closed  TICKET000988672         TICKET000988672\n",
       "61   500MR000004NtTuYAK     Closed  TICKET000988673         TICKET000988673\n",
       "62   500MR000004NtTvYAK     Closed  TICKET000988674         TICKET000988674\n",
       "63   500MR000004NtTwYAK     Closed  TICKET000988675         TICKET000988675\n",
       "64   500MR000004NtTxYAK     Closed  TICKET000988676         TICKET000988676\n",
       "65   500MR000004NtTyYAK     Closed  TICKET000988677         TICKET000988677\n",
       "66   500MR000004NtTzYAK     Closed  TICKET000988678         TICKET000988678\n",
       "67   500MR000004NtU0YAK     Closed  TICKET000988679         TICKET000988679\n",
       "68   500MR000004NtU1YAK     Closed  TICKET000988680         TICKET000988680\n",
       "69   500MR000004NtU2YAK     Closed  TICKET000988681         TICKET000988681\n",
       "70   500MR000004NtU3YAK     Closed  TICKET000988682         TICKET000988682\n",
       "71   500MR000004NtU4YAK     Closed  TICKET000988683         TICKET000988683\n",
       "72   500MR000004NtU5YAK     Closed  TICKET000988684         TICKET000988684\n",
       "73   500MR000004NtU6YAK     Closed  TICKET000988685         TICKET000988685\n",
       "74   500MR000004NtU7YAK     Closed  TICKET000988686         TICKET000988686\n",
       "75   500MR000004NtU8YAK     Closed  TICKET000988687         TICKET000988687\n",
       "76   500MR000004NtU9YAK     Closed  TICKET000988688         TICKET000988688\n",
       "77   500MR000004NtUAYA0     Closed  TICKET000988689         TICKET000988689\n",
       "78   500MR000004NtUBYA0     Closed  TICKET000988690         TICKET000988690\n",
       "79   500MR000004NtUCYA0     Closed  TICKET000988691         TICKET000988691\n",
       "80   500MR000004NtUDYA0     Closed  TICKET000988692         TICKET000988692\n",
       "81   500MR000004NtUEYA0     Closed  TICKET000988693         TICKET000988693\n",
       "82   500MR000004NtUFYA0  Cancelled  TICKET000988694                     NaN\n",
       "83   500MR000004NtUGYA0  Cancelled  TICKET000988695                     NaN\n",
       "84   500MR000004NtUHYA0  Cancelled  TICKET000988696                     NaN\n",
       "85   500MR000004NtUIYA0  Cancelled  TICKET000988697                     NaN\n",
       "86   500MR000004NtUJYA0  Cancelled  TICKET000988698                     NaN\n",
       "87   500MR000004NtUKYA0  Cancelled  TICKET000988699                     NaN\n",
       "88   500MR000004NtULYA0     Closed  TICKET000988700         TICKET000988700\n",
       "89   500MR000004NtUMYA0     Closed  TICKET000988701         TICKET000988701\n",
       "90   500MR000004NtUNYA0     Closed  TICKET000988702         TICKET000988702\n",
       "91   500MR000004NtUOYA0     Closed  TICKET000988703         TICKET000988703\n",
       "92   500MR000004NtUPYA0     Closed  TICKET000988704         TICKET000988704\n",
       "93   500MR000004NtUQYA0     Closed  TICKET000988705         TICKET000988705\n",
       "94   500MR000004NtURYA0     Closed  TICKET000988706         TICKET000988706\n",
       "95   500MR000004NtUSYA0     Closed  TICKET000988707         TICKET000988707\n",
       "96   500MR000004NtUTYA0     Closed  TICKET000988708         TICKET000988708\n",
       "97   500MR000004NtUUYA0     Closed  TICKET000988709         TICKET000988709\n",
       "98   500MR000004NtUVYA0     Closed  TICKET000988710         TICKET000988710\n",
       "99   500MR000004NtUWYA0     Closed  TICKET000988711         TICKET000988711\n",
       "100  500MR000004NtUXYA0     Closed  TICKET000988712         TICKET000988712\n",
       "101  500MR000004NtUYYA0     Closed  TICKET000988713         TICKET000988713\n",
       "102  500MR000004NtUZYA0     Closed  TICKET000988714         TICKET000988714\n",
       "103  500MR000004NtUaYAK     Closed  TICKET000988715         TICKET000988715\n",
       "104  500MR000004NtUbYAK     Closed  TICKET000988716         TICKET000988716\n",
       "105  500MR000004NtUcYAK     Closed  TICKET000988717         TICKET000988717\n",
       "106  500MR000004NtUdYAK     Closed  TICKET000988718         TICKET000988718\n",
       "107  500MR000004NtUeYAK  Cancelled  TICKET000988719                     NaN\n",
       "108  500MR000004NtUfYAK  Cancelled  TICKET000988720                     NaN\n",
       "109  500MR000004NtUgYAK  Cancelled  TICKET000988721                     NaN\n",
       "110  500MR000004NtUhYAK  Cancelled  TICKET000988722                     NaN\n",
       "111  500MR000004NtUiYAK  Cancelled  TICKET000988723                     NaN\n",
       "112  500MR000004NtUjYAK  Cancelled  TICKET000988724                     NaN\n",
       "113  500MR000004NtUkYAK     Closed  TICKET000988725         TICKET000988725\n",
       "114  500MR000004NtUlYAK     Closed  TICKET000988726         TICKET000988726\n",
       "115  500MR000004NtUmYAK     Closed  TICKET000988727         TICKET000988727\n",
       "116  500MR000004NtUnYAK     Closed  TICKET000988728         TICKET000988728\n",
       "117  500MR000004NtUoYAK     Closed  TICKET000988729         TICKET000988729\n",
       "118  500MR000004NtUpYAK     Closed  TICKET000988730         TICKET000988730\n",
       "119  500MR000004NtUqYAK  Cancelled  TICKET000988731                     NaN\n",
       "120  500MR000004NtUrYAK  Cancelled  TICKET000988732                     NaN\n",
       "121  500MR000004NtUsYAK  Cancelled  TICKET000988733                     NaN\n",
       "122  500MR000004NtUtYAK  Cancelled  TICKET000988734                     NaN\n",
       "123  500MR000004NtUuYAK  Cancelled  TICKET000988735                     NaN\n",
       "124  500MR000004NtUvYAK  Cancelled  TICKET000988736                     NaN\n",
       "125  500MR000004NtUwYAK  Cancelled  TICKET000988737                     NaN\n",
       "126  500MR000004NtUxYAK  Cancelled  TICKET000988738                     NaN\n",
       "127  500MR000004NtUyYAK  Cancelled  TICKET000988739                     NaN\n",
       "128  500MR000004NtUzYAK     Closed  TICKET000988740         TICKET000988740\n",
       "129  500MR000004NtV0YAK     Closed  TICKET000988741         TICKET000988741\n",
       "130  500MR000004NtV1YAK     Closed  TICKET000988742         TICKET000988742\n",
       "131  500MR000004NtV2YAK     Closed  TICKET000988743         TICKET000988743\n",
       "132  500MR000004NtV3YAK     Closed  TICKET000988744         TICKET000988744\n",
       "133  500MR000004NtV4YAK     Closed  TICKET000988745         TICKET000988745\n",
       "134  500MR000004NtV5YAK     Closed  TICKET000988746         TICKET000988746\n",
       "135  500MR000004NtV6YAK     Closed  TICKET000988747         TICKET000988747\n",
       "136  500MR000004NtV7YAK     Closed  TICKET000988748         TICKET000988748\n",
       "137  500MR000004NtV8YAK  Cancelled  TICKET000988749                     NaN\n",
       "138  500MR000004NtV9YAK  Cancelled  TICKET000988750                     NaN\n",
       "139  500MR000004NtVAYA0  Cancelled  TICKET000988751                     NaN\n",
       "140  500MR000004NtVBYA0  Cancelled  TICKET000988752                     NaN\n",
       "141  500MR000004NtVCYA0  Cancelled  TICKET000988753                     NaN\n",
       "142  500MR000004NtVDYA0  Cancelled  TICKET000988754                     NaN\n",
       "143  500MR000004NtVEYA0  Cancelled  TICKET000988755                     NaN\n",
       "144  500MR000004NtVFYA0  Cancelled  TICKET000988756                     NaN\n",
       "145  500MR000004NtVGYA0  Cancelled  TICKET000988757                     NaN\n",
       "146  500MR000004NtVHYA0  Cancelled  TICKET000988758                     NaN\n",
       "147  500MR000004NtVIYA0  Cancelled  TICKET000988759                     NaN\n",
       "148  500MR000004NtVJYA0  Cancelled  TICKET000988760                     NaN\n",
       "149  500MR000004NtVKYA0  Cancelled  TICKET000988761                     NaN"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the extracted data from Salesforce after being updated \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\case_150.csv\"\n",
    "\n",
    "df=pd.read_csv(path, delimiter=';')\n",
    "df2 = df[['ID', 'STATUS','CASENUMBER', 'SCC_LEGACY_TICKET_ID__C']]\n",
    "\n",
    "# df3 = df2[df2['SCC_LEGACY_TICKET_ID__C'].notna()]\n",
    "df2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Supported Objects in Queue Name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First Extract all the Queue ID from Group Object, you may use this query:\n",
    "# SELECT Id\n",
    "# where Type ='Queue'\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_group_queueid.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# add one column 'SObjectType' = Case, see the example below:\n",
    "# QueueId,SObjectType\n",
    "# 00GMR0000000trT2AQ,Case\n",
    "\n",
    "df['SObjectType']='Case'\n",
    "df.to_csv('extract_group_queueid.csv', index=False)\n",
    "\n",
    "# Go to Data Loader 'insert' and choose  'QueueSobject' and insert the file with two columns: Id and SObjectType = Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['ERROR'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaste\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataloader_v60.0.2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124merror062124041536955.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[1;32m----> 8\u001b[0m df\u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mERROR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR.1\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['ERROR'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error062124041536955.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df= df.drop('ERROR', axis=1)\n",
    "df= df.drop('ERROR.1', axis=1)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifno</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>Nominal</th>\n",
       "      <th>...</th>\n",
       "      <th>attachment_done</th>\n",
       "      <th>email</th>\n",
       "      <th>full_name</th>\n",
       "      <th>no_telepon</th>\n",
       "      <th>approver_login</th>\n",
       "      <th>approver_name</th>\n",
       "      <th>SLAResolution</th>\n",
       "      <th>submitter_login_id</th>\n",
       "      <th>submitter_user_group</th>\n",
       "      <th>user_login_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LD86852</td>\n",
       "      <td>TTB000002901285</td>\n",
       "      <td>8412.0</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b2714defcfef8c65d3154367beb604aa</td>\n",
       "      <td>40dc6606c2b863ebe08fcb22840cc9b8</td>\n",
       "      <td>23300800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RCAW297</td>\n",
       "      <td>TTB000001778408</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>Nasabah BRI gagal tarik tunai &amp; terdebet di AT...</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b69e3868bc8b6c3231df7d691a5595fe</td>\n",
       "      <td>9f608bd9950ab2da8b30f77130f0685f</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AOY4038</td>\n",
       "      <td>TTB000001776805</td>\n",
       "      <td>8412.0</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>948421d448fb51f1cf27584b1256a120</td>\n",
       "      <td>c8ba7ab42dd55eb9f6b04c7c0215c06d</td>\n",
       "      <td>246300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUA0263</td>\n",
       "      <td>TTB000002020684</td>\n",
       "      <td>8437.0</td>\n",
       "      <td>Nasabah BRI gagal transfer &amp; terdebet jaringan...</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>a4aa532c65d1503dcc41ef396d3c42b8</td>\n",
       "      <td>9ea2aedad78a4ceb57f234085797d272</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOL2556</td>\n",
       "      <td>TTB000025168832</td>\n",
       "      <td>8405.0</td>\n",
       "      <td>Kartu ATM BRI Tertelan di MESIN ATM</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>95d4f5421b5b387726b5d619dbe24155</td>\n",
       "      <td>8a69534ee0c414338ccdb16d5d485d57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90119221.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>RBU6378</td>\n",
       "      <td>TTB000025135298</td>\n",
       "      <td>8808.0</td>\n",
       "      <td>Pembayaran PLN Prabayar Gagal namun Saldo Terd...</td>\n",
       "      <td>2020-08-06 12:11:29</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>69c21efc4229135b07ec284b5fe00b75</td>\n",
       "      <td>70a10671ae2665c0341ea6edeebc9829</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a1e6cd3b223c1edfd8b14e1078172783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90105230.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>a1e6cd3b223c1edfd8b14e1078172783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>MVM7576</td>\n",
       "      <td>TTB000002448185</td>\n",
       "      <td>8439.0</td>\n",
       "      <td>Nasabah BRI gagal transfer &amp; terdebet jaringan...</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>fff60542f6bed23d51f2cd25532002ca</td>\n",
       "      <td>4377f81f335e83f9b29f8cbc46b573a8</td>\n",
       "      <td>2000023.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>EY38563</td>\n",
       "      <td>TTB000002219828</td>\n",
       "      <td>8410.0</td>\n",
       "      <td>Kegagalan TAC / Transfer Lainnya Lewat BRI</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>c63b1ad897ef8e5534c9ad341086d302</td>\n",
       "      <td>c54de8529029870946d1ddc8d818f6fd</td>\n",
       "      <td>4000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>RJN5271</td>\n",
       "      <td>TTB000025223346</td>\n",
       "      <td>8701.0</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>2020-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>d66c9a37d803cdac680b06ef894fbe98</td>\n",
       "      <td>a87adddeab253a1f8bcbbc5616c264db</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5067f8899729a5bf019cbaff71c5c882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60395.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>5067f8899729a5bf019cbaff71c5c882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>E773599</td>\n",
       "      <td>TTB000025125302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SMS Banking</td>\n",
       "      <td>2020-08-06 12:11:29</td>\n",
       "      <td>Phone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7499d8695b22b281f5a8f562eadaeb4b</td>\n",
       "      <td>8bbd8fb06ae7e1de5a0b111fa49396df</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b39aaa9d4a4fa545cf8eb8774e8f5a56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60267.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>b39aaa9d4a4fa545cf8eb8774e8f5a56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cifno        Ticket_ID  Call_Type_ID  \\\n",
       "0      LD86852  TTB000002901285        8412.0   \n",
       "1      RCAW297  TTB000001778408        8812.0   \n",
       "2      AOY4038  TTB000001776805        8412.0   \n",
       "3      AUA0263  TTB000002020684        8437.0   \n",
       "4      HOL2556  TTB000025168832        8405.0   \n",
       "...        ...              ...           ...   \n",
       "99995  RBU6378  TTB000025135298        8808.0   \n",
       "99996  MVM7576  TTB000002448185        8439.0   \n",
       "99997  EY38563  TTB000002219828        8410.0   \n",
       "99998  RJN5271  TTB000025223346        8701.0   \n",
       "99999  E773599  TTB000025125302           NaN   \n",
       "\n",
       "                                               Call_Type          Create_Date  \\\n",
       "0      Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2020-08-06 16:52:14   \n",
       "1      Nasabah BRI gagal tarik tunai & terdebet di AT...  2020-08-06 16:52:14   \n",
       "2      Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2020-08-06 16:52:14   \n",
       "3      Nasabah BRI gagal transfer & terdebet jaringan...  2020-08-06 16:52:14   \n",
       "4                    Kartu ATM BRI Tertelan di MESIN ATM  2020-08-06 16:52:14   \n",
       "...                                                  ...                  ...   \n",
       "99995  Pembayaran PLN Prabayar Gagal namun Saldo Terd...  2020-08-06 12:11:29   \n",
       "99996  Nasabah BRI gagal transfer & terdebet jaringan...  2020-08-06 16:52:14   \n",
       "99997         Kegagalan TAC / Transfer Lainnya Lewat BRI  2020-08-06 16:52:14   \n",
       "99998               Blokir Kartu ATM karena kartu hilang  2020-08-06 16:52:14   \n",
       "99999                                        SMS Banking  2020-08-06 12:11:29   \n",
       "\n",
       "      gateway            Jenis_Laporan                      Nama_Nasabah  \\\n",
       "0       Phone  Complaint - Transaction  b2714defcfef8c65d3154367beb604aa   \n",
       "1       Phone  Complaint - Transaction  b69e3868bc8b6c3231df7d691a5595fe   \n",
       "2       Phone  Complaint - Transaction  948421d448fb51f1cf27584b1256a120   \n",
       "3       Phone  Complaint - Transaction  a4aa532c65d1503dcc41ef396d3c42b8   \n",
       "4       Phone                  Request  95d4f5421b5b387726b5d619dbe24155   \n",
       "...       ...                      ...                               ...   \n",
       "99995   Phone  Complaint - Transaction  69c21efc4229135b07ec284b5fe00b75   \n",
       "99996   Phone  Complaint - Transaction  fff60542f6bed23d51f2cd25532002ca   \n",
       "99997   Phone  Complaint - Transaction  c63b1ad897ef8e5534c9ad341086d302   \n",
       "99998   Phone                  Request  d66c9a37d803cdac680b06ef894fbe98   \n",
       "99999   Phone                      NaN  7499d8695b22b281f5a8f562eadaeb4b   \n",
       "\n",
       "                            No_Rekening     Nominal  ... attachment_done  \\\n",
       "0      40dc6606c2b863ebe08fcb22840cc9b8  23300800.0  ...             NaN   \n",
       "1      9f608bd9950ab2da8b30f77130f0685f   1000000.0  ...             NaN   \n",
       "2      c8ba7ab42dd55eb9f6b04c7c0215c06d    246300.0  ...             NaN   \n",
       "3      9ea2aedad78a4ceb57f234085797d272    100000.0  ...             NaN   \n",
       "4      8a69534ee0c414338ccdb16d5d485d57         0.0  ...             NaN   \n",
       "...                                 ...         ...  ...             ...   \n",
       "99995  70a10671ae2665c0341ea6edeebc9829    200000.0  ...             NaN   \n",
       "99996  4377f81f335e83f9b29f8cbc46b573a8   2000023.0  ...             NaN   \n",
       "99997  c54de8529029870946d1ddc8d818f6fd   4000000.0  ...             NaN   \n",
       "99998  a87adddeab253a1f8bcbbc5616c264db         0.0  ...             NaN   \n",
       "99999  8bbd8fb06ae7e1de5a0b111fa49396df         0.0  ...             NaN   \n",
       "\n",
       "      email                         full_name no_telepon approver_login  \\\n",
       "0       NaN  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "1       NaN  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "2       NaN  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "3       NaN  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "4       NaN  fcc2dc136c7c3d69fb080cdd2b1648f1        NaN            NaN   \n",
       "...     ...                               ...        ...            ...   \n",
       "99995   NaN  a1e6cd3b223c1edfd8b14e1078172783        NaN            NaN   \n",
       "99996   NaN  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "99997   NaN  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "99998   NaN  5067f8899729a5bf019cbaff71c5c882        NaN            NaN   \n",
       "99999   NaN  b39aaa9d4a4fa545cf8eb8774e8f5a56        NaN            NaN   \n",
       "\n",
       "                          approver_name SLAResolution  submitter_login_id  \\\n",
       "0      d41d8cd98f00b204e9800998ecf8427e          20.0                 NaN   \n",
       "1      d41d8cd98f00b204e9800998ecf8427e          10.0                 NaN   \n",
       "2      d41d8cd98f00b204e9800998ecf8427e          20.0                 NaN   \n",
       "3      d41d8cd98f00b204e9800998ecf8427e          20.0                 NaN   \n",
       "4      d41d8cd98f00b204e9800998ecf8427e          20.0          90119221.0   \n",
       "...                                 ...           ...                 ...   \n",
       "99995  d41d8cd98f00b204e9800998ecf8427e          20.0          90105230.0   \n",
       "99996  d41d8cd98f00b204e9800998ecf8427e          20.0                 NaN   \n",
       "99997  d41d8cd98f00b204e9800998ecf8427e          10.0                 NaN   \n",
       "99998  d41d8cd98f00b204e9800998ecf8427e          20.0             60395.0   \n",
       "99999  d41d8cd98f00b204e9800998ecf8427e          20.0             60267.0   \n",
       "\n",
       "       submitter_user_group                   user_login_name  \n",
       "0                       NaN  d41d8cd98f00b204e9800998ecf8427e  \n",
       "1                       NaN  d41d8cd98f00b204e9800998ecf8427e  \n",
       "2                       NaN  d41d8cd98f00b204e9800998ecf8427e  \n",
       "3                       NaN  d41d8cd98f00b204e9800998ecf8427e  \n",
       "4               LCC-CCTCALL  fcc2dc136c7c3d69fb080cdd2b1648f1  \n",
       "...                     ...                               ...  \n",
       "99995           LCC-CCTCALL  a1e6cd3b223c1edfd8b14e1078172783  \n",
       "99996                   NaN  d41d8cd98f00b204e9800998ecf8427e  \n",
       "99997                   NaN  d41d8cd98f00b204e9800998ecf8427e  \n",
       "99998           LCC-CCTCALL  5067f8899729a5bf019cbaff71c5c882  \n",
       "99999           LCC-CCTCALL  b39aaa9d4a4fa545cf8eb8774e8f5a56  \n",
       "\n",
       "[100000 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_case (2)\\bricare_20200806_20240430_0_27.csv\"\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

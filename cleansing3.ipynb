{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script for data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRICARE:\n",
    "\n",
    "BRICARE consists of 2 different types of files by year:\n",
    "\n",
    "a. File after 2022 (2023-2024) = 79 kolom\n",
    "\n",
    "\n",
    "b. File before 2022 (2019-2022) = 27 kolom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type A\n",
    "\n",
    "\n",
    "Data Extraction for File Type A must be 2 Files:\n",
    "\n",
    "\n",
    "A.1 Columns (without \"Details\")\n",
    "\n",
    "\n",
    "A.2 Details only \n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['Create_Date','TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\"\n",
    "\n",
    "- Columns shoud be mapped based on their Call_Type_ID:\n",
    "\n",
    "['Produk','Jenis_Produk','Jenis_Laporan']\n",
    "\n",
    "- PLEASE ADD CIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20230101_20230101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\1840199040.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('NULL', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\1840199040.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('None', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\1840199040.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_file(file_path):\n",
    "\n",
    "    data = []\n",
    "    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "\n",
    "            date_index = next(i for i, part in enumerate(parts) if date_pattern.match(part))\n",
    "\n",
    "            ticket_id = parts[0] \n",
    "            call_type_id = parts[1]  \n",
    "            description = ';'.join(parts[2:date_index])  \n",
    "            create_date = parts[date_index]  \n",
    "\n",
    "      \n",
    "            data.append([ticket_id, call_type_id, description, create_date] + parts[date_index + 1:])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.txt\"\n",
    "\n",
    "df = parse_file(file_path)\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "   \n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\3559677339.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17888\\3559677339.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bricare_csv20230101_20230101.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if len(parts) > 78:\n",
    "                description = ';'.join(parts[2:-75])\n",
    "                new_parts = parts[:2] + [description] + parts[-75:]\n",
    "                data.append(new_parts)\n",
    "            else:\n",
    "                data.append(parts)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    df = df.replace(['0', 0], '')\n",
    "\n",
    "    columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "    for column in columns_to_convert:\n",
    "        df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "    \n",
    "    df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "    return df\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.csv\"\n",
    "df = parse_csv(file_path)\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_csv{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleasing the master Call Type file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_24036\\2288781540.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "master_df_path = r\"C:\\Users\\maste\\Downloads\\bricare\\(REVISED) SLA-OLA_NewUserGrouping_Ringkasan Kirim ME Versi 1.6.csv\"\n",
    "df = pd.read_csv(master_df_path, sep=';')\n",
    "\n",
    "\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "df = df.dropna(how='all')\n",
    "df.iloc[:450]\n",
    "df.to_csv(\"master_calltype.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call type mapping for columns 'Produk', 'Jenis Produk', 'Jenis Laporan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "user_dataset_path = r\"D:\\dataquality\\bricare_csv20230101_20230101.csv\"\n",
    "user_df = pd.read_csv(user_dataset_path)\n",
    "master_df_path = r\"D:\\dataquality\\master_calltype.csv\"\n",
    "master_df = pd.read_csv(master_df_path)\n",
    "\n",
    "\n",
    "master_df = master_df.rename(columns={\n",
    "    'Case Types': 'Call_Type_ID', \n",
    "    'Product': 'Produk', \n",
    "    'Sub Product': 'Jenis_Produk', \n",
    "    'Case Category': 'Jenis_Laporan'\n",
    "})\n",
    "\n",
    "\n",
    "user_df['Call_Type_ID'] = user_df['Call_Type_ID'].astype(str)\n",
    "master_df['Call_Type_ID'] = master_df['Call_Type_ID'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(user_df, master_df[['Call_Type_ID', 'Produk', 'Jenis_Produk', 'Jenis_Laporan']], on='Call_Type_ID', how='left')\n",
    "\n",
    "user_df['Produk'] = merged_df['Produk_y']\n",
    "user_df['Jenis_Produk'] = merged_df['Jenis_Produk_y']\n",
    "user_df['Jenis_Laporan'] = merged_df['Jenis_Laporan_y']\n",
    "\n",
    "user_df.to_csv(user_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_text_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:  \n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]  \n",
    "                current_entry.append(parts[3].strip())  \n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def remove_bom_and_strip(df):\n",
    "    return df.applymap(lambda x: x.replace('\\ufeff', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.txt\"\n",
    "processed_data = process_text_data(file_path)\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "df_final=df_final.iloc[:10]\n",
    "df_final.iloc[:10].to_csv('details_20230101_20230101.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to details_csv20230101_20230101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:\n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]\n",
    "                current_entry.append(parts[3].strip())\n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.csv\"\n",
    "processed_data = process_csv_data(file_path)\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "\n",
    "df_final = df_final.iloc[:10]\n",
    "output_path = \"details_csv20230101_20230101.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the file A.1 and file A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#just take 10 lines for an example\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.iloc[:10].to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path_1 = r\"D:\\dataquality\\bricare_csv20230101_20230101.csv\"\n",
    "file_path_2 = r\"D:\\dataquality\\details_csv20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "df_tenline_bricare = pd.read_csv(file_path_1)\n",
    "df_detail_bricare_10line = pd.read_csv(file_path_2)\n",
    "\n",
    "df_detail_bricare_10line.columns = ['Ticket_ID', 'Details']\n",
    "\n",
    "merged_df = pd.merge(df_tenline_bricare, df_detail_bricare_10line, on='Ticket_ID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r\"D:\\dataquality\\bricare_csv20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "column_to_move=\"Details\"\n",
    "merged_df = merged_df[[col for col in merged_df if col != column_to_move][:3] + [column_to_move] + [col for col in merged_df if col != column_to_move][3:]] \n",
    "\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type B\n",
    "\n",
    "\n",
    "Data Extraction for File Type B (27 columns)\n",
    "\n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['TanggalClosed', 'tanggalTransaksi','Create_Date']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\" if any\n",
    "\n",
    "- PLEASE ADD CIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_33500\\3452762791.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20200101_20200101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\BRICARE_25042024 masking.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "\n",
    "data['Column1'] = data['Column1'].astype(str)\n",
    "data_cleaned = data[data['Column1'].str.match(r'TTB\\d+')]\n",
    "\n",
    "data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n",
    "data_cleaned = data_cleaned[data_cleaned['Column2'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "data_cleaned['Column4'] = pd.to_datetime(data_cleaned['Column4'], errors='coerce')\n",
    "data_cleaned = data_cleaned.dropna(subset=['Column4'])\n",
    "\n",
    "\n",
    "data_to_drop = ['Column28', 'Column29', 'Column30', 'Column31', 'Column32']\n",
    "data_cleaned = data_cleaned.drop(columns=data_to_drop)\n",
    "\n",
    "\n",
    "if len(data_cleaned.columns) <= len(column_list):\n",
    "    data_cleaned.columns = column_list[:len(data_cleaned.columns)]\n",
    "\n",
    "\n",
    "data_cleaned.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "data_cleaned.fillna('', inplace=True)\n",
    "data_cleaned = data_cleaned.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    data_cleaned[column] = pd.to_datetime(data_cleaned[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    data_cleaned[column] = data_cleaned[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "# Just take 10 lines for an example\n",
    "data_cleaned = data_cleaned.iloc[:10]\n",
    "\n",
    "\n",
    "data_cleaned.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(data_cleaned['Create_Date']))\n",
    "enddate = pd.Timestamp(max(data_cleaned['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "data_cleaned.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "27\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Details', 'Create_Date',\n",
      "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
      "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
      "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
      "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
      "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
      "       'user_login_name', 'Jenis_Produk', 'Last_Modified_By', 'Merchant_ID',\n",
      "       'Modified_Date', 'NOTAS', 'Produk', 'SLA_Status', 'TID',\n",
      "       'tanggalAttachmentDone', 'Tgl_Assigned', 'Tgl_Eskalasi', 'AnalisaSkils',\n",
      "       'Attachment_', 'Bank_BRI', 'Biaya_Admin', 'Suku_Bunga', 'Bunga',\n",
      "       'Butuh_Attachment', 'Cicilan', 'Hasil_Kunjungan', 'Log_Name',\n",
      "       'MMS_Ticket_Id', 'Mass_Ticket_Upload_Flag', 'Nama_Supervisor',\n",
      "       'Nama_TL', 'Nama_Wakabag', 'Nasabah_Prioritas', 'Notify_By',\n",
      "       'Organization', 'Output_Settlement', 'phone_survey', 'Return_Ticket',\n",
      "       'Settlement_By', 'Settlement_ID', 'Settlement', 'Site_User',\n",
      "       'Status_Return', 'Status_Transaksi', 'Submitter_Region',\n",
      "       'Submitter_SiteGroup', 'Submitter_User_group_ID', 'Tanggal_Settlement',\n",
      "       'Tgl_Foward', 'Tgl_In_Progress', 'Tgl_Returned', 'Ticket_Referensi',\n",
      "       'Tiket_Urgency', 'Tipe_Remark', 'UniqueID', 'users', 'Usergroup_ID'],\n",
      "      dtype='object')\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date', 'gateway',\n",
      "       'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal', 'status',\n",
      "       'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur', 'Nomor_Kartu',\n",
      "       'user_group', 'assgined_to', 'attachment_done', 'email', 'full_name',\n",
      "       'no_telepon', 'approver_login', 'approver_name', 'SLAResolution',\n",
      "       'submitter_login_id', 'submitter_user_group', 'user_login_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path1=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "path2=r\"D:\\dataquality\\bricare_20200101_20200101.csv\"\n",
    "\n",
    "df1=pd.read_csv(path1)\n",
    "df2=pd.read_csv(path2)\n",
    "print(len(df1.columns))\n",
    "print(len(df2.columns))\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Compare two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape_match': True,\n",
       " 'columns_match': True,\n",
       " 'column_differences': {},\n",
       " 'value_differences': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the newly uploaded files for detailed comparison\n",
    "processed_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "expected_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "# Read the files\n",
    "processed_df_newest = pd.read_csv(processed_file_path_newest)\n",
    "expected_df_newest = pd.read_csv(expected_file_path_newest)\n",
    "\n",
    "# Check for exact match first\n",
    "exact_match = processed_df_newest.equals(expected_df_newest)\n",
    "\n",
    "# Initialize a dictionary to store detailed comparison results\n",
    "comparison_details = {\n",
    "    'shape_match': processed_df_newest.shape == expected_df_newest.shape,\n",
    "    'columns_match': processed_df_newest.columns.equals(expected_df_newest.columns),\n",
    "    'column_differences': {},\n",
    "    'value_differences': {}\n",
    "}\n",
    "\n",
    "# Compare each aspect if exact match is not true\n",
    "if not exact_match:\n",
    "    # Check shape\n",
    "    if not comparison_details['shape_match']:\n",
    "        comparison_details['shape_details'] = {\n",
    "            'processed_shape': processed_df_newest.shape,\n",
    "            'expected_shape': expected_df_newest.shape\n",
    "        }\n",
    "    \n",
    "    # Check columns\n",
    "    if not comparison_details['columns_match']:\n",
    "        comparison_details['column_details'] = {\n",
    "            'processed_columns': processed_df_newest.columns.tolist(),\n",
    "            'expected_columns': expected_df_newest.columns.tolist()\n",
    "        }\n",
    "\n",
    "    # Check column-by-column values\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            comparison_details['column_differences'][column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "\n",
    "# Summarize value differences\n",
    "if not comparison_details['columns_match']:\n",
    "    value_differences = {}\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            value_differences[column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "    comparison_details['value_differences'] = value_differences\n",
    "\n",
    "comparison_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To collect all Error logs in a path\n",
    "\n",
    "directory_path is the error logs path as well as the location where the combined error log file will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined error logs saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_error_logs(directory_path):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.startswith(\"error\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Normalize the 'TICKET_ID' to ensure duplicates are identified\n",
    "    combined_df['TICKET_ID'] = combined_df['TICKET_ID'].str.upper()\n",
    "\n",
    "    combined_df = combined_df.drop_duplicates(subset='TICKET_ID')\n",
    "\n",
    "    output_path = os.path.join(directory_path, 'error_logs.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Combined error logs saved to: {output_path}\")\n",
    "    return output_path, combined_df\n",
    "\n",
    "directory_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\"\n",
    "output_path, combined_df = combine_error_logs(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To cleanse user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_43760\\4048578058.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import html\n",
    "\n",
    "\n",
    "# Function to decode HTML entities in a DataFrame\n",
    "def decode_html_entities(df):\n",
    "    df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "    return df_decoded\n",
    "\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\USER.txt\" \n",
    "\n",
    "# Read the raw file content\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_data = file.readlines()\n",
    "    \n",
    "# Split headers and data    \n",
    "header = raw_data[0].replace(\"&#124;\", \"|\").strip()\n",
    "data = [line.replace(\"&#124;\", \"|\").strip() for line in raw_data[1:]]\n",
    "\n",
    "# Read the cleaned data into a pandas DataFrame\n",
    "df_cleaned = pd.DataFrame([line.split('|') for line in data], columns=header.split('|'))\n",
    "\n",
    "# Decode HTML entities\n",
    "df_cleaned = decode_html_entities(df_cleaned)\n",
    "df_cleaned\n",
    "\n",
    "# Remove the quotes in Dataframe\n",
    "def remove_quotes(df):\n",
    "    df.columns = df.columns.str.replace('\"', '')\n",
    "    df = df.apply(lambda col: col.str.replace('\"', '', regex=True))\n",
    "    return df\n",
    "\n",
    "df_cleaned = remove_quotes(df_cleaned)\n",
    "df_cleaned\n",
    "df_cleaned.to_csv(\"user_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on real-like data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_31356\\1628187349.py:7: DtypeWarning: Columns (1,8,17,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep=',', on_bad_lines='skip')  # Skip bad lines\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TTB000026204763</th>\n",
       "      <th>8425</th>\n",
       "      <th>Pen-delete-an Status Registrasi Layanan yang Ada di BRI</th>\n",
       "      <th>2020-01-01 07:19:37.000</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Maintenance</th>\n",
       "      <th>MERSY ANGELIA ELISAB</th>\n",
       "      <th>021201053488501</th>\n",
       "      <th>0.00</th>\n",
       "      <th>Closed</th>\n",
       "      <th>...</th>\n",
       "      <th>NULL.2</th>\n",
       "      <th>NULL.3</th>\n",
       "      <th>Really Artha Ully Manik</th>\n",
       "      <th>082158717189</th>\n",
       "      <th>NULL.4</th>\n",
       "      <th>NULL.5</th>\n",
       "      <th>20</th>\n",
       "      <th>90136590</th>\n",
       "      <th>NULL.6</th>\n",
       "      <th>Really Artha Ully Manik.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000026204728</td>\n",
       "      <td>8405.0</td>\n",
       "      <td>Kartu ATM BRI Tertelan di MESIN ATM</td>\n",
       "      <td>2020-01-01 07:19:30.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Information</td>\n",
       "      <td>I NYOMAN SAJA SAPUTRA</td>\n",
       "      <td>106701004427504</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DELLA LARASSARI</td>\n",
       "      <td>085238111553</td>\n",
       "      <td>90022934</td>\n",
       "      <td>Adhi Nitidharma</td>\n",
       "      <td>20</td>\n",
       "      <td>90135196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DELLA LARASSARI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000026204747</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>Informasi Product Banking</td>\n",
       "      <td>2020-01-01 07:19:27.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Information</td>\n",
       "      <td>ENY INTI SURYANI</td>\n",
       "      <td>009801126221505</td>\n",
       "      <td>741700.00</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kartika Fitriani</td>\n",
       "      <td>081369386737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>90141079</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>Kartika Fitriani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000026204659</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>Nasabah BRI gagal tarik tunai &amp; terdebet di AT...</td>\n",
       "      <td>2020-01-01 07:00:34.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Information</td>\n",
       "      <td>ANDI GUSMANTO</td>\n",
       "      <td>504601012686530</td>\n",
       "      <td>1000000.00</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMELIA RAHMADANI</td>\n",
       "      <td>081242958898</td>\n",
       "      <td>00000723</td>\n",
       "      <td>Ismail</td>\n",
       "      <td>10</td>\n",
       "      <td>60443</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>AMELIA RAHMADANI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000026204577</td>\n",
       "      <td>8202.0</td>\n",
       "      <td>Informasi Product Banking</td>\n",
       "      <td>2020-01-01 07:09:29.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Information</td>\n",
       "      <td>SEPTIVA HERLIN ARTAT</td>\n",
       "      <td>054401000469567</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bacuelkueh@gmail.com</td>\n",
       "      <td>Amalia Fitriana</td>\n",
       "      <td>082127363449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>60422</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>Amalia Fitriana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000026204719</td>\n",
       "      <td>8410.0</td>\n",
       "      <td>Kegagalan TAC / Transfer Lainnya Lewat BRI</td>\n",
       "      <td>2020-01-01 07:07:31.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Information</td>\n",
       "      <td>MHD INDANA FARHAN S</td>\n",
       "      <td>530001017365535</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nuriah</td>\n",
       "      <td>085761011162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>90137097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nuriah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361887</th>\n",
       "      <td>TTB000026715685</td>\n",
       "      <td>8411</td>\n",
       "      <td>Salah Transfer antar BRI</td>\n",
       "      <td>2020-01-31 19:32:26.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complain</td>\n",
       "      <td>HERIWATI</td>\n",
       "      <td>546501003139532</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADE SUTISNA</td>\n",
       "      <td>082387480456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>90135194</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>ADE SUTISNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361888</th>\n",
       "      <td>TTB000026713843</td>\n",
       "      <td>8411</td>\n",
       "      <td>Salah Transfer antar BRI</td>\n",
       "      <td>2020-01-31 17:28:34.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complain</td>\n",
       "      <td>MUTIAH</td>\n",
       "      <td>609401002283508</td>\n",
       "      <td>1950000.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Umar Fahruddin Pratama</td>\n",
       "      <td>082136107896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>90135689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Umar Fahruddin Pratama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361889</th>\n",
       "      <td>TTB000026714586</td>\n",
       "      <td>7700</td>\n",
       "      <td>Komplain Transaksi Kartu Kredit tidak di akui</td>\n",
       "      <td>2020-01-31 18:24:32.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complain</td>\n",
       "      <td>MULIYA HARDIYANTO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cintia Fadila</td>\n",
       "      <td>05264513380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>90123773</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>Cintia Fadila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361890</th>\n",
       "      <td>TTB000026713292</td>\n",
       "      <td>8411</td>\n",
       "      <td>Salah Transfer antar BRI</td>\n",
       "      <td>2020-01-31 16:56:02.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complain</td>\n",
       "      <td>S I M O N</td>\n",
       "      <td>064201002986507</td>\n",
       "      <td>9650000.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alfera Dyah Pangestu</td>\n",
       "      <td>081241313888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>90138706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alfera Dyah Pangestu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361891</th>\n",
       "      <td>TTB000026717333</td>\n",
       "      <td>8411</td>\n",
       "      <td>Salah Transfer antar BRI</td>\n",
       "      <td>2020-01-31 22:08:12.000</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complain</td>\n",
       "      <td>AGUS SUPRAPTO</td>\n",
       "      <td>601401026226534</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUHAMMAD ARHANDY KOES NANDA</td>\n",
       "      <td>081228501106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>60477</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>MUHAMMAD ARHANDY KOES NANDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361892 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TTB000026204763    8425  \\\n",
       "0       TTB000026204728  8405.0   \n",
       "1       TTB000026204747  8202.0   \n",
       "2       TTB000026204659  8812.0   \n",
       "3       TTB000026204577  8202.0   \n",
       "4       TTB000026204719  8410.0   \n",
       "...                 ...     ...   \n",
       "361887  TTB000026715685    8411   \n",
       "361888  TTB000026713843    8411   \n",
       "361889  TTB000026714586    7700   \n",
       "361890  TTB000026713292    8411   \n",
       "361891  TTB000026717333    8411   \n",
       "\n",
       "       Pen-delete-an Status Registrasi Layanan yang Ada di BRI  \\\n",
       "0                     Kartu ATM BRI Tertelan di MESIN ATM        \n",
       "1                               Informasi Product Banking        \n",
       "2       Nasabah BRI gagal tarik tunai & terdebet di AT...        \n",
       "3                               Informasi Product Banking        \n",
       "4              Kegagalan TAC / Transfer Lainnya Lewat BRI        \n",
       "...                                                   ...        \n",
       "361887                           Salah Transfer antar BRI        \n",
       "361888                           Salah Transfer antar BRI        \n",
       "361889      Komplain Transaksi Kartu Kredit tidak di akui        \n",
       "361890                           Salah Transfer antar BRI        \n",
       "361891                           Salah Transfer antar BRI        \n",
       "\n",
       "        2020-01-01 07:19:37.000  Phone  Maintenance   MERSY ANGELIA ELISAB  \\\n",
       "0       2020-01-01 07:19:30.000  Phone  Information  I NYOMAN SAJA SAPUTRA   \n",
       "1       2020-01-01 07:19:27.000  Phone  Information       ENY INTI SURYANI   \n",
       "2       2020-01-01 07:00:34.000  Phone  Information          ANDI GUSMANTO   \n",
       "3       2020-01-01 07:09:29.000  Phone  Information   SEPTIVA HERLIN ARTAT   \n",
       "4       2020-01-01 07:07:31.000  Phone  Information    MHD INDANA FARHAN S   \n",
       "...                         ...    ...          ...                    ...   \n",
       "361887  2020-01-31 19:32:26.000  Phone     Complain               HERIWATI   \n",
       "361888  2020-01-31 17:28:34.000  Phone     Complain                 MUTIAH   \n",
       "361889  2020-01-31 18:24:32.000  Phone     Complain      MULIYA HARDIYANTO   \n",
       "361890  2020-01-31 16:56:02.000  Phone     Complain              S I M O N   \n",
       "361891  2020-01-31 22:08:12.000  Phone     Complain          AGUS SUPRAPTO   \n",
       "\n",
       "        021201053488501        0.00  Closed  ... NULL.2                NULL.3  \\\n",
       "0       106701004427504        0.00  Closed  ...    NaN                   NaN   \n",
       "1       009801126221505   741700.00  Closed  ...    NaN                   NaN   \n",
       "2       504601012686530  1000000.00  Closed  ...    NaN                   NaN   \n",
       "3       054401000469567   500000.00  Closed  ...    NaN  bacuelkueh@gmail.com   \n",
       "4       530001017365535        0.00  Closed  ...    NaN                   NaN   \n",
       "...                 ...         ...     ...  ...    ...                   ...   \n",
       "361887  546501003139532    100001.0  Closed  ...    0.0                   NaN   \n",
       "361888  609401002283508   1950000.0  Closed  ...    NaN                   NaN   \n",
       "361889              NaN         0.0  Closed  ...    NaN                   NaN   \n",
       "361890  064201002986507   9650000.0  Closed  ...    0.0                   NaN   \n",
       "361891  601401026226534   2000000.0  Closed  ...    NaN                   NaN   \n",
       "\n",
       "            Really Artha Ully Manik  082158717189    NULL.4           NULL.5  \\\n",
       "0                   DELLA LARASSARI  085238111553  90022934  Adhi Nitidharma   \n",
       "1                  Kartika Fitriani  081369386737       NaN              NaN   \n",
       "2                  AMELIA RAHMADANI  081242958898  00000723           Ismail   \n",
       "3                   Amalia Fitriana  082127363449       NaN              NaN   \n",
       "4                            Nuriah  085761011162       NaN              NaN   \n",
       "...                             ...           ...       ...              ...   \n",
       "361887                  ADE SUTISNA  082387480456       NaN              NaN   \n",
       "361888       Umar Fahruddin Pratama  082136107896       NaN              NaN   \n",
       "361889                Cintia Fadila   05264513380       NaN              NaN   \n",
       "361890         Alfera Dyah Pangestu  081241313888       NaN              NaN   \n",
       "361891  MUHAMMAD ARHANDY KOES NANDA  081228501106       NaN              NaN   \n",
       "\n",
       "        20  90136590       NULL.6    Really Artha Ully Manik.1  \n",
       "0       20  90135196          NaN              DELLA LARASSARI  \n",
       "1       20  90141079  LCC-CCTCALL             Kartika Fitriani  \n",
       "2       10     60443  LCC-CCTCALL             AMELIA RAHMADANI  \n",
       "3       20     60422  LCC-CCTCALL              Amalia Fitriana  \n",
       "4       10  90137097          NaN                       Nuriah  \n",
       "...     ..       ...          ...                          ...  \n",
       "361887  20  90135194  LCC-CCTCALL                  ADE SUTISNA  \n",
       "361888  20  90135689          NaN       Umar Fahruddin Pratama  \n",
       "361889  67  90123773  LCC-CCTCALL                Cintia Fadila  \n",
       "361890  20  90138706          NaN         Alfera Dyah Pangestu  \n",
       "361891  20     60477  LCC-CCTCALL  MUHAMMAD ARHANDY KOES NANDA  \n",
       "\n",
       "[361892 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Option 1: Check for quoted fields or use different delimiter\n",
    "try:\n",
    "    df = pd.read_csv(path, sep=',', on_bad_lines='skip')  # Skip bad lines\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing file: {e}\")\n",
    "\n",
    "# Display the dataframe to check if it loaded successfully\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricaredatareal_20200101_20200201.csv\n",
      "Problematic data saved to bricaredatareal_problematic_20200101_20200201.csv\n",
      "Number of rows in dataframe: 463581\n",
      "Number of problematic lines: 881\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"\n",
    "]\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Initialize lists to store data and problematic lines\n",
    "data = []\n",
    "problematic_lines = []\n",
    "\n",
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    try:\n",
    "        # Identifying Ticket_ID\n",
    "        ticket_id_index = next(i for i, part in enumerate(parts) if part.startswith('TTB'))\n",
    "        ticket_id = parts[ticket_id_index]\n",
    "        \n",
    "        # Identifying Call_Type_ID\n",
    "        call_type_id_index = ticket_id_index + 1\n",
    "        call_type_id = parts[call_type_id_index]\n",
    "        \n",
    "        # Identifying Create_Date (the part that looks like a datetime string)\n",
    "        create_date_index = next(i for i, part in enumerate(parts) if '-' in part and ':' in part)\n",
    "        create_date = parts[create_date_index]\n",
    "        \n",
    "        # Call_Type is everything between Call_Type_ID and Create_Date\n",
    "        call_type = ' '.join(parts[call_type_id_index + 1:create_date_index])\n",
    "        \n",
    "        # The rest of the fields in order\n",
    "        rest = parts[create_date_index + 1:]\n",
    "        \n",
    "        # Combine into a single list in the correct order\n",
    "        structured_line = [ticket_id, call_type_id, call_type, create_date] + rest\n",
    "        \n",
    "        # If the length is correct, return it\n",
    "        if len(structured_line) == len(column_list):\n",
    "            return structured_line\n",
    "        # If there are extra fields, handle them (for example, by merging or ignoring)\n",
    "        elif len(structured_line) > len(column_list):\n",
    "            return structured_line[:len(column_list)]\n",
    "        else:\n",
    "            return None\n",
    "    except StopIteration:\n",
    "        # If any part of the parsing fails, consider the line problematic\n",
    "        return None\n",
    "\n",
    "# Read the file line by line and parse it\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        parsed_line = parse_line(lines[i])\n",
    "        if parsed_line:\n",
    "            data.append(parsed_line)\n",
    "            i += 1\n",
    "        else:\n",
    "            # Check if merging the next line solves the issue\n",
    "            if i + 1 < len(lines):\n",
    "                merged_line = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "                parsed_line = parse_line(merged_line)\n",
    "                if parsed_line:\n",
    "                    data.append(parsed_line)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    problematic_lines.append((i, lines[i]))\n",
    "                    i += 1\n",
    "            else:\n",
    "                problematic_lines.append((i, lines[i]))\n",
    "                i += 1\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=column_list)\n",
    "\n",
    "# Data cleaning steps\n",
    "df['Ticket_ID'] = df['Ticket_ID'].astype(str)\n",
    "df = df[df['Ticket_ID'].str.match(r'TTB\\d+')]\n",
    "\n",
    "df['Call_Type_ID'] = df['Call_Type_ID'].astype(str)\n",
    "df = df[df['Call_Type_ID'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "df = df.dropna(subset=['Create_Date'])\n",
    "\n",
    "df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "# Drop any unnamed columns\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Filename formatting\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricaredatareal_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "# Save cleaned data to CSV\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Save problematic lines to a DataFrame\n",
    "problematic_df = pd.DataFrame(problematic_lines, columns=['Line_Number', 'Data'])\n",
    "\n",
    "# Save problematic data to a CSV file\n",
    "problematic_filename = f\"bricaredatareal_problematic_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "problematic_df.to_csv(problematic_filename, index=False)\n",
    "\n",
    "print(f\"Problematic data saved to {problematic_filename}\")\n",
    "\n",
    "# Check the number of rows to ensure all data is loaded\n",
    "print(f\"Number of rows in dataframe: {len(df)}\")\n",
    "print(f\"Number of problematic lines: {len(problematic_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gdown\n",
    "\n",
    "def get_file_list_from_folder(folder_url):\n",
    "    response = requests.get(folder_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    file_links = soup.find_all('a', {'class': 'Q5txwe'})\n",
    "    \n",
    "    file_ids = []\n",
    "    for link in file_links:\n",
    "        file_id = link['href'].split('/')[-2]\n",
    "        file_ids.append(file_id)\n",
    "        \n",
    "    return file_ids\n",
    "\n",
    "def download_files_from_folder(folder_url, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    \n",
    "    file_ids = get_file_list_from_folder(folder_url)\n",
    "    \n",
    "    for file_id in file_ids:\n",
    "        download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(download_url, output=os.path.join(destination_folder, f\"{file_id}.file\"))\n",
    "\n",
    "# Replace with your Google Drive folder URL\n",
    "folder_url = 'https://drive.google.com/drive/u/0/folders/1CgN7DE3pNRNh_4BA_zrrMLqWz6KquwuD'\n",
    "destination_folder = 'D:\\PYTHON\\Books'\n",
    "\n",
    "download_files_from_folder(folder_url, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gdown\n",
    "\n",
    "def get_file_links_from_folder(folder_url):\n",
    "    response = requests.get(folder_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    file_links = []\n",
    "    for link in soup.find_all('a', {'href': True}):\n",
    "        href = link['href']\n",
    "        if 'drive.google.com/file/d/' in href:\n",
    "            file_id = href.split('/d/')[1].split('/')[0]\n",
    "            file_links.append(file_id)\n",
    "    return file_links\n",
    "\n",
    "def download_files(file_ids, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    \n",
    "    for file_id in file_ids:\n",
    "        download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        output_path = os.path.join(destination_folder, f\"{file_id}.file\")\n",
    "        gdown.download(download_url, output=output_path, quiet=False)\n",
    "\n",
    "# Replace with your Google Drive folder URL\n",
    "folder_url = 'https://drive.google.com/drive/u/0/folders/1CgN7DE3pNRNh_4BA_zrrMLqWz6KquwuD'\n",
    "destination_folder = 'D:\\PYTHON\\Books'\n",
    "\n",
    "file_ids = get_file_links_from_folder(folder_url)\n",
    "download_files(file_ids, destination_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create masked Data for UAT\n",
    "\n",
    "\n",
    "To Mask:\n",
    "\n",
    "Nama_Nasabah\n",
    "No_Rekening\n",
    "full_name\n",
    "no_telp\n",
    "approver_name = beda dengan Nama_Nasabah\n",
    "user_login_name = beda \n",
    "Log_Name\n",
    "Nama_Supervisor\n",
    "Nama_TL\n",
    "Nama_Wakabag\n",
    "\n",
    "Kolom Detail:\n",
    "\n",
    "Kode Cabang          : 0307 4 digits\n",
    "No Kartu             : 5221843130736932 16 digits\n",
    "No Rekening          : 030701098507501 15 digits\n",
    "Nama                 : SITI SHOLEHA\n",
    "No ID                : 3316022012770004\n",
    "\n",
    "\n",
    "Kode Cabang          : 0307 \n",
    "No Kartu             : 5221843130736932 \n",
    "No Rekening          : 030701098507501 \n",
    "Nama                 : SITI SHOLEHA\n",
    "No ID                : 3316022012770004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\3051316347.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_masked['Details'] = df_masked['Details'].astype(str).apply(mask_detail)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "fake = Faker('id_ID')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "path = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "def mask_data(df):\n",
    "    df['Nama_Nasabah'] = df['Nama_Nasabah'].apply(lambda x: fake.name())\n",
    "    df['No_Rekening'] = df['No_Rekening'].apply(lambda x: fake.bban())\n",
    "    df['full_name'] = df['full_name'].apply(lambda x: fake.name())\n",
    "    df['no_telepon'] = df['no_telepon'].apply(lambda x: fake.phone_number())\n",
    "    df['approver_name'] = df['approver_name'].apply(lambda x: fake.name())\n",
    "    df['user_login_name'] = df['user_login_name'].apply(lambda x: fake.user_name())\n",
    "    df['Log_Name'] = df['Log_Name'].apply(lambda x: fake.user_name())\n",
    "    df['Nama_Supervisor'] = df['Nama_Supervisor'].apply(lambda x: fake.name())\n",
    "    df['Nama_TL'] = df['Nama_TL'].apply(lambda x: fake.name())\n",
    "    df['Nama_Wakabag'] = df['Nama_Wakabag'].apply(lambda x: fake.name())\n",
    "    return df\n",
    "\n",
    "\n",
    "df_masked = mask_data(df)\n",
    "\n",
    "def generate_nik():\n",
    "    return f'{fake.random_number(digits=16)}'\n",
    "\n",
    "def mask_detail(detail):\n",
    "    if isinstance(detail, float) and pd.isna(detail):\n",
    "        return detail\n",
    "    lines = str(detail).split('\\n')\n",
    "    masked_lines = []\n",
    "    for line in lines:\n",
    "        if 'Kode Cabang' in line:\n",
    "            line = f'Kode Cabang          : {fake.random_number(digits=4)}'\n",
    "        elif 'No Kartu' in line:\n",
    "            line = f'No Kartu             : {fake.credit_card_number()}'\n",
    "        elif 'No Rekening' in line:\n",
    "            line = f'No Rekening          : {fake.bban()}'\n",
    "        elif 'Nama' in line:\n",
    "            line = f'Nama                 : {fake.name()}'\n",
    "        elif 'No ID' in line:\n",
    "            line = f'No ID                : {generate_nik()}'\n",
    "        masked_lines.append(line)\n",
    "    return '\\n'.join(masked_lines)\n",
    "\n",
    "df_masked = df_masked.iloc[:10]\n",
    "\n",
    "# Ensure 'Details' column is treated as string and apply mask_detail function\n",
    "df_masked['Details'] = df_masked['Details'].astype(str).apply(mask_detail)\n",
    "\n",
    "df_masked = df_masked.iloc[:10]\n",
    "df_masked['Details']\n",
    "output_path = 'D:\\dataquality\\details_4_uat.csv'\n",
    "df_masked.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Details column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#BRILINKMOB\\n\\nDATA outlet BRILINK\\nKode Outle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#CALL TERPUTUS\\n\\nif ch call back ,layanan IB ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nasabah gagal melakukan transaksi tarik tunai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ch infokan melakukan registrasi brimo, namun m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Ch terputus\\n\\nNasabah mengajukan pemblokiran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nasabah gagal melakukan transaksi tarik tunai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>limitasi, penyetoran, no rekening tujuan,Nasab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nasabah gagal melakukan transaksi setor tunai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#BRILink\\nKode TID BRILink\\t: 26064156\\nKode M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Details\n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...\n",
       "1  #BRILINKMOB\\n\\nDATA outlet BRILINK\\nKode Outle...\n",
       "2  #CALL TERPUTUS\\n\\nif ch call back ,layanan IB ...\n",
       "3  Nasabah gagal melakukan transaksi tarik tunai ...\n",
       "4  ch infokan melakukan registrasi brimo, namun m...\n",
       "5  #Ch terputus\\n\\nNasabah mengajukan pemblokiran...\n",
       "6  Nasabah gagal melakukan transaksi tarik tunai ...\n",
       "7  limitasi, penyetoran, no rekening tujuan,Nasab...\n",
       "8  Nasabah gagal melakukan transaksi setor tunai ...\n",
       "9  #BRILink\\nKode TID BRILink\\t: 26064156\\nKode M..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('id_ID')  \n",
    "\n",
    "def generate_nik():\n",
    "    return f'{fake.random_number(digits=16)}'\n",
    "\n",
    "def mask_detail(detail):\n",
    "    lines = detail.split('\\n')\n",
    "    masked_lines = []\n",
    "    for line in lines:\n",
    "        if 'Kode Cabang' in line:\n",
    "            line = f'Kode Cabang          : {fake.random_number(digits=4)}'\n",
    "        elif 'No Kartu' in line:\n",
    "            line = f'No Kartu             : {fake.credit_card_number()}'\n",
    "        elif 'No Rekening' in line:\n",
    "            line = f'No Rekening          : {fake.bban()}'\n",
    "        elif 'Nama' in line:\n",
    "            line = f'Nama                 : {fake.name()}'\n",
    "        elif 'No ID' in line:\n",
    "            line = f'No ID                : {generate_nik()}'\n",
    "        masked_lines.append(line)\n",
    "    return '\\n'.join(masked_lines)\n",
    "\n",
    "# Load the CSV file\n",
    "path = 'D:\\dataquality\\details4uat.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Mask the Details column\n",
    "df['Details'] = df['Details'].apply(mask_detail)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = 'D:\\dataquality\\details_4_uat.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f'Masked data saved to {output_file}')\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

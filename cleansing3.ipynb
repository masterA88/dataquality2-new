{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script for data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRICARE:\n",
    "\n",
    "BRICARE consists of 2 different types of files by year:\n",
    "\n",
    "a. File after 2022 (2023-2024) = 79 kolom\n",
    "\n",
    "\n",
    "b. File before 2022 (2019-2022) = 27 kolom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type A\n",
    "\n",
    "\n",
    "Data Extraction for File Type A must be 2 Files:\n",
    "\n",
    "\n",
    "A.1 Columns (without \"Details\")\n",
    "\n",
    "\n",
    "A.2 Details only \n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['Create_Date','TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\"\n",
    "\n",
    "- Columns shoud be mapped based on their Call_Type_ID:\n",
    "\n",
    "['Produk','Jenis_Produk','Jenis_Laporan']\n",
    "\n",
    "- PLEASE ADD CIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_uat20230101_20230101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('NULL', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('None', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_file(file_path):\n",
    "\n",
    "    data = []\n",
    "    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "\n",
    "            date_index = next(i for i, part in enumerate(parts) if date_pattern.match(part))\n",
    "\n",
    "            ticket_id = parts[0] \n",
    "            call_type_id = parts[1]  \n",
    "            description = ';'.join(parts[2:date_index])  \n",
    "            create_date = parts[date_index]  \n",
    "\n",
    "      \n",
    "            data.append([ticket_id, call_type_id, description, create_date] + parts[date_index + 1:])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.txt\"\n",
    "\n",
    "df = parse_file(file_path)\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "   \n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_uat{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bricare_20230101_20230101.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if len(parts) > 78:\n",
    "                description = ';'.join(parts[2:-75])\n",
    "                new_parts = parts[:2] + [description] + parts[-75:]\n",
    "                data.append(new_parts)\n",
    "            else:\n",
    "                data.append(parts)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    df = df.replace(['0', 0], '')\n",
    "\n",
    "    columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "    for column in columns_to_convert:\n",
    "        df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "    \n",
    "    df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "    return df\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.csv\"\n",
    "df = parse_csv(file_path)\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleasing the master Call Type file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_24036\\2288781540.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "master_df_path = r\"C:\\Users\\maste\\Downloads\\bricare\\(REVISED) SLA-OLA_NewUserGrouping_Ringkasan Kirim ME Versi 1.6.csv\"\n",
    "df = pd.read_csv(master_df_path, sep=';')\n",
    "\n",
    "\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "df = df.dropna(how='all')\n",
    "df.iloc[:450]\n",
    "df.to_csv(\"master_calltype.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Type di tes MMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Data Mapping - Case_Type.csv\"\n",
    "df=pd.read_csv(path)\n",
    "# case_types = ['8634', '8635', '8636', '8623', '8624', '8628', '8629', '8630', '8412', '8625', '8615', '8616', '8617', '8618', '8619', '8638', '8620']\n",
    "\n",
    "case_types = ['8412']\n",
    "# Filter the dataframe\n",
    "df = df[df['Case Types'].isin(case_types)]\n",
    "df['Segment'] = df['Segment'].replace('All', '')\n",
    "df\n",
    "df.to_csv(r'C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Case_Type_MMS2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call type final add external ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "1     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "2     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "3     Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "4                                                                 bad value for restricted picklist field: Giro\n",
       "5                                                Customer Segment: bad value for restricted picklist field: All\n",
       "6                                                Customer Segment: bad value for restricted picklist field: All\n",
       "7                                                         Product: bad value for restricted picklist field: SBN\n",
       "8                                                        Product: bad value for restricted picklist field: DPLK\n",
       "9                                                Customer Segment: bad value for restricted picklist field: All\n",
       "10                                               Customer Segment: bad value for restricted picklist field: All\n",
       "11                                               Customer Segment: bad value for restricted picklist field: All\n",
       "12                                               Customer Segment: bad value for restricted picklist field: All\n",
       "13    Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "14                                               Customer Segment: bad value for restricted picklist field: All\n",
       "15                                               Customer Segment: bad value for restricted picklist field: All\n",
       "16             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "17                                               Customer Segment: bad value for restricted picklist field: All\n",
       "18                                               Customer Segment: bad value for restricted picklist field: All\n",
       "19                                               Customer Segment: bad value for restricted picklist field: All\n",
       "20                                               Customer Segment: bad value for restricted picklist field: All\n",
       "21                                               Customer Segment: bad value for restricted picklist field: All\n",
       "22                                               Customer Segment: bad value for restricted picklist field: All\n",
       "23                                               Customer Segment: bad value for restricted picklist field: All\n",
       "24                                                bad value for restricted picklist field: Transaction Solution\n",
       "25                                                bad value for restricted picklist field: Transaction Solution\n",
       "26                                               Customer Segment: bad value for restricted picklist field: All\n",
       "27                                               Customer Segment: bad value for restricted picklist field: All\n",
       "28                                               Customer Segment: bad value for restricted picklist field: All\n",
       "29                                               Customer Segment: bad value for restricted picklist field: All\n",
       "30                                               Customer Segment: bad value for restricted picklist field: All\n",
       "31                                               Customer Segment: bad value for restricted picklist field: All\n",
       "32                                               Customer Segment: bad value for restricted picklist field: All\n",
       "33                                               Customer Segment: bad value for restricted picklist field: All\n",
       "34                                               Customer Segment: bad value for restricted picklist field: All\n",
       "35                                               Customer Segment: bad value for restricted picklist field: All\n",
       "36                                               Customer Segment: bad value for restricted picklist field: All\n",
       "37                                               Customer Segment: bad value for restricted picklist field: All\n",
       "38                                               Customer Segment: bad value for restricted picklist field: All\n",
       "39                                               Customer Segment: bad value for restricted picklist field: All\n",
       "40                                               Customer Segment: bad value for restricted picklist field: All\n",
       "41    Customer Segment: bad value for restricted picklist field: Individu Umum, Individu Priority &amp; Private\n",
       "42             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "43                                                bad value for restricted picklist field: Transaction Solution\n",
       "44                                                bad value for restricted picklist field: Transaction Solution\n",
       "45                                               Customer Segment: bad value for restricted picklist field: All\n",
       "46                                               Customer Segment: bad value for restricted picklist field: All\n",
       "47                                               Customer Segment: bad value for restricted picklist field: All\n",
       "48                                               Customer Segment: bad value for restricted picklist field: All\n",
       "49                                                        Product: bad value for restricted picklist field: KTA\n",
       "50                                               Customer Segment: bad value for restricted picklist field: All\n",
       "51                                               Customer Segment: bad value for restricted picklist field: All\n",
       "52                                          Customer Segment: bad value for restricted picklist field: Individu\n",
       "53                                          Customer Segment: bad value for restricted picklist field: Individu\n",
       "54             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "55                                               Customer Segment: bad value for restricted picklist field: All\n",
       "56                                               Customer Segment: bad value for restricted picklist field: All\n",
       "57                                                        Product: bad value for restricted picklist field: KTA\n",
       "58                                                                bad value for restricted picklist field: Giro\n",
       "59                                                                bad value for restricted picklist field: Giro\n",
       "60                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "61                                                                bad value for restricted picklist field: Giro\n",
       "62                                                       Product: bad value for restricted picklist field: DPLK\n",
       "63                                                       Product: bad value for restricted picklist field: DPLK\n",
       "64                                                       Product: bad value for restricted picklist field: DPLK\n",
       "65                                                       Product: bad value for restricted picklist field: DPLK\n",
       "66                                                       Product: bad value for restricted picklist field: DPLK\n",
       "67                                                       Product: bad value for restricted picklist field: DPLK\n",
       "68                               Customer Segment: bad value for restricted picklist field: Individu Umum, UMKM\n",
       "69                                               Customer Segment: bad value for restricted picklist field: All\n",
       "70                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "71                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "72                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "73                                          Product: bad value for restricted picklist field: KPR, KUR, KMK, KI\n",
       "74             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "75             Case Category: bad value for restricted picklist field: Customer Feedback / Sentiment Monitoring\n",
       "Name: ERROR, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"D:\\dataquality2\\new as per 5 June\\error060824083047752_SIT.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "df.loc[df['Case Types'].isin([8418, 8419]), 'PRODUCT'] = 'Transaction Banking'\n",
    "df.loc[df['Case Types'].isin([8708, 8711]), 'PRODUCT'] = 'Transaction Banking'\n",
    "df['CASE CATEGORY'] = df['CASE CATEGORY'].replace('Customer Feedback / Sentiment Monitoring', 'Customer Feedback')\n",
    "\n",
    "segment_mapping = {\n",
    "    'Individu': 'Individu Umum',\n",
    "    'Individu Umum, Individu Priority &amp; Private' : 'Individu Umum',\n",
    "    'All' : 'Individu Umum',\n",
    "    'Individu Umum, UMKM' : 'Individu Umum',\n",
    "    \n",
    "}\n",
    "\n",
    "product_mapping = {\n",
    "    'loans': 'Loans',\n",
    "    'savings': 'Savings',\n",
    "    'KTA': ''\n",
    "}\n",
    "\n",
    "subproduk_mapping = {\n",
    "    'Giro': 'Giro (CA)',\n",
    "    'KPR, KUR, KMK, KI': '',\n",
    "    'SBN': '',\n",
    "    'DPLK': ''\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "data[\"ERROR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\new as per 5 June\\error060824061742698_uatcalltype.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df=df.iloc[:50]\n",
    "df['SEGMENT']='Individu Umum'\n",
    "df.to_csv('error060824061742698_uatcalltype_resolved.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the error in Segment Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'Individu umum']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_40000\\1757643033.py:45: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Data Mapping - Case_Type.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# df['Product'].unique()\n",
    "\n",
    "# I have a dataset, I want in column Segment you map\n",
    "# Individu=Individu umum\n",
    "# All becomes empty\n",
    "# and column Product\n",
    "\n",
    "# loans = Loans\n",
    "# savings = Savings\n",
    "\n",
    "\n",
    "\n",
    "# mapping\n",
    "\n",
    "segment_mapping = {\n",
    "    'Individu': 'Individu umum',\n",
    "}\n",
    "\n",
    "product_mapping = {\n",
    "    'loans': 'Loans',\n",
    "    'savings': 'Savings',\n",
    "    'KTA' : ''\n",
    "}\n",
    "\n",
    "subproduk_mapping = {\n",
    "    'Giro' : 'Giro (CA)',\n",
    "    'KPR, KUR, KMK, KI' : '',\n",
    "    'SBN' : '',\n",
    "    'DPLK' : ''\n",
    "}\n",
    "\n",
    "case_category = {\n",
    "    'Customer Feedback / Sentiment Monitoring' : 'Customer Feedback'\n",
    "}\n",
    "\n",
    "\n",
    "df['Segment'] = df['Segment'].apply(lambda x: segment_mapping.get(x, ''))\n",
    "df['Product'] = df['Product'].map(product_mapping).fillna(df['Product'])\n",
    "\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "print(df['Segment'].unique())\n",
    "\n",
    "# df['Product'].unique()\n",
    "\n",
    "# output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\master_casetype_cleaned.csv\"\n",
    "\n",
    "# See my code above I wnat another mapping in column Case Category and Sub Produk\n",
    "\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_40000\\3962062038.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# path= r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424015731096.csv\"\n",
    "# path2 = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424020750586.csv\"\n",
    "path3 = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424022013225.csv\"\n",
    "df = pd.read_csv(path3)\n",
    "\n",
    "# segment_mapping = {\n",
    "#     'Individu': 'Individu umum',\n",
    "# }\n",
    "\n",
    "# product_mapping = {\n",
    "#     'loans': 'Loans',\n",
    "#     'savings': 'Savings',\n",
    "#     'KTA': ''\n",
    "# }\n",
    "\n",
    "# subproduk_mapping = {\n",
    "#     'Giro': 'Giro (CA)',\n",
    "#     'KPR, KUR, KMK, KI': '',\n",
    "#     'SBN': '',\n",
    "#     'DPLK': ''\n",
    "# }\n",
    "\n",
    "# case_category = {\n",
    "#     'Customer Feedback / Sentiment Monitoring': 'Customer Feedback',\n",
    "#     # Add more mappings for other Case Category values here\n",
    "# }\n",
    "\n",
    "# # Apply mapping to columns\n",
    "# df['SEGMENT'] = df['SEGMENT'].apply(lambda x: segment_mapping.get(x, ''))\n",
    "# df['PRODUCT'] = df['PRODUCT'].map(product_mapping).fillna(df['PRODUCT'])\n",
    "# df['CASE CATEGORY'] = df['CASE CATEGORY'].apply(lambda x: case_category.get(x, x))  # Apply default value if not mapped\n",
    "# df['SUB PRODUCT'] = df['SUB PRODUCT'].map(subproduk_mapping).fillna(df['SUB PRODUCT'])\n",
    "df.fillna('', inplace=True)\n",
    "df.replace('Transaction Solution', '', inplace=True)\n",
    "output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\master_casetype_cleaned_error3.csv\"\n",
    "\n",
    "# df['SUB PRODUK'] = df.apply(lambda row: row['SUB PRODUK'].replace('Transaction Solution', '') if 'Servicing' in row['PRODUK'] else row['SUB PRODUK'], axis=1)\n",
    "# Copy values from NEW CASE DESCRIPTION to CASE DESCRIPTION if empty\n",
    "df['CASE DESCRIPTION']='Nasabah Mengajukan Unlock User Microsite DPLK'\n",
    "df\n",
    "\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call type mapping for columns 'Produk', 'Jenis Produk', 'Jenis Laporan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "user_dataset_path = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "user_df = pd.read_csv(user_dataset_path)\n",
    "master_df_path = r\"D:\\dataquality\\master_calltype.csv\"\n",
    "master_df = pd.read_csv(master_df_path)\n",
    "\n",
    "\n",
    "master_df = master_df.rename(columns={\n",
    "    'Case Types': 'Call_Type_ID', \n",
    "    'Product': 'Produk', \n",
    "    'Sub Product': 'Jenis_Produk', \n",
    "    'Case Category': 'Jenis_Laporan'\n",
    "})\n",
    "\n",
    "\n",
    "user_df['Call_Type_ID'] = user_df['Call_Type_ID'].astype(str)\n",
    "master_df['Call_Type_ID'] = master_df['Call_Type_ID'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(user_df, master_df[['Call_Type_ID', 'Produk', 'Jenis_Produk', 'Jenis_Laporan']], on='Call_Type_ID', how='left')\n",
    "\n",
    "user_df['Produk'] = merged_df['Produk_y']\n",
    "user_df['Jenis_Produk'] = merged_df['Jenis_Produk_y']\n",
    "user_df['Jenis_Laporan'] = merged_df['Jenis_Laporan_y']\n",
    "\n",
    "\n",
    "\n",
    "user_df.to_csv(user_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_text_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:  \n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]  \n",
    "                current_entry.append(parts[3].strip())  \n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def remove_bom_and_strip(df):\n",
    "    return df.applymap(lambda x: x.replace('\\ufeff', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.txt\"\n",
    "processed_data = process_text_data(file_path)\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "# df_final=df_final.iloc[:10]\n",
    "df_final.iloc[:10].to_csv('details_uat_20230101_20230101.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to details_20230101_20230101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:\n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]\n",
    "                current_entry.append(parts[3].strip())\n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.csv\"\n",
    "processed_data = process_csv_data(file_path)\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "\n",
    "df_final = df_final.iloc[:10]\n",
    "output_path = \"details_20230101_20230101.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the file A.1 and file A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#just take 10 lines for an example\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.iloc[:10].to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path_1 = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "file_path_2 = r\"D:\\dataquality2\\details_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "df_tenline_bricare = pd.read_csv(file_path_1)\n",
    "df_detail_bricare_10line = pd.read_csv(file_path_2)\n",
    "\n",
    "df_detail_bricare_10line.columns = ['Ticket_ID', 'Details']\n",
    "\n",
    "merged_df = pd.merge(df_tenline_bricare, df_detail_bricare_10line, on='Ticket_ID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "column_to_move=\"Details\"\n",
    "merged_df = merged_df[[col for col in merged_df if col != column_to_move][:3] + [column_to_move] + [col for col in merged_df if col != column_to_move][3:]] \n",
    "\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type B\n",
    "\n",
    "\n",
    "Data Extraction for File Type B (27 columns)\n",
    "\n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['TanggalClosed', 'tanggalTransaksi','Create_Date']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\" if any\n",
    "\n",
    "- PLEASE ADD CIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_33500\\3452762791.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20200101_20200101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\BRICARE_25042024 masking.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "\n",
    "data['Column1'] = data['Column1'].astype(str)\n",
    "data_cleaned = data[data['Column1'].str.match(r'TTB\\d+')]\n",
    "\n",
    "data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n",
    "data_cleaned = data_cleaned[data_cleaned['Column2'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "data_cleaned['Column4'] = pd.to_datetime(data_cleaned['Column4'], errors='coerce')\n",
    "data_cleaned = data_cleaned.dropna(subset=['Column4'])\n",
    "\n",
    "\n",
    "data_to_drop = ['Column28', 'Column29', 'Column30', 'Column31', 'Column32']\n",
    "data_cleaned = data_cleaned.drop(columns=data_to_drop)\n",
    "\n",
    "\n",
    "if len(data_cleaned.columns) <= len(column_list):\n",
    "    data_cleaned.columns = column_list[:len(data_cleaned.columns)]\n",
    "\n",
    "\n",
    "data_cleaned.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "data_cleaned.fillna('', inplace=True)\n",
    "data_cleaned = data_cleaned.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    data_cleaned[column] = pd.to_datetime(data_cleaned[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    data_cleaned[column] = data_cleaned[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "# Just take 10 lines for an example\n",
    "data_cleaned = data_cleaned.iloc[:10]\n",
    "\n",
    "\n",
    "data_cleaned.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(data_cleaned['Create_Date']))\n",
    "enddate = pd.Timestamp(max(data_cleaned['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "data_cleaned.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "27\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Details', 'Create_Date',\n",
      "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
      "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
      "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
      "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
      "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
      "       'user_login_name', 'Jenis_Produk', 'Last_Modified_By', 'Merchant_ID',\n",
      "       'Modified_Date', 'NOTAS', 'Produk', 'SLA_Status', 'TID',\n",
      "       'tanggalAttachmentDone', 'Tgl_Assigned', 'Tgl_Eskalasi', 'AnalisaSkils',\n",
      "       'Attachment_', 'Bank_BRI', 'Biaya_Admin', 'Suku_Bunga', 'Bunga',\n",
      "       'Butuh_Attachment', 'Cicilan', 'Hasil_Kunjungan', 'Log_Name',\n",
      "       'MMS_Ticket_Id', 'Mass_Ticket_Upload_Flag', 'Nama_Supervisor',\n",
      "       'Nama_TL', 'Nama_Wakabag', 'Nasabah_Prioritas', 'Notify_By',\n",
      "       'Organization', 'Output_Settlement', 'phone_survey', 'Return_Ticket',\n",
      "       'Settlement_By', 'Settlement_ID', 'Settlement', 'Site_User',\n",
      "       'Status_Return', 'Status_Transaksi', 'Submitter_Region',\n",
      "       'Submitter_SiteGroup', 'Submitter_User_group_ID', 'Tanggal_Settlement',\n",
      "       'Tgl_Foward', 'Tgl_In_Progress', 'Tgl_Returned', 'Ticket_Referensi',\n",
      "       'Tiket_Urgency', 'Tipe_Remark', 'UniqueID', 'users', 'Usergroup_ID'],\n",
      "      dtype='object')\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date', 'gateway',\n",
      "       'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal', 'status',\n",
      "       'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur', 'Nomor_Kartu',\n",
      "       'user_group', 'assgined_to', 'attachment_done', 'email', 'full_name',\n",
      "       'no_telepon', 'approver_login', 'approver_name', 'SLAResolution',\n",
      "       'submitter_login_id', 'submitter_user_group', 'user_login_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path1=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "path2=r\"D:\\dataquality\\bricare_20200101_20200101.csv\"\n",
    "\n",
    "df1=pd.read_csv(path1)\n",
    "df2=pd.read_csv(path2)\n",
    "print(len(df1.columns))\n",
    "print(len(df2.columns))\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Compare two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape_match': True,\n",
       " 'columns_match': True,\n",
       " 'column_differences': {},\n",
       " 'value_differences': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the newly uploaded files for detailed comparison\n",
    "processed_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "expected_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "# Read the files\n",
    "processed_df_newest = pd.read_csv(processed_file_path_newest)\n",
    "expected_df_newest = pd.read_csv(expected_file_path_newest)\n",
    "\n",
    "# Check for exact match first\n",
    "exact_match = processed_df_newest.equals(expected_df_newest)\n",
    "\n",
    "# Initialize a dictionary to store detailed comparison results\n",
    "comparison_details = {\n",
    "    'shape_match': processed_df_newest.shape == expected_df_newest.shape,\n",
    "    'columns_match': processed_df_newest.columns.equals(expected_df_newest.columns),\n",
    "    'column_differences': {},\n",
    "    'value_differences': {}\n",
    "}\n",
    "\n",
    "# Compare each aspect if exact match is not true\n",
    "if not exact_match:\n",
    "    # Check shape\n",
    "    if not comparison_details['shape_match']:\n",
    "        comparison_details['shape_details'] = {\n",
    "            'processed_shape': processed_df_newest.shape,\n",
    "            'expected_shape': expected_df_newest.shape\n",
    "        }\n",
    "    \n",
    "    # Check columns\n",
    "    if not comparison_details['columns_match']:\n",
    "        comparison_details['column_details'] = {\n",
    "            'processed_columns': processed_df_newest.columns.tolist(),\n",
    "            'expected_columns': expected_df_newest.columns.tolist()\n",
    "        }\n",
    "\n",
    "    # Check column-by-column values\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            comparison_details['column_differences'][column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "\n",
    "# Summarize value differences\n",
    "if not comparison_details['columns_match']:\n",
    "    value_differences = {}\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            value_differences[column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "    comparison_details['value_differences'] = value_differences\n",
    "\n",
    "comparison_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To collect all Error logs in a path\n",
    "\n",
    "directory_path is the error logs path as well as the location where the combined error log file will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined error logs saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_error_logs(directory_path):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.startswith(\"error\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Normalize the 'TICKET_ID' to ensure duplicates are identified\n",
    "    combined_df['TICKET_ID'] = combined_df['TICKET_ID'].str.upper()\n",
    "\n",
    "    combined_df = combined_df.drop_duplicates(subset='TICKET_ID')\n",
    "\n",
    "    output_path = os.path.join(directory_path, 'error_logs.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Combined error logs saved to: {output_path}\")\n",
    "    return output_path, combined_df\n",
    "\n",
    "directory_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\"\n",
    "output_path, combined_df = combine_error_logs(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To cleanse user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_43760\\4048578058.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import html\n",
    "\n",
    "\n",
    "# Function to decode HTML entities in a DataFrame\n",
    "def decode_html_entities(df):\n",
    "    df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "    return df_decoded\n",
    "\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\USER.txt\" \n",
    "\n",
    "# Read the raw file content\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_data = file.readlines()\n",
    "    \n",
    "# Split headers and data    \n",
    "header = raw_data[0].replace(\"&#124;\", \"|\").strip()\n",
    "data = [line.replace(\"&#124;\", \"|\").strip() for line in raw_data[1:]]\n",
    "\n",
    "# Read the cleaned data into a pandas DataFrame\n",
    "df_cleaned = pd.DataFrame([line.split('|') for line in data], columns=header.split('|'))\n",
    "\n",
    "# Decode HTML entities\n",
    "df_cleaned = decode_html_entities(df_cleaned)\n",
    "df_cleaned\n",
    "\n",
    "# Remove the quotes in Dataframe\n",
    "def remove_quotes(df):\n",
    "    df.columns = df.columns.str.replace('\"', '')\n",
    "    df = df.apply(lambda col: col.str.replace('\"', '', regex=True))\n",
    "    return df\n",
    "\n",
    "df_cleaned = remove_quotes(df_cleaned)\n",
    "df_cleaned\n",
    "df_cleaned.to_csv(\"user_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_15156\\3036651791.py:7: DtypeWarning: Columns (1,8,17,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep=',', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Option 1: Check for quoted fields or use different delimiter\n",
    "try:\n",
    "    df = pd.read_csv(path, sep=',', on_bad_lines='skip') \n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricaredatareal_20200101_20200201.csv\n",
      "Problematic data saved to bricaredatareal_problematic_20200101_20200201.csv\n",
      "Number of rows in dataframe: 463581\n",
      "Number of problematic lines: 881\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"\n",
    "]\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Initialize lists to store data and problematic lines\n",
    "data = []\n",
    "problematic_lines = []\n",
    "\n",
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    try:\n",
    "        # Identifying Ticket_ID\n",
    "        ticket_id_index = next(i for i, part in enumerate(parts) if part.startswith('TTB'))\n",
    "        ticket_id = parts[ticket_id_index]\n",
    "        \n",
    "        # Identifying Call_Type_ID\n",
    "        call_type_id_index = ticket_id_index + 1\n",
    "        call_type_id = parts[call_type_id_index]\n",
    "        \n",
    "        # Identifying Create_Date (the part that looks like a datetime string)\n",
    "        create_date_index = next(i for i, part in enumerate(parts) if '-' in part and ':' in part)\n",
    "        create_date = parts[create_date_index]\n",
    "        \n",
    "        # Call_Type is everything between Call_Type_ID and Create_Date\n",
    "        call_type = ' '.join(parts[call_type_id_index + 1:create_date_index])\n",
    "        \n",
    "        # The rest of the fields in order\n",
    "        rest = parts[create_date_index + 1:]\n",
    "        \n",
    "        # Combine into a single list in the correct order\n",
    "        structured_line = [ticket_id, call_type_id, call_type, create_date] + rest\n",
    "        \n",
    "        # If the length is correct, return it\n",
    "        if len(structured_line) == len(column_list):\n",
    "            return structured_line\n",
    "        # If there are extra fields, handle them (for example, by merging or ignoring)\n",
    "        elif len(structured_line) > len(column_list):\n",
    "            return structured_line[:len(column_list)]\n",
    "        else:\n",
    "            return None\n",
    "    except StopIteration:\n",
    "        # If any part of the parsing fails, consider the line problematic\n",
    "        return None\n",
    "\n",
    "# Read the file line by line and parse it\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        parsed_line = parse_line(lines[i])\n",
    "        if parsed_line:\n",
    "            data.append(parsed_line)\n",
    "            i += 1\n",
    "        else:\n",
    "            # Check if merging the next line solves the issue\n",
    "            if i + 1 < len(lines):\n",
    "                merged_line = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "                parsed_line = parse_line(merged_line)\n",
    "                if parsed_line:\n",
    "                    data.append(parsed_line)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    problematic_lines.append((i, lines[i]))\n",
    "                    i += 1\n",
    "            else:\n",
    "                problematic_lines.append((i, lines[i]))\n",
    "                i += 1\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=column_list)\n",
    "\n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].astype(str)\n",
    "df = df[df['Ticket_ID'].str.match(r'TTB\\d+')]\n",
    "\n",
    "df['Call_Type_ID'] = df['Call_Type_ID'].astype(str)\n",
    "df = df[df['Call_Type_ID'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "df = df.dropna(subset=['Create_Date'])\n",
    "\n",
    "df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricaredatareal_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n",
    "problematic_df = pd.DataFrame(problematic_lines, columns=['Line_Number', 'Data'])\n",
    "\n",
    "\n",
    "problematic_filename = f\"bricaredatareal_problematic_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "problematic_df.to_csv(problematic_filename, index=False)\n",
    "\n",
    "print(f\"Problematic data saved to {problematic_filename}\")\n",
    "\n",
    "\n",
    "print(f\"Number of rows in dataframe: {len(df)}\")\n",
    "print(f\"Number of problematic lines: {len(problematic_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create masked Data for UAT\n",
    "\n",
    "\n",
    "To Mask:\n",
    "\n",
    "Nama_Nasabah\n",
    "No_Rekening\n",
    "full_name\n",
    "no_telepon\n",
    "approver_name = beda dengan Nama_Nasabah\n",
    "user_login_name = beda \n",
    "Log_Name\n",
    "Nama_Supervisor\n",
    "Nama_TL\n",
    "Nama_Wakabag\n",
    "Remark\n",
    "\n",
    "Kolom Detail:\n",
    "\n",
    "Kode Cabang          : 0307 4 digits\n",
    "No Kartu             : 5221843130736932 16 digits\n",
    "No Rekening          : 030701098507501 15 digits\n",
    "Nama                 : SITI SHOLEHA\n",
    "No ID                : 3316022012770004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random \n",
    "\n",
    "fake = Faker('id_ID')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "def mask_data(df):\n",
    "    df['Nama_Nasabah'] = df['Nama_Nasabah'].apply(lambda x: fake.name())\n",
    "    df['No_Rekening'] = df['No_Rekening'].apply(lambda x: fake.bban())\n",
    "    df['full_name'] = df['full_name'].apply(lambda x: fake.name())\n",
    "    df['no_telepon'] = df['no_telepon'].apply(lambda x: fake.phone_number())\n",
    "    df['approver_name'] = df['approver_name'].apply(lambda x: fake.name())\n",
    "    df['user_login_name'] = df['user_login_name'].apply(lambda x: fake.user_name())\n",
    "    df['Log_Name'] = df['Log_Name'].apply(lambda x: fake.user_name())\n",
    "    df['Nama_Supervisor'] = df['Nama_Supervisor'].apply(lambda x: fake.name())\n",
    "    df['Nama_TL'] = df['Nama_TL'].apply(lambda x: fake.name())\n",
    "    df['Nama_Wakabag'] = df['Nama_Wakabag'].apply(lambda x: fake.name())\n",
    "    return df\n",
    "\n",
    "\n",
    "df_masked = mask_data(df)\n",
    "\n",
    "def generate_nik():\n",
    "    return f'{fake.random_number(digits=16)}'\n",
    "\n",
    "def mask_detail(detail):\n",
    "    if isinstance(detail, float) and pd.isna(detail):\n",
    "        return detail\n",
    "    lines = str(detail).split('\\n')\n",
    "    masked_lines = []\n",
    "    for line in lines:\n",
    "        if 'Kode Cabang' in line:\n",
    "            line = f'Kode Cabang          : {fake.random_number(digits=4)}'\n",
    "        elif 'No Kartu' in line:\n",
    "            line = f'No Kartu             : {fake.credit_card_number()}'\n",
    "        elif 'No Rekening' in line:\n",
    "            line = f'No Rekening          : {fake.bban()}'\n",
    "        elif 'Nama' in line:\n",
    "            line = f'Nama                 : {fake.name()}'\n",
    "        elif 'No ID' in line:\n",
    "            line = f'No ID                : {generate_nik()}'\n",
    "        masked_lines.append(line)\n",
    "    return '\\n'.join(masked_lines)\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "\n",
    "\n",
    "# Ensure 'Details' column is treated as string and apply mask_detail function\n",
    "df_masked['Details'] = df_masked['Details'].astype(str).apply(mask_detail)\n",
    "\n",
    "\n",
    "\n",
    "# gateway_options = [\n",
    "#     'Email', 'Phone', 'Instagram', 'Walk-In', 'MMS', \n",
    "#     'Twitter', 'Facebook', 'BRImo', 'BRILink', 'Sabrina', 'Ceria'\n",
    "# ]\n",
    "\n",
    "#UAT\n",
    "gateway_options = [\n",
    "    'Email', 'Phone', 'Web', 'Facebook', 'Twitter'\n",
    "]\n",
    "# Assign random values to the 'gateway' column\n",
    "df_masked['gateway'] = df_masked['gateway'].apply(lambda x: random.choice(gateway_options))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "df_masked\n",
    "output_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101_masking.csv\"\n",
    "df_masked.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"bricare_uat_20230101_20230101_masking.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['status']='New'\n",
    "df.to_csv(r\"bricare_uat_20230101_20230101_masking_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in SIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\SIT\\dummy_data_case1100000.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "# df['User*']='00GMR0000000dCf2AI'\n",
    "df['User*']='00GMR0000000dEH2AY'\n",
    "df['Status']='Escalated'\n",
    "df['Status'] = df['Status'].replace('New', 'Escalated')\n",
    "df['Case Origin']='Email'\n",
    "df['Case Type']='a1BMR00000010h72AA'\n",
    "df['Account']='001MR000004C7l2YAC'\n",
    "\n",
    "df.to_csv('dummy_data_uat60k.csv', index=False)\n",
    "\n",
    "# df['Merchant ID']\n",
    "# df['TID']\n",
    "# df['User*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in UAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "df.to_csv('dummy_data_case_uat60k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\CHM\\CHM\\Part2\\bricare_get_cleansing_77_kolom(1).csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "\n",
    "output_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\bricare_get_cleansing_77_kolom_fixed_return.csv\"\n",
    "df=df.iloc[:1]\n",
    "df['status']='Return'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# output_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "ref_fiturid_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\ref_fiturid.csv\"\n",
    "id_calltype_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_calltype_uat.csv\"\n",
    "\n",
    "ref_fiturid = pd.read_csv(ref_fiturid_path)\n",
    "id_calltype = pd.read_csv(id_calltype_path)\n",
    "\n",
    "# Convert calltype and NAME columns to string type\n",
    "ref_fiturid['calltype'] = ref_fiturid['calltype'].astype(str)\n",
    "id_calltype['NAME'] = id_calltype['NAME'].astype(str)\n",
    "\n",
    "# Merge the dataframes to add the Call_Type_ID column based on the calltype column\n",
    "ref_fiturid = ref_fiturid.merge(id_calltype[['NAME', 'ID']], left_on='calltype', right_on='NAME', how='left')\n",
    "\n",
    "# Rename the ID column to Call_Type_ID\n",
    "ref_fiturid.rename(columns={'ID': 'Call_Type_ID'}, inplace=True)\n",
    "\n",
    "# Drop the unnecessary NAME column from the merge\n",
    "ref_fiturid.drop(columns=['NAME'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\"\n",
    "# ref_fiturid=ref_fiturid.iloc[62:63]\n",
    "ref_fiturid.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Updated CSV file saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>Call Type Info</th>\n",
       "      <th>Type</th>\n",
       "      <th>User</th>\n",
       "      <th>Title</th>\n",
       "      <th>Detail Bricare</th>\n",
       "      <th>Deskripsi</th>\n",
       "      <th>Empati</th>\n",
       "      <th>Konfirmasi</th>\n",
       "      <th>...</th>\n",
       "      <th>Verifikasi</th>\n",
       "      <th>Gali Informasi</th>\n",
       "      <th>Pembuatan Laporan</th>\n",
       "      <th>Konfirmasi Ulang</th>\n",
       "      <th>Percepatan Komplain</th>\n",
       "      <th>Solusi, Informasi</th>\n",
       "      <th>Edukasi &amp; Cross Selling</th>\n",
       "      <th>Closing</th>\n",
       "      <th>Article Type</th>\n",
       "      <th>URL_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>Nasabah Menanyakan Informasi Pengajuan Terkait...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1000 - Nasabah Menanyakan Informasi Pengajuan ...</td>\n",
       "      <td>nomor ponsel nasabah yang bisa dihubungi\\nID C...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...</td>\n",
       "      <td>Jika Nasabah menanyakan Cara &amp; Syarat Pengajua...</td>\n",
       "      <td>...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nVerifikas...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nGali Info...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nBuat lapo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nInformasi...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1002</td>\n",
       "      <td>Nasabah Menyakan Terkait Promo dan Program CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1002 - Nasabah Menyakan Terkait Promo dan Prog...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\nJika...</td>\n",
       "      <td>Konfirmasi :\\nJika Nasabah menanyakan promo ce...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi\\nTidak dilakukan Verifikasi\\n</td>\n",
       "      <td>Gali Informasi\\t\\n\\t i. Jenis promo/...</td>\n",
       "      <td>Pembuatan Laporan\\t\\n\\ti. Nomor laporan ti...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\t\\n\\t  i.              \\...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1003</td>\n",
       "      <td>Nasabah Mengajukan Pelunasan Awal Cicilan CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1003 - Nasabah Mengajukan Pelunasan Awal Cicil...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\n\"Bap...</td>\n",
       "      <td>Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...</td>\n",
       "      <td>Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi\\n1.\\tCicilan telah berjalan...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1005</td>\n",
       "      <td>Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1005 - Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>Nasabah mengajukan pemblokiran Sementara Akun ...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati\\nJika nasabah infokan ingin melakukan p...</td>\n",
       "      <td>Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...</td>\n",
       "      <td>Pembuatan Laporan\\nNasabah mengajukan pembloki...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\n1.\\tAgent Konfirmasi ke...</td>\n",
       "      <td>Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...</td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1005-Nasabah-Mengajukan-Pemblokiran-CERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1008</td>\n",
       "      <td>Nasabah Mengajukan Pengaktifan Akun Ceria Terb...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1008 - Nasabah Mengajukan Pengaktifan Akun Cer...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 Call Type                                     Call Type Info  \\\n",
       "0            1      1000  Nasabah Menanyakan Informasi Pengajuan Terkait...   \n",
       "1            2      1002   Nasabah Menyakan Terkait Promo dan Program CERIA   \n",
       "2            3      1003    Nasabah Mengajukan Pelunasan Awal Cicilan CERIA   \n",
       "3            4      1005               Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4            5      1008  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...   \n",
       "..         ...       ...                                                ...   \n",
       "437                                                                           \n",
       "438                                                                           \n",
       "439                                                                           \n",
       "440                                                                           \n",
       "441                                                                           \n",
       "\n",
       "      Type                  User  \\\n",
       "0    CERIA  Agent Contact Center   \n",
       "1    CERIA  Agent Contact Center   \n",
       "2    CERIA  Agent Contact Center   \n",
       "3    CERIA  Agent Contact Center   \n",
       "4    CERIA  Agent Contact Center   \n",
       "..     ...                   ...   \n",
       "437                                \n",
       "438                                \n",
       "439                                \n",
       "440                                \n",
       "441                                \n",
       "\n",
       "                                                 Title  \\\n",
       "0    1000 - Nasabah Menanyakan Informasi Pengajuan ...   \n",
       "1    1002 - Nasabah Menyakan Terkait Promo dan Prog...   \n",
       "2    1003 - Nasabah Mengajukan Pelunasan Awal Cicil...   \n",
       "3          1005 - Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4    1008 - Nasabah Mengajukan Pengaktifan Akun Cer...   \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Detail Bricare  \\\n",
       "0    nomor ponsel nasabah yang bisa dihubungi\\nID C...   \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Nasabah mengajukan pemblokiran Sementara Akun ...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                             Deskripsi  \\\n",
       "0    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "1    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "2    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "3    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                                Empati  \\\n",
       "0    Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...   \n",
       "1    Empati sesuai kondisi (pilih salah satu)\\nJika...   \n",
       "2    Empati sesuai kondisi (pilih salah satu)\\n\"Bap...   \n",
       "3    Empati\\nJika nasabah infokan ingin melakukan p...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                            Konfirmasi  ...  \\\n",
       "0    Jika Nasabah menanyakan Cara & Syarat Pengajua...  ...   \n",
       "1    Konfirmasi :\\nJika Nasabah menanyakan promo ce...  ...   \n",
       "2    Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...  ...   \n",
       "3    Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...  ...   \n",
       "4                                                       ...   \n",
       "..                                                 ...  ...   \n",
       "437                                                     ...   \n",
       "438                                                     ...   \n",
       "439                                                     ...   \n",
       "440                                                     ...   \n",
       "441                                                     ...   \n",
       "\n",
       "                                            Verifikasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nVerifikas...   \n",
       "1             Verifikasi\\nTidak dilakukan Verifikasi\\n   \n",
       "2    Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...   \n",
       "3    Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Gali Informasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nGali Info...   \n",
       "1    Gali Informasi\\t\\n\\t i. Jenis promo/...   \n",
       "2    Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...   \n",
       "3    Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                     Pembuatan Laporan Konfirmasi Ulang  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nBuat lapo...                    \n",
       "1    Pembuatan Laporan\\t\\n\\ti. Nomor laporan ti...                    \n",
       "2    Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...                    \n",
       "3    Pembuatan Laporan\\nNasabah mengajukan pembloki...                    \n",
       "4                                                                         \n",
       "..                                                 ...              ...   \n",
       "437                                                                       \n",
       "438                                                                       \n",
       "439                                                                       \n",
       "440                                                                       \n",
       "441                                                                       \n",
       "\n",
       "    Percepatan Komplain                                  Solusi, Informasi  \\\n",
       "0                        1\\tCara & Syarat pengajuan Aplikasi\\nInformasi...   \n",
       "1                        Informasi & Solusi \\t\\t\\n\\t  i.              \\...   \n",
       "2                        Informasi & Solusi\\n1.\\tCicilan telah berjalan...   \n",
       "3                        Informasi & Solusi \\t\\n1.\\tAgent Konfirmasi ke...   \n",
       "4                                                                            \n",
       "..                  ...                                                ...   \n",
       "437                                                                          \n",
       "438                                                                          \n",
       "439                                                                          \n",
       "440                                                                          \n",
       "441                                                                          \n",
       "\n",
       "                               Edukasi & Cross Selling  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                               Closing         Article Type  \\\n",
       "0    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "1    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "2    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "3    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "4                                                       Working Instruction   \n",
       "..                                                 ...                  ...   \n",
       "437                                                     Working Instruction   \n",
       "438                                                     Working Instruction   \n",
       "439                                                     Working Instruction   \n",
       "440                                                     Working Instruction   \n",
       "441                                                     Working Instruction   \n",
       "\n",
       "                                              URL_name  \n",
       "0    1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...  \n",
       "1    1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...  \n",
       "2    1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...  \n",
       "3            1005-Nasabah-Mengajukan-Pemblokiran-CERIA  \n",
       "4    1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...  \n",
       "..                                                 ...  \n",
       "437                                                     \n",
       "438                                                     \n",
       "439                                                     \n",
       "440                                                     \n",
       "441                                                     \n",
       "\n",
       "[375 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare(WI).csv\"\n",
    "df=pd.read_csv(path, encoding='ISO-8859-1')\n",
    "df = df.fillna('')\n",
    "df['Article Type']='Working Instruction'\n",
    "\n",
    "output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\artikel_all.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Change 'Brilink' to 'BRILink' in 'Type' column\n",
    "df['Type'] = df['Type'].replace('Brilink', 'BRILink')\n",
    "\n",
    "# Remove rows with 'Wholesale' in 'Type' column\n",
    "df = df[df['Type'] != 'Wholesale']\n",
    "\n",
    "# df['Type'].unique()\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('artikel2.csv', index= False)\n",
    "# df.iloc[:1].to_csv(output, index= False)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df= pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424041601915.csv\")\n",
    "df.to_csv('error.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare 2.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path)\n",
    "df=df.iloc[:1]\n",
    "# df['Gali Informasi']\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "df.to_csv('artikel_bullet.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate 1k roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/extract_role.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the uploaded CSV file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/extract_role.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m roles_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the content of the CSV file to understand its structure\u001b[39;00m\n\u001b[0;32m      8\u001b[0m roles_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/extract_role.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '/mnt/data/extract_role.csv'\n",
    "roles_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the content of the CSV file to understand its structure\n",
    "roles_df.head()\n",
    "\n",
    "\n",
    "# Number of additional roles needed to reach a total of 1000\n",
    "num_additional_roles_needed = 1000 - len(expanded_roles_df)\n",
    "\n",
    "# Duplicate the existing roles to meet the required number of roles\n",
    "additional_roles_more = expanded_roles_df.sample(num_additional_roles_needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "# Generate unique names for the additional roles\n",
    "additional_roles_more['NAME'] = additional_roles_more['NAME'] + \"_more_\" + (additional_roles_more.index + 1).astype(str)\n",
    "\n",
    "# Combine the original roles with the additional roles\n",
    "expanded_roles_df_more = pd.concat([expanded_roles_df, additional_roles_more], ignore_index=True)\n",
    "\n",
    "# Ensure we have a total of 1000 roles\n",
    "expanded_roles_df_more = expanded_roles_df_more.head(1000)\n",
    "\n",
    "# Save the expanded roles to a new CSV file\n",
    "output_file_path_more = '/mnt/data/expanded_roles_1000.csv'\n",
    "expanded_roles_df_more.to_csv(output_file_path_more, index=False)\n",
    "\n",
    "output_file_path_more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Extraction Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check data before 2023 = 27 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns in DataFrame: {'Tgl_Assigned'}\n",
      "Extra columns in DataFrame: {'TglAssigned', 'cifno', 'Details'}\n"
     ]
    }
   ],
   "source": [
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_27_kolom.csv\"\n",
    "path2=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_79_kolom.csv\"\n",
    "# df= pd.read_csv(path, delimiter=';')\n",
    "df= pd.read_csv(path2, delimiter=';')\n",
    "\n",
    "\n",
    "# column_list_set = set(column_list)\n",
    "column_list_set = set(column_names)\n",
    "df_columns_set = set(df.columns)\n",
    "\n",
    "missing_columns = column_list_set - df_columns_set\n",
    "extra_columns = df_columns_set - column_list_set\n",
    "\n",
    "if not missing_columns and not extra_columns:\n",
    "    print(\"The column names match.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "    if extra_columns:\n",
    "        print(f\"Extra columns in DataFrame: {extra_columns}\")\n",
    "\n",
    "################################################\n",
    "# column_list_set2 = set(column_names)\n",
    "# df_columns_set2 = set(df2.columns)\n",
    "\n",
    "# missing_columns2 = column_list_set2 - df_columns_set2\n",
    "# extra_columns2 = df_columns_set2 - column_list_set2\n",
    "\n",
    "# if not missing_columns and not extra_columns:\n",
    "#     print(\"The column names match.\")\n",
    "# else:\n",
    "#     if missing_columns2:\n",
    "#         print(f\"Missing columns in DataFrame: {missing_columns2}\")\n",
    "#     if extra_columns2:\n",
    "#         print(f\"Extra columns in DataFrame: {extra_columns2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data dengan 27 kolom:\n",
    "- jumlah kolom sudah ok\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- attachment\n",
    "- penambahan nama file diakhir \"27\"\n",
    "\n",
    "\n",
    "\n",
    "Data dengan 79 kolom:\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- penambahan nama file diakhir \"79\"\n",
    "\n",
    "\n",
    "\n",
    "Data Zendesk:\n",
    "- harus hilangkan double quotes \"\"\n",
    "- delimiternya ;\n",
    "- nama file zendesk\n",
    "\n",
    "\n",
    "Data Omni:\n",
    "- SMS: harus format csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Unable to create/update fields: LastModifiedDa...\n",
       "1    Unable to create/update fields: LastModifiedDa...\n",
       "2    Unable to create/update fields: LastModifiedDa...\n",
       "3    Unable to create/update fields: LastModifiedDa...\n",
       "4    Unable to create/update fields: LastModifiedDa...\n",
       "5    Unable to create/update fields: LastModifiedDa...\n",
       "6    Unable to create/update fields: LastModifiedDa...\n",
       "7    Unable to create/update fields: LastModifiedDa...\n",
       "8    Unable to create/update fields: LastModifiedDa...\n",
       "9    Unable to create/update fields: LastModifiedDa...\n",
       "Name: ERROR, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060524083553095.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['ERROR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zendesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
      "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
      "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
      "\n",
      "     Requester name                                     Ticket subject  \\\n",
      "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
      "1  Masruroh Sodikin                                                  P   \n",
      "\n",
      "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
      "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
      "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
      "\n",
      "  Ticket solved - Timestamp                 Tickets  \n",
      "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
      "1       2024-01-01T00:46:27  1.00000000000000000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket ID</th>\n",
       "      <th>Ticket channel</th>\n",
       "      <th>Assignee ID</th>\n",
       "      <th>Assignee name</th>\n",
       "      <th>Requester ID</th>\n",
       "      <th>Requester name</th>\n",
       "      <th>Ticket subject</th>\n",
       "      <th>Requester created - Timestamp</th>\n",
       "      <th>Ticket created - Timestamp</th>\n",
       "      <th>Ticket solved - Timestamp</th>\n",
       "      <th>Tickets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3435272</td>\n",
       "      <td>Any channel</td>\n",
       "      <td>405257525413</td>\n",
       "      <td>Agent Sosmed 4</td>\n",
       "      <td>27064719563033</td>\n",
       "      <td>vianovia94</td>\n",
       "      <td>[IGC] Minimal opo boloo... ......#relsvideo #r...</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:30:56</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3435273</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>405257335633</td>\n",
       "      <td>Agent Sosmed 3</td>\n",
       "      <td>27064768687769</td>\n",
       "      <td>Masruroh Sodikin</td>\n",
       "      <td>P</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:46:27</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
       "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
       "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
       "\n",
       "     Requester name                                     Ticket subject  \\\n",
       "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
       "1  Masruroh Sodikin                                                  P   \n",
       "\n",
       "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
       "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
       "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
       "\n",
       "  Ticket solved - Timestamp                 Tickets  \n",
       "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
       "1       2024-01-01T00:46:27  1.00000000000000000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\Data Zendesk 1-15 Januari 2024 - sampel.csv\"\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "# Read the CSV file with proper handling of quotes and custom separator\n",
    "with open(file_path, 'r', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "    rows = [row for row in reader]\n",
    "\n",
    "# Split the combined columns that have commas within them\n",
    "split_rows = []\n",
    "for row in rows:\n",
    "    split_row = row[0].split(',') + row[1:]  # Split the first column and add the rest as they are\n",
    "    split_rows.append(split_row)\n",
    "\n",
    "# Create DataFrame from the processed rows\n",
    "df = pd.DataFrame(split_rows[1:], columns=split_rows[0])\n",
    "\n",
    "# Remove double quotes from column names\n",
    "df.columns = df.columns.str.replace('\"', '')\n",
    "\n",
    "# Remove double quotes from all data in the DataFrame\n",
    "df = df.replace('\"', '', regex=True)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "print(df.head())\n",
    "df\n",
    "\n",
    "# If you need to save the cleaned DataFrame to a new CSV file\n",
    "# cleaned_file_path = '/mnt/data/Cleaned_Zendesk_Data.csv'\n",
    "# df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# df=pd.read_csv(path)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# df\u001b[39;00m\n\u001b[0;32m     10\u001b[0m path2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaste\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataloader_v60.0.2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata check\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnew as per 5 June\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcase.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\bricare_20221213_20240604_case_account.csv\"\n",
    "# df=pd.read_csv(path)\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "path2=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\case.csv\"\n",
    "df=pd.read_csv(path2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy data for Alex7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for account with quoting (DONE)\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_dummy_data(file_path, file_name, num_rows):\n",
    "    # Define possible values for each column\n",
    "    account_names = ['John Doe', 'Jane Smith', 'Mike Brown', 'Lisa Green', 'Mark Taylor']\n",
    "    account_owners = ['Owner A', 'Owner B', 'Owner C', 'Owner D']\n",
    "    nasabah_types = ['Nasabah', 'Non Nasabah']\n",
    "    account_record_types = ['Personal', 'Non Personal']\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Writing to the CSV file\n",
    "    with open(os.path.join(file_path, file_name), mode='w', newline='') as file:\n",
    "        writer = csv.writer(file, quotechar='\"', quoting=csv.QUOTE_ALL)  # Enforce quoting for all fields\n",
    "        writer.writerow(['Account Name', 'CIF No', 'Account Owner', 'No Telp', 'Email', 'Nasabah Type', 'Account Record Type'])\n",
    "        \n",
    "        for _ in range(num_rows):\n",
    "            account_name = random.choice(account_names)\n",
    "            email = f\"{account_name.split(' ')[0].lower()}.{account_name.split(' ')[1].lower()}@example.com\"\n",
    "            writer.writerow([\n",
    "                account_name,\n",
    "                ''.join([\"{}\".format(random.randint(0, 9)) for _ in range(10)]),  # 10-digit CIF No\n",
    "                random.choice(account_owners),\n",
    "                f'+62{random.randint(1000000000, 9999999999)}',  # Phone number\n",
    "                email,\n",
    "                random.choice(nasabah_types),\n",
    "                random.choice(account_record_types)\n",
    "            ])\n",
    "\n",
    "\n",
    "file_path = 'D:\\dataquality2'  # Adjust the path as needed\n",
    "file_name = 'dummy_data_account.csv'\n",
    "num_rows = 5  # Adjust the number of rows as needed\n",
    "\n",
    "generate_dummy_data(file_path, file_name, num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_salesforce_case_dummy_data(num_rows=100):\n",
    "    import pandas as pd\n",
    "    import random\n",
    "\n",
    "    # Define the columns and possible values\n",
    "    statuses = [\"New\", \"Working\", \"Escalated\"]\n",
    "    types = [\"Electronic\", \"Electrical\", \"Mechanical\"]\n",
    "    case_reasons = [\"Performance\", \"Breakdown\"]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        \"Status\": [random.choice(statuses) for _ in range(num_rows)],\n",
    "        \"Type\": [random.choice(types) for _ in range(num_rows)],\n",
    "        \"Case Reason\": [random.choice(case_reasons) for _ in range(num_rows)],\n",
    "        \"Subject\": [f\"Subject {i+1}\" for i in range(num_rows)],\n",
    "        \"Description\": [f\"Description of issue {i+1}\" for i in range(num_rows)],\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = 'dummy_casefor_alex9.csv'\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "# Example usage\n",
    "file_path = create_salesforce_case_dummy_data(num_rows=10)\n",
    "file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script for data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRICARE:\n",
    "\n",
    "BRICARE consists of 2 different types of files by year:\n",
    "\n",
    "a. File after 2022 (2023-2024) = 79 kolom\n",
    "\n",
    "\n",
    "b. File before 2022 (2019-2022) = 27 kolom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case Origin\n",
    "\n",
    "origin_mappings = {\n",
    "    \"Sms\": \"SMS\",\n",
    "    \"Telegram\": \"Telegram\",\n",
    "    \"WebChat\": \"WebChat\",\n",
    "    \"Sabrina\": \"Sabrina\",\n",
    "    \"Twitter\": \"Twitter\",\n",
    "    \"Facebook\": \"Facebook\",\n",
    "    \"Instagram\": \"Instagram\",\n",
    "    \"Pesan IB\": \"Pesan IB\",\n",
    "    \"Email\": \"Email\",\n",
    "    \"Video Banking\": \"Video Banking\",\n",
    "    \"Media Cetak\": \"Media Cetak\",\n",
    "    \"Media Online\": \"Media Online\",\n",
    "    \"OJK\": \"OJK\",\n",
    "    \"Lembaga Hukum Lainya\": \"Lembaga Hukum Lainya\",\n",
    "    \"Media Umum Lainya\": \"Media Umum Lainya\",\n",
    "    \"Phone\": \"Phone\",\n",
    "    \"Surat\": \"Surat\",\n",
    "    \"Walk In\": \"Walk In\",\n",
    "    \"ËDC & Brilink\": \"EDC\",\n",
    "    \"Brilink\": \"BRILink\",\n",
    "    \"BRIMO\": \"BRIMO\",\n",
    "    \"PPID\": \"PPID\",\n",
    "    \"SP4N LAPOR\": \"SP4N LAPOR\",\n",
    "    \"Media Konsumen\": \"Media Konsumen\",\n",
    "    \"Bank Indonesia\": \"Bank Indonesia\",\n",
    "    \"Ombudsman\": \"Ombudsman\",\n",
    "    \"BPKN\": \"BPKN\",\n",
    "    \"MMS\": \"MMS\",\n",
    "    \"PKSS Oten\": \"PKSS Oten\",\n",
    "    \"BRIPENS\": \"BRIPENS\",\n",
    "    \"Ceria\": \"Ceria\"\n",
    "}\n",
    "len(origin_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type A\n",
    "\n",
    "\n",
    "Data Extraction for File Type A must be 2 Files:\n",
    "\n",
    "\n",
    "A.1 Columns (without \"Details\")\n",
    "\n",
    "\n",
    "A.2 Details only \n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['Create_Date','TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\"\n",
    "\n",
    "- Columns shoud be mapped based on their Call_Type_ID:\n",
    "\n",
    "['Produk','Jenis_Produk','Jenis_Laporan']\n",
    "\n",
    "- PLEASE ADD CIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_uat20230101_20230101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('NULL', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('None', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_file(file_path):\n",
    "\n",
    "    data = []\n",
    "    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "\n",
    "            date_index = next(i for i, part in enumerate(parts) if date_pattern.match(part))\n",
    "\n",
    "            ticket_id = parts[0] \n",
    "            call_type_id = parts[1]  \n",
    "            description = ';'.join(parts[2:date_index])  \n",
    "            create_date = parts[date_index]  \n",
    "\n",
    "      \n",
    "            data.append([ticket_id, call_type_id, description, create_date] + parts[date_index + 1:])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.txt\"\n",
    "\n",
    "df = parse_file(file_path)\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "   \n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_uat{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bricare_20230101_20230101.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if len(parts) > 78:\n",
    "                description = ';'.join(parts[2:-75])\n",
    "                new_parts = parts[:2] + [description] + parts[-75:]\n",
    "                data.append(new_parts)\n",
    "            else:\n",
    "                data.append(parts)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    df = df.replace(['0', 0], '')\n",
    "\n",
    "    columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "    for column in columns_to_convert:\n",
    "        df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "    \n",
    "    df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "    return df\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.csv\"\n",
    "df = parse_csv(file_path)\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Type di tes MMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Data Mapping - Case_Type.csv\"\n",
    "df=pd.read_csv(path)\n",
    "# case_types = ['8634', '8635', '8636', '8623', '8624', '8628', '8629', '8630', '8412', '8625', '8615', '8616', '8617', '8618', '8619', '8638', '8620']\n",
    "\n",
    "case_types = ['8412']\n",
    "# Filter the dataframe\n",
    "df = df[df['Case Types'].isin(case_types)]\n",
    "df['Segment'] = df['Segment'].replace('All', '')\n",
    "df\n",
    "df.to_csv(r'C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Case_Type_MMS2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Type Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the fix document\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"D:\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan (5).xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='FIX Upload - Update Here')\n",
    "\n",
    "#df = df.drop(columns=['Salesforce', 'Status.1', 'Revisi Detail', 'Notes', 'User Grouping_Old', 'T4.3 - Technical Team', 'T5.1 - Third Party', 'T5.2 - Third Party', 'T5.3 - Third Party', 'Jenis Produk dan atau Layanan LKPBU', 'Kategori Permasalahan LKPBU', 'Jenis Transaksi LKPBU', 'Penyebab Pengaduan LKPBU', 'Jenis Produk LBUT', 'Sandi Jenis  Produk LBUT', 'Sebab Pengaduan LBUT', 'Sandi Penyebab LBUT'])\n",
    "\n",
    "\n",
    "# df=df.drop(columns=['Actual SLA\\n(days) Old','Target SLA \\n(days) Old', 'BRI Notes','Team Name', 'New SLA', 'Status', 'crosscek', 'GIC', 'Maker', 'Checker', 'Signer', 'Courtesy Awal', 'Kebutuhan Data'])\n",
    "\n",
    "columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "df[columns_to_replace] = df[columns_to_replace].replace('-', '')\n",
    "\n",
    "if 'Active' in df.columns:\n",
    "    df['Active'] = df['Active'].astype(str).str.upper()\n",
    "\n",
    "\n",
    "df['Detail BRICare'] = df['Detail BRICare'].fillna('-')\n",
    "df\n",
    "\n",
    "def clean_case_types(value):\n",
    "    if pd.isna(value) or value == '':\n",
    "        return '0'\n",
    "    match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "    if match:\n",
    "        year, month = match.groups()\n",
    "        return f'{year}-{int(month)}'\n",
    "    value = re.sub(r'\\.0$', '', str(value))\n",
    "    return value\n",
    "\n",
    "df['Case Types'] = df['Case Types'].apply(clean_case_types) \n",
    "df['external_id'] = df['Case Types']\n",
    "df['send to drone'] = df['send to drone'].astype(str)\n",
    "\n",
    "# Then convert all values in 'Column2' to uppercase\n",
    "df['send to drone'] = df['send to drone'].str.upper()\n",
    "\n",
    "\n",
    "output_path = 'Cleaned4_Call_Type.xlsx'\n",
    "output_path2 = 'Cleaned4_Call_Type.csv'\n",
    "df.to_excel(output_path, index=False)\n",
    "df.to_csv(output_path2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Case Types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Case Types]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all call types are covered\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your CSV files\n",
    "file1_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\calltype_psuyanto.csv\"\n",
    "# file2_path = r\"D:\\dataquality2\\Combined3_Cleaned_Call_Type.csv\"\n",
    "\n",
    "file2_path = r\"D:\\dataquality2\\Cleaned4_Call_Type.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Extract the 'Case Types' column from each DataFrame\n",
    "case_types_1 = df1['calltypeidtxt'].astype(str)\n",
    "case_types_2 = df2['Case Types'].astype(str)\n",
    "\n",
    "# Convert to sets for comparison\n",
    "set_case_types_1 = set(case_types_1)\n",
    "set_case_types_2 = set(case_types_2)\n",
    "\n",
    "# Find values in file1 that are missing in file2\n",
    "missing_values = set_case_types_1 - set_case_types_2\n",
    "\n",
    "# Convert the missing values set to a DataFrame\n",
    "missing_values_df = pd.DataFrame(list(missing_values), columns=['Missing Case Types'])\n",
    "\n",
    "# Save the missing values to a new CSV file\n",
    "# missing_values_df.to_csv('Missing_Case_Types.csv', index=False)\n",
    "missing_values_df\n",
    "# print(f\"Missing values from file1 that are not in file2 have been saved to 'Missing_Case_Types.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique case types: 520\n",
      "Duplicate case types:\n",
      "Number of rows dropped: 0\n",
      "   No Case Types  Active               User Grouping_New        Segment  \\\n",
      "0   1       1000    True               Resolution Center  Individu Umum   \n",
      "1   2       1002    True               Resolution Center  Individu Umum   \n",
      "2   3       1003    True                 Digital Banking  Individu Umum   \n",
      "3   4       1005    True  Card, Digital Lending & Assset  Individu Umum   \n",
      "4   5       1006    True                Operational Risk  Individu Umum   \n",
      "\n",
      "  Product    Sub Product                 Case Category  \\\n",
      "0   Loans  KTA - Digital                       Inquiry   \n",
      "1   Loans  KTA - Digital  Products / Promotion Inquiry   \n",
      "2   Loans  KTA - Digital                       Request   \n",
      "3   Loans  KTA - Digital                       Request   \n",
      "4   Loans  KTA - Digital                       Request   \n",
      "\n",
      "                                Old Case Description  \\\n",
      "0  CERIA - Status Pengajuan, Aplikasi, Cara & Syarat   \n",
      "1                    CERIA - Promo dan Program CERIA   \n",
      "2          CERIA - Permintaan Pelunasan Awal Cicilan   \n",
      "3                          CERIA - Permintaan Blokir   \n",
      "4  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...   \n",
      "\n",
      "                                New Case Description  ...  \\\n",
      "0  Nasabah Menanyakan Informasi Pengajuan Terkait...  ...   \n",
      "1   Nasabah Menyakan Terkait Promo Dan Program Ceria  ...   \n",
      "2    Nasabah Mengajukan Pelunasan Awal Cicilan Ceria  ...   \n",
      "3               Nasabah Mengajukan Pemblokiran Ceria  ...   \n",
      "4  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...  ...   \n",
      "\n",
      "   Eskalasi Team 4 Full Agent CCT/Frontliner  OLA Eskalasi team 1  \\\n",
      "0                   NaN                  1.0                  NaN   \n",
      "1                   NaN                  1.0                  NaN   \n",
      "2                   NaN                  1.0                  2.0   \n",
      "3                   NaN                  1.0                  1.0   \n",
      "4                   NaN                  1.0                  8.0   \n",
      "\n",
      "  OLA Eskalasi team 2 OLA Eskalasi team 3 OLA Eskalasi team 4 Total SLA  \\\n",
      "0                 NaN                 NaN                 NaN         1   \n",
      "1                 NaN                 NaN                 NaN         1   \n",
      "2                 2.0                 NaN                 NaN         5   \n",
      "3                 NaN                 NaN                 NaN         2   \n",
      "4                 NaN                 NaN                 NaN         9   \n",
      "\n",
      "                                      Detail BRICare external_id  Unnamed: 40  \n",
      "0  Information Cara pengajuan Ceria : \\nNasabah m...        1000          1.0  \n",
      "1                                                xxx        1002          NaN  \n",
      "2                                                xxx        1003          NaN  \n",
      "3  Nasabah mengajukan pemblokiran Sementara Akun ...        1005          NaN  \n",
      "4  Nasabah Request Buka/Release Blokir Akun Ceria...        1006          NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Remove duplicates if any\n",
    "\n",
    "# remove any duplicates\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "# path = r\"D:\\dataquality2\\Combined3_Cleaned_Call_Type.csv\"\n",
    "\n",
    "path = r\"D:\\dataquality2\\Cleaned4_Call_Type.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Step 1: Check for duplicates in 'Case Types'\n",
    "unique_case_types = df['Case Types'].unique()\n",
    "num_unique_case_types = len(unique_case_types)\n",
    "\n",
    "print(f\"Number of unique case types: {num_unique_case_types}\")\n",
    "\n",
    "# Find and print duplicate values in the 'Case Types' column\n",
    "duplicate_case_types = df['Case Types'][df['Case Types'].duplicated()].unique()\n",
    "print(\"Duplicate case types:\")\n",
    "for case_type in duplicate_case_types:\n",
    "    print(case_type)\n",
    "\n",
    "# Step 2: Identify and remove rows where 'Case Types' is a duplicate and 'Active' is False\n",
    "\n",
    "# Identify duplicate 'Case Types' values\n",
    "duplicates = df[df.duplicated(subset=['Case Types'], keep=False)]\n",
    "\n",
    "# Find rows where 'Case Types' is a duplicate and 'Active' is False\n",
    "to_drop = duplicates[(duplicates['Active'] == False)]\n",
    "\n",
    "# Drop these rows from the original DataFrame\n",
    "df_cleaned = df.drop(to_drop.index)\n",
    "\n",
    "# Optionally, save the cleaned DataFrame to a new CSV file\n",
    "output_path = r\"D:\\dataquality2\\Combined_Cleaned_Call_Type.csv\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of rows dropped\n",
    "print(f\"Number of rows dropped: {len(to_drop)}\")\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # Define the path to your Excel file\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# # Display all columns in the DataFrame\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Load the Excel file\n",
    "# df = pd.read_excel(path, sheet_name='missing_calltype')\n",
    "\n",
    "# # Replace 0 with NaN and then fill NaN with empty string\n",
    "# df = df.replace(0, np.nan).fillna('')\n",
    "\n",
    "# # Define the columns to check\n",
    "# columns_to_check = [\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner', 'T3.2 - Product Owner', 'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team', 'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# # Add new columns for Level 1-4 OLA\n",
    "# df['Level 1 OLA'] = None\n",
    "# df['Level 2 OLA'] = None\n",
    "# df['Level 3 OLA'] = None\n",
    "# df['Level 4 OLA'] = None\n",
    "\n",
    "# # Function to fill in the OLA levels based on non-empty columns in order\n",
    "# def fill_ola_levels(row):\n",
    "#     values = []\n",
    "#     for col in columns_to_check:\n",
    "#         if pd.notna(row[col]):\n",
    "#             values.append(row[col])\n",
    "#         if len(values) == 4:\n",
    "#             break\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if i < len(values):\n",
    "#             row[f'Level {i+1} OLA'] = values[i]\n",
    "#         else:\n",
    "#             row[f'Level {i+1} OLA'] = None\n",
    "#     return row\n",
    "\n",
    "# # Apply the function to each row\n",
    "# df = df.apply(fill_ola_levels, axis=1)\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'Level 1 OLA',\n",
    "#     'Level 2 OLA',\n",
    "#     'Level 3 OLA',\n",
    "#     'Level 4 OLA',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "        \n",
    "# # Function to remove .0 from float values and handle empty strings\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     if value == '':\n",
    "#         return np.nan\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero).astype('Int64')\n",
    "\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # df = df.drop(columns=columns_to_check)\n",
    "# df.to_csv('Cleaned3_Call_Type_missing.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning missing call type\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # Define the path to your Excel file\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# # Display all columns in the DataFrame\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Load the Excel file\n",
    "# df = pd.read_excel(path, sheet_name='missing_calltype')\n",
    "\n",
    "\n",
    "\n",
    "# # Replace 0 with NaN and then fill NaN with empty string\n",
    "# df = df.replace(0, np.nan).fillna('')\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'No',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Ensure columns_to_format exist in df\n",
    "# existing_columns_to_format = [col for col in columns_to_format if col in df.columns]\n",
    "\n",
    "# # Function to format columns\n",
    "# def format_columns(col):\n",
    "#     try:\n",
    "#         return str(float(col)).rstrip('.0')\n",
    "#     except ValueError:\n",
    "#         return col\n",
    "\n",
    "# # Apply the formatting function to selected columns\n",
    "# df[existing_columns_to_format] = df[existing_columns_to_format].applymap(format_columns)\n",
    "\n",
    "# # Ensure the 'active' column exists before modifying\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # Convert relevant columns to numeric for addition if they exist\n",
    "# numeric_columns = [\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# existing_numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "\n",
    "# for col in existing_numeric_columns:\n",
    "#     df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# # Combine columns with addition\n",
    "# if 'T2.2 - Supporting Unit' in df.columns and 'T2.3 - Supporting Unit' in df.columns:\n",
    "#     df['T2.2 - Supporting Unit'] += df['T2.3 - Supporting Unit']\n",
    "#     df = df.drop(columns=['T2.3 - Supporting Unit'])\n",
    "\n",
    "# if 'T3.1 - Product Owner' in df.columns and 'T3.2 - Product Owner' in df.columns and 'T3.3 - Product Owner' in df.columns:\n",
    "#     df['T3.1 - Product Owner'] += df['T3.2 - Product Owner'] + df['T3.3 - Product Owner']\n",
    "#     df = df.drop(columns=['T3.2 - Product Owner', 'T3.3 - Product Owner'])\n",
    "\n",
    "# if 'T4.1 - Technical Team' in df.columns and 'T4.2 - Technical Team' in df.columns:\n",
    "#     df['T4.1 - Technical Team'] += df['T4.2 - Technical Team']\n",
    "#     df = df.drop(columns=['T4.2 - Technical Team'])\n",
    "\n",
    "# # Format the combined columns to remove .0\n",
    "# combined_columns = ['T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team']\n",
    "# for col in combined_columns:\n",
    "#     if col in df.columns:\n",
    "#         df[col] = df[col].apply(format_columns)\n",
    "\n",
    "# # Columns to replace '-' with empty string\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# # Ensure columns_to_replace exist in df\n",
    "# existing_columns_to_replace = [col for col in columns_to_replace if col in df.columns]\n",
    "\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# df[existing_columns_to_replace] = df[existing_columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# # Add the 'external_id' column\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero)\n",
    "    \n",
    "# # Save the cleaned data to a CSV file\n",
    "# df.to_csv('Cleaned2_Call_Type_missing.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Load the Excel file\n",
    "# file_path = r\"C:\\Users\\maste\\Downloads\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan (2).xlsx\"\n",
    "# df = pd.read_excel(file_path, sheet_name='FIX Upload - Update Here')\n",
    "# # df = df.drop(columns=['Salesforce', 'Status.1', 'Revisi Detail', 'Notes', 'User Grouping_Old', 'T4.3 - Technical Team', 'T5.1 - Third Party', 'T5.2 - Third Party', 'T5.3 - Third Party', 'Jenis Produk dan atau Layanan LKPBU', 'Kategori Permasalahan LKPBU', 'Jenis Transaksi LKPBU', 'Penyebab Pengaduan LKPBU', 'Jenis Produk LBUT', 'Sandi Jenis  Produk LBUT', 'Sebab Pengaduan LBUT', 'Sandi Penyebab LBUT'])\n",
    "\n",
    "# # Define the columns to check\n",
    "# columns_to_check = [\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner', 'T3.2 - Product Owner', 'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team', 'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# # Add new columns for Level 1-4 OLA\n",
    "# df['Level 1 OLA'] = None\n",
    "# df['Level 2 OLA'] = None\n",
    "# df['Level 3 OLA'] = None\n",
    "# df['Level 4 OLA'] = None\n",
    "\n",
    "# # Function to fill in the OLA levels based on non-empty columns in order\n",
    "# def fill_ola_levels(row):\n",
    "#     values = []\n",
    "#     for col in columns_to_check:\n",
    "#         if pd.notna(row[col]):\n",
    "#             values.append(row[col])\n",
    "#         if len(values) == 4:\n",
    "#             break\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if i < len(values):\n",
    "#             row[f'Level {i+1} OLA'] = values[i]\n",
    "#         else:\n",
    "#             row[f'Level {i+1} OLA'] = None\n",
    "#     return row\n",
    "\n",
    "# # Apply the function to each row\n",
    "# df = df.apply(fill_ola_levels, axis=1)\n",
    "\n",
    "# # df = df.drop(columns=columns_to_check)\n",
    "\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# df[columns_to_replace] = df[columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'Level 1 OLA',\n",
    "#     'Level 2 OLA',\n",
    "#     'Level 3 OLA',\n",
    "#     'Level 4 OLA',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "    \n",
    "# ]\n",
    "\t\t\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero).astype('Int64')\n",
    "\n",
    "\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# df['Detail BRICare'] = df['Detail BRICare'].fillna('-')\n",
    "# # Save the modified dataframe to a new Excel file\n",
    "# output_path = 'Cleaned3_Call_Type.xlsx'\n",
    "# output_path2 = 'Cleaned3_Call_Type.csv'\n",
    "# df.to_excel(output_path, index=False)\n",
    "# df.to_csv(output_path2, index=False)\n",
    "\n",
    "# # Output path of the updated file\n",
    "# print(output_path)\n",
    "# print(output_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_52768\\1505204889.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[columns_to_format] = df[columns_to_format].applymap(format_columns)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan.xlsx\"\n",
    "# # path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# pd.set_option('display.max_columns', None)  \n",
    "# # pd.set_option('display.max_rows', None) \n",
    "# df = pd.read_excel(path)\n",
    "# df = df.drop(columns=['Salesforce', 'Status.1', 'Revisi Detail', 'Notes', 'User Grouping_Old','T4.3 - Technical Team','T5.1 - Third Party','T5.2 - Third Party','T5.3 - Third Party','Jenis Produk dan atau Layanan LKPBU','Kategori Permasalahan LKPBU','Jenis Transaksi LKPBU','Penyebab Pengaduan LKPBU','Jenis Produk LBUT','Sandi Jenis  Produk LBUT','Sebab Pengaduan LBUT','Sandi Penyebab LBUT'])\n",
    "\n",
    "# df = df.replace(0, np.nan)\n",
    "# df = df.fillna('')\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'No',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to format columns\n",
    "# def format_columns(col):\n",
    "#     try:\n",
    "#         # Convert to float to handle empty strings and non-numeric values\n",
    "#         return str(float(col)).rstrip('.0')\n",
    "#     except ValueError:\n",
    "#         return col  # Return original value if conversion fails\n",
    "\n",
    "# # Apply the formatting function to selected columns\n",
    "# df[columns_to_format] = df[columns_to_format].applymap(format_columns)\n",
    "# df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # Define the columns to check\n",
    "# columns_to_check = [\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner', 'T3.2 - Product Owner', 'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team', 'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# # Add new columns for Level 1-4 OLA\n",
    "# df['Level 1 OLA'] = None\n",
    "# df['Level 2 OLA'] = None\n",
    "# df['Level 3 OLA'] = None\n",
    "# df['Level 4 OLA'] = None\n",
    "\n",
    "\n",
    "# # Function to fill in the OLA levels based on non-empty columns in order\n",
    "# def fill_ola_levels(row):\n",
    "#     values = []\n",
    "#     for col in columns_to_check:\n",
    "#         if pd.notna(row[col]):\n",
    "#             values.append(row[col])\n",
    "#         if len(values) == 4:\n",
    "#             break\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if i < len(values):\n",
    "#             row[f'Level {i+1} OLA'] = values[i]\n",
    "#         else:\n",
    "#             row[f'Level {i+1} OLA'] = None\n",
    "#     return row\n",
    "\n",
    "# # Apply the function to each row\n",
    "# df = df.apply(fill_ola_levels, axis=1)\n",
    "\n",
    "# # Apply the function to each row\n",
    "\n",
    "# df = df.drop(columns=columns_to_check)\n",
    "# # ====================================================================================================================Convert relevant columns to numeric for addition\n",
    "# # df['T2.2 - Supporting Unit'] = pd.to_numeric(df['T2.2 - Supporting Unit'], errors='coerce').fillna(0)\n",
    "# # df['T2.3 - Supporting Unit'] = pd.to_numeric(df['T2.3 - Supporting Unit'], errors='coerce').fillna(0)\n",
    "# # df['T3.1 - Product Owner'] = pd.to_numeric(df['T3.1 - Product Owner'], errors='coerce').fillna(0)\n",
    "# # df['T3.2 - Product Owner'] = pd.to_numeric(df['T3.2 - Product Owner'], errors='coerce').fillna(0)\n",
    "# # df['T3.3 - Product Owner'] = pd.to_numeric(df['T3.3 - Product Owner'], errors='coerce').fillna(0)\n",
    "# # df['T4.1 - Technical Team'] = pd.to_numeric(df['T4.1 - Technical Team'], errors='coerce').fillna(0)\n",
    "# # df['T4.2 - Technical Team'] = pd.to_numeric(df['T4.2 - Technical Team'], errors='coerce').fillna(0)\n",
    "\n",
    "# # # Combine columns with addition\n",
    "# # df['T2.2 - Supporting Unit'] = df['T2.2 - Supporting Unit'] + df['T2.3 - Supporting Unit']\n",
    "# # df = df.drop(columns=['T2.3 - Supporting Unit'])\n",
    "\n",
    "# # df['T3.1 - Product Owner'] = df['T3.1 - Product Owner'] + df['T3.2 - Product Owner'] + df['T3.3 - Product Owner']\n",
    "# # df = df.drop(columns=['T3.2 - Product Owner', 'T3.3 - Product Owner'])\n",
    "\n",
    "# # df['T4.1 - Technical Team'] = df['T4.1 - Technical Team'] + df['T4.2 - Technical Team']\n",
    "# # df = df.drop(columns=['T4.2 - Technical Team'])\n",
    "\n",
    "# # # Format the combined columns to remove .0\n",
    "# # df['T2.2 - Supporting Unit'] = df['T2.2 - Supporting Unit'].apply(format_columns)\n",
    "# # df['T3.1 - Product Owner'] = df['T3.1 - Product Owner'].apply(format_columns)\n",
    "# # df['T4.1 - Technical Team'] = df['T4.1 - Technical Team'].apply(format_columns)\n",
    "# # =============================================================================================================================================================\n",
    "# # Columns to replace '-' with empty string\n",
    "\n",
    "\n",
    "\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# df[columns_to_replace] = df[columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero)\n",
    "\n",
    "# df.to_csv('Cleaned3_Call_Type.csv', index=False)\n",
    "# # df['external_id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_62668\\3534049870.py:48: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[existing_columns_to_format] = df[existing_columns_to_format].applymap(format_columns)\n"
     ]
    }
   ],
   "source": [
    "# # Cleaning missing call type\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # Define the path to your Excel file\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# # Display all columns in the DataFrame\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Load the Excel file\n",
    "# df = pd.read_excel(path, sheet_name='missing_calltype')\n",
    "\n",
    "\n",
    "\n",
    "# # Replace 0 with NaN and then fill NaN with empty string\n",
    "# df = df.replace(0, np.nan).fillna('')\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'No',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Ensure columns_to_format exist in df\n",
    "# existing_columns_to_format = [col for col in columns_to_format if col in df.columns]\n",
    "\n",
    "# # Function to format columns\n",
    "# def format_columns(col):\n",
    "#     try:\n",
    "#         return str(float(col)).rstrip('.0')\n",
    "#     except ValueError:\n",
    "#         return col\n",
    "\n",
    "# # Apply the formatting function to selected columns\n",
    "# df[existing_columns_to_format] = df[existing_columns_to_format].applymap(format_columns)\n",
    "\n",
    "# # Ensure the 'active' column exists before modifying\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # Convert relevant columns to numeric for addition if they exist\n",
    "# numeric_columns = [\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# existing_numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "\n",
    "# for col in existing_numeric_columns:\n",
    "#     df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# # Combine columns with addition\n",
    "# if 'T2.2 - Supporting Unit' in df.columns and 'T2.3 - Supporting Unit' in df.columns:\n",
    "#     df['T2.2 - Supporting Unit'] += df['T2.3 - Supporting Unit']\n",
    "#     df = df.drop(columns=['T2.3 - Supporting Unit'])\n",
    "\n",
    "# if 'T3.1 - Product Owner' in df.columns and 'T3.2 - Product Owner' in df.columns and 'T3.3 - Product Owner' in df.columns:\n",
    "#     df['T3.1 - Product Owner'] += df['T3.2 - Product Owner'] + df['T3.3 - Product Owner']\n",
    "#     df = df.drop(columns=['T3.2 - Product Owner', 'T3.3 - Product Owner'])\n",
    "\n",
    "# if 'T4.1 - Technical Team' in df.columns and 'T4.2 - Technical Team' in df.columns:\n",
    "#     df['T4.1 - Technical Team'] += df['T4.2 - Technical Team']\n",
    "#     df = df.drop(columns=['T4.2 - Technical Team'])\n",
    "\n",
    "# # Format the combined columns to remove .0\n",
    "# combined_columns = ['T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team']\n",
    "# for col in combined_columns:\n",
    "#     if col in df.columns:\n",
    "#         df[col] = df[col].apply(format_columns)\n",
    "\n",
    "# # Columns to replace '-' with empty string\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# # Ensure columns_to_replace exist in df\n",
    "# existing_columns_to_replace = [col for col in columns_to_replace if col in df.columns]\n",
    "\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# df[existing_columns_to_replace] = df[existing_columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# # Add the 'external_id' column\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero)\n",
    "    \n",
    "# # Save the cleaned data to a CSV file\n",
    "# df.to_csv('Cleaned2_Call_Type_missing.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates in File 1.\n",
      "Duplicates in File 2:\n",
      "443    1201\n",
      "Name: Case Types, dtype: object\n",
      "File 1 has all unique values: True\n",
      "File 2 has all unique values: False\n",
      "No overlapping values in 'Case Types' between the two files.\n"
     ]
    }
   ],
   "source": [
    "# check if the missing call type and Cleaned file not overlapping case types\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your two CSV files\n",
    "file1_path = r\"D:\\dataquality2\\Cleaned3_Call_Type_missing.csv\"\n",
    "file2_path = r\"D:\\dataquality2\\Cleaned3_Call_Type.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Extract the 'Case Types' column from each DataFrame\n",
    "case_types_1 = df1['Case Types'].astype(str)\n",
    "case_types_2 = df2['Case Types'].astype(str)\n",
    "\n",
    "# Check for duplicates within each file\n",
    "duplicates_1 = case_types_1[case_types_1.duplicated()]\n",
    "duplicates_2 = case_types_2[case_types_2.duplicated()]\n",
    "\n",
    "# Print duplicates within each file\n",
    "if not duplicates_1.empty:\n",
    "    print(\"Duplicates in File 1:\")\n",
    "    print(duplicates_1)\n",
    "else:\n",
    "    print(\"No duplicates in File 1.\")\n",
    "\n",
    "if not duplicates_2.empty:\n",
    "    print(\"Duplicates in File 2:\")\n",
    "    print(duplicates_2)\n",
    "else:\n",
    "    print(\"No duplicates in File 2.\")\n",
    "\n",
    "# Check if each file has unique values in the 'Case Types' column\n",
    "unique_case_types_1 = case_types_1.nunique() == len(case_types_1)\n",
    "unique_case_types_2 = case_types_2.nunique() == len(case_types_2)\n",
    "\n",
    "print(f\"File 1 has all unique values: {unique_case_types_1}\")\n",
    "print(f\"File 2 has all unique values: {unique_case_types_2}\")\n",
    "\n",
    "# Convert to sets for comparison\n",
    "set_case_types_1 = set(case_types_1)\n",
    "set_case_types_2 = set(case_types_2)\n",
    "\n",
    "# Find overlapping values\n",
    "overlapping_values = set_case_types_1.intersection(set_case_types_2)\n",
    "\n",
    "if overlapping_values:\n",
    "    print(\"There are overlapping values in 'Case Types' between the two files:\")\n",
    "    print(overlapping_values)\n",
    "else:\n",
    "    print(\"No overlapping values in 'Case Types' between the two files.\")\n",
    "\n",
    "# Optionally, save the results to CSV files if needed\n",
    "# pd.Series(list(overlapping_values)).to_csv('Overlapping_Values.csv', index=False)\n",
    "\n",
    "if not duplicates_1.empty:\n",
    "    duplicates_1.to_csv('Duplicates_in_File1.csv', index=False)\n",
    "if not duplicates_2.empty:\n",
    "    duplicates_2.to_csv('Duplicates_in_File2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_1100\\104105547.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([df1, df2], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Combine Cleaned Call Type with Missing Call Type:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV files\n",
    "file_1_path = r\"D:\\dataquality2\\Cleaned3_Call_Type_missing.csv\"\n",
    "file_2_path = r\"D:\\dataquality2\\Cleaned3_Call_Type.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_1_path)\n",
    "df2 = pd.read_csv(file_2_path)\n",
    "\n",
    "# Identify columns in File 2 that are not in File 1\n",
    "missing_in_file_1 = set(df2.columns) - set(df1.columns)\n",
    "\n",
    "# Ensure all columns from the larger dataframe are present in the smaller dataframe before combining\n",
    "# Add missing columns to df1 with NaN values\n",
    "for col in missing_in_file_1:\n",
    "    df1[col] = pd.NA\n",
    "\n",
    "# Reorder columns in df1 to match the order of columns in df2\n",
    "df1 = df1[df2.columns]\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     combined_df[column] = combined_df[column].apply(remove_decimal_zero)\n",
    "\n",
    "\n",
    "# Columns to format\n",
    "columns_to_format = [\n",
    "    'Level 1 OLA',\n",
    "    'Level 2 OLA',\n",
    "    'Level 3 OLA',\n",
    "    'Level 4 OLA',\n",
    "    'T1 - Agent CCT/Frontliner',\n",
    "    'Total SLA',\n",
    "    'New SLA'\n",
    "]\n",
    "        \n",
    "# Function to remove .0 from float values and handle empty strings\n",
    "def remove_decimal_zero(value):\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return int(value)\n",
    "    if value == '':\n",
    "        return np.nan\n",
    "    return value\n",
    "\n",
    "# Apply function to specified columns\n",
    "for column in columns_to_format:\n",
    "    combined_df[column] = combined_df[column].apply(remove_decimal_zero).astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_file_path = 'Combined3_Cleaned_Call_Type.csv'\n",
    "combined_file_path2 = 'Combined3_Cleaned_Call_Type.xlsx'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "combined_df.to_excel(combined_file_path2, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Call Type IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Call Type IDs]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Compare the call type Pak SUyanto/ bricare with master call type data\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# calltype_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\calltype_psuyanto.csv\")\n",
    "# # final_df = pd.read_excel(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan.xlsx\", sheet_name=None)\n",
    "# final_call_type_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\calltype_psuyanto2.csv\")\n",
    "\n",
    "# # final_call_type_df = final_df['Full Call Type Fix']\n",
    "\n",
    "\n",
    "# calltype_ids = calltype_df['calltypeidtxt'].astype(str)\n",
    "# case_types = final_call_type_df['Case Types'].astype(str)\n",
    "\n",
    "\n",
    "# missing_calltype_ids = calltype_ids[~calltype_ids.isin(case_types)]\n",
    "\n",
    "# missing_calltype_ids_df = missing_calltype_ids.to_frame(name='Missing Call Type IDs')\n",
    "\n",
    "\n",
    "# # missing_calltype_ids_df.to_csv(r\"C:\\Users\\maste\\Downloads\\missing_calltype_ids.csv\", index=False)\n",
    "\n",
    "# # print(\"Missing call type IDs have been saved to missing_calltype_ids.csv\")\n",
    "# missing_calltype_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add Escalation Team\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the provided CSV files\n",
    "# combined_cleaned_call_type = pd.read_csv(r\"D:\\dataquality2\\Combined_Cleaned_Call_Type.csv\")\n",
    "# sla_ola_new_user_grouping = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan - Appendix Team Name.csv\")\n",
    "\n",
    "# # Create a mapping from the SLA & OLA dataframe\n",
    "# team_mapping = sla_ola_new_user_grouping.set_index('/xc//.vs')['Full Name / Queue Name'].to_dict()\n",
    "\n",
    "# # Adding more mappings based on the provided information\n",
    "# additional_mappings = {\n",
    "#     'RO BRILink': 'RO - Business Channel BRILink',\n",
    "#     'RO ITE': 'RO - Information, Technology & E-Channel',\n",
    "#     'BRANCH(Uker)': 'Branch - Operational Unit',\n",
    "#     'CDD(CLF-1DATAM)': 'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     'CDD(CLF-1 DATAM)': 'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     'CCTFrontliner': 'CCT - Contact Center Frontliner'\n",
    "# }\n",
    "\n",
    "# # Update the original mapping with additional mappings\n",
    "# team_mapping.update(additional_mappings)\n",
    "\n",
    "# # Function to clean team names\n",
    "# def clean_team_name_further(team_name):\n",
    "#     if pd.isna(team_name):\n",
    "#         return team_name\n",
    "#     # Remove specific patterns and clean up the string\n",
    "#     team_name = team_name.replace(\"Agent\", \"\").replace(\"Include\", \"\").strip()\n",
    "#     team_name = team_name.lstrip(\"-\").strip()\n",
    "#     # Remove spaces within names for better mapping\n",
    "#     team_name = team_name.replace(\" \", \"\")\n",
    "#     team_name = ' '.join(word.strip('-').strip() for word in team_name.split())\n",
    "#     return team_name\n",
    "\n",
    "# # Applying the cleaning function to the correct column 'Team Name'\n",
    "# combined_cleaned_call_type['Cleaned Team Name'] = combined_cleaned_call_type['Team Name'].apply(clean_team_name_further)\n",
    "\n",
    "# # Function to clean and translate team names with specific rules\n",
    "# def clean_and_translate_team_name_with_specific_rules(team_name, mapping):\n",
    "#     if pd.isna(team_name) or team_name == \"\" or isinstance(team_name, float):\n",
    "#         return [None, None, None, None]\n",
    "#     team_name_cleaned = ' '.join(word.strip('-').strip() for word in str(team_name).split())\n",
    "#     # Specific rule: if the team name contains \"T1-CCT/Frontliner\", return None for all levels\n",
    "#     if \"T1-CCT/Frontliner\" in team_name:\n",
    "#         return [None, None, None, None]\n",
    "#     # Remove \"Insurance Company\" and \"Vendor\"\n",
    "#     team_name_cleaned = team_name_cleaned.replace(\"InsuranceCompany\", \"\").replace(\"Vendor\", \"\").strip()\n",
    "#     # Translate using the mapping\n",
    "#     teams = [mapping.get(t.strip(), t.strip()) for t in team_name_cleaned.split(',')]\n",
    "#     return teams + [None] * (4 - len(teams))\n",
    "\n",
    "# # Reapplying the cleaning and translation process with the specific rules\n",
    "# levels = combined_cleaned_call_type['Cleaned Team Name'].apply(lambda x: clean_and_translate_team_name_with_specific_rules(x, team_mapping))\n",
    "\n",
    "# # Ensuring the result is a DataFrame with correct None values\n",
    "# level_columns = pd.DataFrame(levels.tolist(), columns=['Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team'], index=combined_cleaned_call_type.index)\n",
    "\n",
    "# # Creating placeholders for OLA levels\n",
    "# ola_columns = pd.DataFrame(levels.tolist(), columns=['Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA'], index=combined_cleaned_call_type.index)\n",
    "\n",
    "# # Assigning the translated names to the escalation levels and placeholders to OLA levels\n",
    "# combined_cleaned_call_type = pd.concat([combined_cleaned_call_type, level_columns, ola_columns], axis=1)\n",
    "\n",
    "# # Manually correcting specific known issues\n",
    "# index_to_correct = combined_cleaned_call_type['Cleaned Team Name'] == 'T1-CCT/Frontliner'\n",
    "# combined_cleaned_call_type.loc[index_to_correct, ['Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team', 'Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA']] = None\n",
    "\n",
    "# index_to_correct = combined_cleaned_call_type['Cleaned Team Name'].str.contains('CDD\\(ACUOTO\\),CDD\\(ACUCHARGEBACK\\),CDD\\(CLF-1DATAM\\)', na=False)\n",
    "# combined_cleaned_call_type.loc[index_to_correct, ['Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team', 'Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA']] = [\n",
    "#     'CDD - Auth,Chrgbck & UserCo - OTO',\n",
    "#     'CDD - Auth,Chrgbck & UserCo - CHARGEBACK',\n",
    "#     'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     None,\n",
    "#     'CDD - Auth,Chrgbck & UserCo - OTO',\n",
    "#     'CDD - Auth,Chrgbck & UserCo - CHARGEBACK',\n",
    "#     'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     None\n",
    "# ]\n",
    "\n",
    "# # Keeping all specified columns\n",
    "# columns_to_keep = [\n",
    "#     'No', 'Case Types', 'active', 'User Grouping_New', 'Segment', 'Product', 'Sub Product', 'Case Category', 'Case Description',\n",
    "#     'New Case Description', 'Required Sub-Description', 'Sub', 'Doc Intake', 'List Kebutuhan Dokumen', 'Suggestion', 'Suggestion - Breakdown',\n",
    "#     'Level of Effort', 'Range of Days \\n(Actual SLA)', 'Actual SLA\\n(days)', 'Target SLA \\n(days)', 'BRI Notes', 'Team Name', 'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team', 'Total SLA', 'New SLA', 'Status', 'GIC', 'Maker',\n",
    "#     'Checker', 'Signer', 'Courtesy Awal', 'Kebutuhan Data', 'Detail BRICare', 'Unnamed: 58', 'external_id', 'Cleaned Team Name',\n",
    "#     'Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team', 'Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA'\n",
    "# ]\n",
    "\n",
    "# # Selecting the specified columns\n",
    "# final_dataset_with_all_columns = combined_cleaned_call_type[columns_to_keep]\n",
    "\n",
    "# # Saving the final dataset to a new CSV file\n",
    "# final_dataset_with_all_columns.to_csv('Final_Cleaned_Escalation_Teams_All_Columns.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\Final_Revised_Combined_Cleaned_Call_Type_with_Specific_Rules_Corrected.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "columns_to_keep = [\n",
    "    'No', 'Case Types', 'active', 'User Grouping_New', 'Segment', 'Product', 'Sub Product', 'Case Category', 'Case Description',\n",
    "    'New Case Description', 'Required Sub-Description', 'Sub', 'Doc Intake', 'List Kebutuhan Dokumen', 'Suggestion', 'Suggestion - Breakdown',\n",
    "    'Level of Effort', 'Range of Days \\n(Actual SLA)', 'Actual SLA\\n(days)', 'Target SLA \\n(days)', 'BRI Notes', 'Team Name', 'T1 - Agent CCT/Frontliner',\n",
    "    'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team', 'Total SLA', 'New SLA', 'Status', 'GIC', 'Maker',\n",
    "    'Checker', 'Signer', 'Courtesy Awal', 'Kebutuhan Data', 'Detail BRICare', 'Unnamed: 58', 'external_id', 'Cleaned Team Name',\n",
    "    'Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team'\n",
    "]\n",
    "\n",
    "# Selecting the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "columns_to_process = [\n",
    "    'Level 1 Escalation Team',\n",
    "    'Level 2 Escalation Team',\n",
    "    'Level 3 Escalation Team',\n",
    "    'Level 4 Escalation Team'\n",
    "]\n",
    "\n",
    "# Replace \"Vendor\" and \"Insurance Company\" with an empty string in the specified columns\n",
    "for column in columns_to_process:\n",
    "    df[column] = df[column].replace(['Vendor', 'Insurance Company'], '')\n",
    "    \n",
    "df['Detail BRICare'] = df['Detail BRICare'].fillna('-')\n",
    "\n",
    "df.to_csv('final_cleaned_callType.csv', index=False)\n",
    "# Save to Excel\n",
    "df.to_excel('final_cleaned_callType.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call type mapping for columns 'Produk', 'Jenis Produk', 'Jenis Laporan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "user_dataset_path = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "user_df = pd.read_csv(user_dataset_path)\n",
    "master_df_path = r\"D:\\dataquality\\master_calltype.csv\"\n",
    "master_df = pd.read_csv(master_df_path)\n",
    "\n",
    "\n",
    "master_df = master_df.rename(columns={\n",
    "    'Case Types': 'Call_Type_ID', \n",
    "    'Product': 'Produk', \n",
    "    'Sub Product': 'Jenis_Produk', \n",
    "    'Case Category': 'Jenis_Laporan'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "user_df['Call_Type_ID'] = user_df['Call_Type_ID'].astype(str)\n",
    "master_df['Call_Type_ID'] = master_df['Call_Type_ID'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(user_df, master_df[['Call_Type_ID', 'Produk', 'Jenis_Produk', 'Jenis_Laporan']], on='Call_Type_ID', how='left')\n",
    "\n",
    "user_df['Produk'] = merged_df['Produk_y']\n",
    "user_df['Jenis_Produk'] = merged_df['Jenis_Produk_y']\n",
    "user_df['Jenis_Laporan'] = merged_df['Jenis_Laporan_y']\n",
    "\n",
    "\n",
    "\n",
    "user_df.to_csv(user_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_text_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:  \n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]  \n",
    "                current_entry.append(parts[3].strip())  \n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def remove_bom_and_strip(df):\n",
    "    return df.applymap(lambda x: x.replace('\\ufeff', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.txt\"\n",
    "processed_data = process_text_data(file_path)\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "# df_final=df_final.iloc[:10]\n",
    "df_final.iloc[:10].to_csv('details_uat_20230101_20230101.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to details_20230101_20230101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:\n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]\n",
    "                current_entry.append(parts[3].strip())\n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.csv\"\n",
    "processed_data = process_csv_data(file_path)\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "\n",
    "df_final = df_final.iloc[:10]\n",
    "output_path = \"details_20230101_20230101.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the file A.1 and file A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#just take 10 lines for an example\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.iloc[:10].to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path_1 = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "file_path_2 = r\"D:\\dataquality2\\details_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "df_tenline_bricare = pd.read_csv(file_path_1)\n",
    "df_detail_bricare_10line = pd.read_csv(file_path_2)\n",
    "\n",
    "df_detail_bricare_10line.columns = ['Ticket_ID', 'Details']\n",
    "\n",
    "merged_df = pd.merge(df_tenline_bricare, df_detail_bricare_10line, on='Ticket_ID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "column_to_move=\"Details\"\n",
    "merged_df = merged_df[[col for col in merged_df if col != column_to_move][:3] + [column_to_move] + [col for col in merged_df if col != column_to_move][3:]] \n",
    "\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type B\n",
    "\n",
    "\n",
    "Data Extraction for File Type B (27 columns)\n",
    "\n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['TanggalClosed', 'tanggalTransaksi','Create_Date']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\" if any\n",
    "\n",
    "- PLEASE ADD CIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_33500\\3452762791.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20200101_20200101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\BRICARE_25042024 masking.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "\n",
    "data['Column1'] = data['Column1'].astype(str)\n",
    "data_cleaned = data[data['Column1'].str.match(r'TTB\\d+')]\n",
    "\n",
    "data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n",
    "data_cleaned = data_cleaned[data_cleaned['Column2'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "data_cleaned['Column4'] = pd.to_datetime(data_cleaned['Column4'], errors='coerce')\n",
    "data_cleaned = data_cleaned.dropna(subset=['Column4'])\n",
    "\n",
    "\n",
    "data_to_drop = ['Column28', 'Column29', 'Column30', 'Column31', 'Column32']\n",
    "data_cleaned = data_cleaned.drop(columns=data_to_drop)\n",
    "\n",
    "\n",
    "if len(data_cleaned.columns) <= len(column_list):\n",
    "    data_cleaned.columns = column_list[:len(data_cleaned.columns)]\n",
    "\n",
    "\n",
    "data_cleaned.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "data_cleaned.fillna('', inplace=True)\n",
    "data_cleaned = data_cleaned.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    data_cleaned[column] = pd.to_datetime(data_cleaned[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    data_cleaned[column] = data_cleaned[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "# Just take 10 lines for an example\n",
    "data_cleaned = data_cleaned.iloc[:10]\n",
    "\n",
    "\n",
    "data_cleaned.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(data_cleaned['Create_Date']))\n",
    "enddate = pd.Timestamp(max(data_cleaned['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "data_cleaned.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "27\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Details', 'Create_Date',\n",
      "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
      "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
      "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
      "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
      "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
      "       'user_login_name', 'Jenis_Produk', 'Last_Modified_By', 'Merchant_ID',\n",
      "       'Modified_Date', 'NOTAS', 'Produk', 'SLA_Status', 'TID',\n",
      "       'tanggalAttachmentDone', 'Tgl_Assigned', 'Tgl_Eskalasi', 'AnalisaSkils',\n",
      "       'Attachment_', 'Bank_BRI', 'Biaya_Admin', 'Suku_Bunga', 'Bunga',\n",
      "       'Butuh_Attachment', 'Cicilan', 'Hasil_Kunjungan', 'Log_Name',\n",
      "       'MMS_Ticket_Id', 'Mass_Ticket_Upload_Flag', 'Nama_Supervisor',\n",
      "       'Nama_TL', 'Nama_Wakabag', 'Nasabah_Prioritas', 'Notify_By',\n",
      "       'Organization', 'Output_Settlement', 'phone_survey', 'Return_Ticket',\n",
      "       'Settlement_By', 'Settlement_ID', 'Settlement', 'Site_User',\n",
      "       'Status_Return', 'Status_Transaksi', 'Submitter_Region',\n",
      "       'Submitter_SiteGroup', 'Submitter_User_group_ID', 'Tanggal_Settlement',\n",
      "       'Tgl_Foward', 'Tgl_In_Progress', 'Tgl_Returned', 'Ticket_Referensi',\n",
      "       'Tiket_Urgency', 'Tipe_Remark', 'UniqueID', 'users', 'Usergroup_ID'],\n",
      "      dtype='object')\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date', 'gateway',\n",
      "       'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal', 'status',\n",
      "       'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur', 'Nomor_Kartu',\n",
      "       'user_group', 'assgined_to', 'attachment_done', 'email', 'full_name',\n",
      "       'no_telepon', 'approver_login', 'approver_name', 'SLAResolution',\n",
      "       'submitter_login_id', 'submitter_user_group', 'user_login_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path1=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "path2=r\"D:\\dataquality\\bricare_20200101_20200101.csv\"\n",
    "\n",
    "df1=pd.read_csv(path1)\n",
    "df2=pd.read_csv(path2)\n",
    "print(len(df1.columns))\n",
    "print(len(df2.columns))\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Compare two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape_match': True,\n",
       " 'columns_match': True,\n",
       " 'column_differences': {},\n",
       " 'value_differences': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the newly uploaded files for detailed comparison\n",
    "processed_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "expected_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "# Read the files\n",
    "processed_df_newest = pd.read_csv(processed_file_path_newest)\n",
    "expected_df_newest = pd.read_csv(expected_file_path_newest)\n",
    "\n",
    "# Check for exact match first\n",
    "exact_match = processed_df_newest.equals(expected_df_newest)\n",
    "\n",
    "# Initialize a dictionary to store detailed comparison results\n",
    "comparison_details = {\n",
    "    'shape_match': processed_df_newest.shape == expected_df_newest.shape,\n",
    "    'columns_match': processed_df_newest.columns.equals(expected_df_newest.columns),\n",
    "    'column_differences': {},\n",
    "    'value_differences': {}\n",
    "}\n",
    "\n",
    "# Compare each aspect if exact match is not true\n",
    "if not exact_match:\n",
    "    # Check shape\n",
    "    if not comparison_details['shape_match']:\n",
    "        comparison_details['shape_details'] = {\n",
    "            'processed_shape': processed_df_newest.shape,\n",
    "            'expected_shape': expected_df_newest.shape\n",
    "        }\n",
    "    \n",
    "    # Check columns\n",
    "    if not comparison_details['columns_match']:\n",
    "        comparison_details['column_details'] = {\n",
    "            'processed_columns': processed_df_newest.columns.tolist(),\n",
    "            'expected_columns': expected_df_newest.columns.tolist()\n",
    "        }\n",
    "\n",
    "    # Check column-by-column values\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            comparison_details['column_differences'][column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "\n",
    "# Summarize value differences\n",
    "if not comparison_details['columns_match']:\n",
    "    value_differences = {}\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            value_differences[column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "    comparison_details['value_differences'] = value_differences\n",
    "\n",
    "comparison_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To collect all Error logs in a path\n",
    "\n",
    "directory_path is the error logs path as well as the location where the combined error log file will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined error logs saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_error_logs(directory_path):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.startswith(\"error\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Normalize the 'TICKET_ID' to ensure duplicates are identified\n",
    "    combined_df['TICKET_ID'] = combined_df['TICKET_ID'].str.upper()\n",
    "\n",
    "    combined_df = combined_df.drop_duplicates(subset='TICKET_ID')\n",
    "\n",
    "    output_path = os.path.join(directory_path, 'error_logs.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Combined error logs saved to: {output_path}\")\n",
    "    return output_path, combined_df\n",
    "\n",
    "directory_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\"\n",
    "output_path, combined_df = combine_error_logs(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to C:\\Users\\maste\\OneDrive\\file\\log\\log\\combined_errors.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "directory = r\"C:\\Users\\maste\\OneDrive\\file\\log\\log\"\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if 'error' in filename:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read {filename}: {e}\")\n",
    "\n",
    "\n",
    "combined_file_path = os.path.join(directory, 'combined_errors.csv')\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "\n",
    "print(f\"Combined file saved to {combined_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To cleanse user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_43760\\4048578058.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import html\n",
    "\n",
    "\n",
    "# Function to decode HTML entities in a DataFrame\n",
    "def decode_html_entities(df):\n",
    "    df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "    return df_decoded\n",
    "\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\USER.txt\" \n",
    "\n",
    "# Read the raw file content\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_data = file.readlines()\n",
    "    \n",
    "# Split headers and data    \n",
    "header = raw_data[0].replace(\"&#124;\", \"|\").strip()\n",
    "data = [line.replace(\"&#124;\", \"|\").strip() for line in raw_data[1:]]\n",
    "\n",
    "# Read the cleaned data into a pandas DataFrame\n",
    "df_cleaned = pd.DataFrame([line.split('|') for line in data], columns=header.split('|'))\n",
    "\n",
    "# Decode HTML entities\n",
    "df_cleaned = decode_html_entities(df_cleaned)\n",
    "df_cleaned\n",
    "\n",
    "# Remove the quotes in Dataframe\n",
    "def remove_quotes(df):\n",
    "    df.columns = df.columns.str.replace('\"', '')\n",
    "    df = df.apply(lambda col: col.str.replace('\"', '', regex=True))\n",
    "    return df\n",
    "\n",
    "df_cleaned = remove_quotes(df_cleaned)\n",
    "df_cleaned\n",
    "df_cleaned.to_csv(\"user_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_15156\\3036651791.py:7: DtypeWarning: Columns (1,8,17,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep=',', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Option 1: Check for quoted fields or use different delimiter\n",
    "try:\n",
    "    df = pd.read_csv(path, sep=',', on_bad_lines='skip') \n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricaredatareal_20200101_20200201.csv\n",
      "Problematic data saved to bricaredatareal_problematic_20200101_20200201.csv\n",
      "Number of rows in dataframe: 463581\n",
      "Number of problematic lines: 881\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"\n",
    "]\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Initialize lists to store data and problematic lines\n",
    "data = []\n",
    "problematic_lines = []\n",
    "\n",
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    try:\n",
    "        # Identifying Ticket_ID\n",
    "        ticket_id_index = next(i for i, part in enumerate(parts) if part.startswith('TTB'))\n",
    "        ticket_id = parts[ticket_id_index]\n",
    "        \n",
    "        # Identifying Call_Type_ID\n",
    "        call_type_id_index = ticket_id_index + 1\n",
    "        call_type_id = parts[call_type_id_index]\n",
    "        \n",
    "        # Identifying Create_Date (the part that looks like a datetime string)\n",
    "        create_date_index = next(i for i, part in enumerate(parts) if '-' in part and ':' in part)\n",
    "        create_date = parts[create_date_index]\n",
    "        \n",
    "        # Call_Type is everything between Call_Type_ID and Create_Date\n",
    "        call_type = ' '.join(parts[call_type_id_index + 1:create_date_index])\n",
    "        \n",
    "        # The rest of the fields in order\n",
    "        rest = parts[create_date_index + 1:]\n",
    "        \n",
    "        # Combine into a single list in the correct order\n",
    "        structured_line = [ticket_id, call_type_id, call_type, create_date] + rest\n",
    "        \n",
    "        # If the length is correct, return it\n",
    "        if len(structured_line) == len(column_list):\n",
    "            return structured_line\n",
    "        # If there are extra fields, handle them (for example, by merging or ignoring)\n",
    "        elif len(structured_line) > len(column_list):\n",
    "            return structured_line[:len(column_list)]\n",
    "        else:\n",
    "            return None\n",
    "    except StopIteration:\n",
    "        # If any part of the parsing fails, consider the line problematic\n",
    "        return None\n",
    "\n",
    "# Read the file line by line and parse it\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        parsed_line = parse_line(lines[i])\n",
    "        if parsed_line:\n",
    "            data.append(parsed_line)\n",
    "            i += 1\n",
    "        else:\n",
    "            # Check if merging the next line solves the issue\n",
    "            if i + 1 < len(lines):\n",
    "                merged_line = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "                parsed_line = parse_line(merged_line)\n",
    "                if parsed_line:\n",
    "                    data.append(parsed_line)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    problematic_lines.append((i, lines[i]))\n",
    "                    i += 1\n",
    "            else:\n",
    "                problematic_lines.append((i, lines[i]))\n",
    "                i += 1\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=column_list)\n",
    "\n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].astype(str)\n",
    "df = df[df['Ticket_ID'].str.match(r'TTB\\d+')]\n",
    "\n",
    "df['Call_Type_ID'] = df['Call_Type_ID'].astype(str)\n",
    "df = df[df['Call_Type_ID'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "df = df.dropna(subset=['Create_Date'])\n",
    "\n",
    "df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricaredatareal_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n",
    "problematic_df = pd.DataFrame(problematic_lines, columns=['Line_Number', 'Data'])\n",
    "\n",
    "\n",
    "problematic_filename = f\"bricaredatareal_problematic_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "problematic_df.to_csv(problematic_filename, index=False)\n",
    "\n",
    "print(f\"Problematic data saved to {problematic_filename}\")\n",
    "\n",
    "\n",
    "print(f\"Number of rows in dataframe: {len(df)}\")\n",
    "print(f\"Number of problematic lines: {len(problematic_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create masked Data for UAT\n",
    "\n",
    "\n",
    "To Mask:\n",
    "\n",
    "Nama_Nasabah\n",
    "No_Rekening\n",
    "full_name\n",
    "no_telepon\n",
    "approver_name = beda dengan Nama_Nasabah\n",
    "user_login_name = beda \n",
    "Log_Name\n",
    "Nama_Supervisor\n",
    "Nama_TL\n",
    "Nama_Wakabag\n",
    "Remark\n",
    "\n",
    "Kolom Detail:\n",
    "\n",
    "Kode Cabang          : 0307 4 digits\n",
    "No Kartu             : 5221843130736932 16 digits\n",
    "No Rekening          : 030701098507501 15 digits\n",
    "Nama                 : SITI SHOLEHA\n",
    "No ID                : 3316022012770004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random \n",
    "\n",
    "fake = Faker('id_ID')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "def mask_data(df):\n",
    "    df['Nama_Nasabah'] = df['Nama_Nasabah'].apply(lambda x: fake.name())\n",
    "    df['No_Rekening'] = df['No_Rekening'].apply(lambda x: fake.bban())\n",
    "    df['full_name'] = df['full_name'].apply(lambda x: fake.name())\n",
    "    df['no_telepon'] = df['no_telepon'].apply(lambda x: fake.phone_number())\n",
    "    df['approver_name'] = df['approver_name'].apply(lambda x: fake.name())\n",
    "    df['user_login_name'] = df['user_login_name'].apply(lambda x: fake.user_name())\n",
    "    df['Log_Name'] = df['Log_Name'].apply(lambda x: fake.user_name())\n",
    "    df['Nama_Supervisor'] = df['Nama_Supervisor'].apply(lambda x: fake.name())\n",
    "    df['Nama_TL'] = df['Nama_TL'].apply(lambda x: fake.name())\n",
    "    df['Nama_Wakabag'] = df['Nama_Wakabag'].apply(lambda x: fake.name())\n",
    "    return df\n",
    "\n",
    "\n",
    "df_masked = mask_data(df)\n",
    "\n",
    "def generate_nik():\n",
    "    return f'{fake.random_number(digits=16)}'\n",
    "\n",
    "def mask_detail(detail):\n",
    "    if isinstance(detail, float) and pd.isna(detail):\n",
    "        return detail\n",
    "    lines = str(detail).split('\\n')\n",
    "    masked_lines = []\n",
    "    for line in lines:\n",
    "        if 'Kode Cabang' in line:\n",
    "            line = f'Kode Cabang          : {fake.random_number(digits=4)}'\n",
    "        elif 'No Kartu' in line:\n",
    "            line = f'No Kartu             : {fake.credit_card_number()}'\n",
    "        elif 'No Rekening' in line:\n",
    "            line = f'No Rekening          : {fake.bban()}'\n",
    "        elif 'Nama' in line:\n",
    "            line = f'Nama                 : {fake.name()}'\n",
    "        elif 'No ID' in line:\n",
    "            line = f'No ID                : {generate_nik()}'\n",
    "        masked_lines.append(line)\n",
    "    return '\\n'.join(masked_lines)\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "\n",
    "\n",
    "# Ensure 'Details' column is treated as string and apply mask_detail function\n",
    "df_masked['Details'] = df_masked['Details'].astype(str).apply(mask_detail)\n",
    "\n",
    "\n",
    "\n",
    "# gateway_options = [\n",
    "#     'Email', 'Phone', 'Instagram', 'Walk-In', 'MMS', \n",
    "#     'Twitter', 'Facebook', 'BRImo', 'BRILink', 'Sabrina', 'Ceria'\n",
    "# ]\n",
    "\n",
    "#UAT\n",
    "gateway_options = [\n",
    "    'Email', 'Phone', 'Web', 'Facebook', 'Twitter'\n",
    "]\n",
    "# Assign random values to the 'gateway' column\n",
    "df_masked['gateway'] = df_masked['gateway'].apply(lambda x: random.choice(gateway_options))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "df_masked\n",
    "output_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101_masking.csv\"\n",
    "df_masked.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"bricare_uat_20230101_20230101_masking.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['status']='New'\n",
    "df.to_csv(r\"bricare_uat_20230101_20230101_masking_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in SIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\SIT\\dummy_data_case1100000.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "# df['User*']='00GMR0000000dCf2AI'\n",
    "df['User*']='00GMR0000000dEH2AY'\n",
    "df['Status']='Escalated'\n",
    "df['Status'] = df['Status'].replace('New', 'Escalated')\n",
    "df['Case Origin']='Email'\n",
    "df['Case Type']='a1BMR00000010h72AA'\n",
    "df['Account']='001MR000004C7l2YAC'\n",
    "\n",
    "df.to_csv('dummy_data_uat60k.csv', index=False)\n",
    "\n",
    "# df['Merchant ID']\n",
    "# df['TID']\n",
    "# df['User*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in UAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "df.to_csv('dummy_data_case_uat60k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\CHM\\CHM\\Part2\\bricare_get_cleansing_77_kolom(1).csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "\n",
    "output_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\bricare_get_cleansing_77_kolom_fixed_return.csv\"\n",
    "df=df.iloc[:1]\n",
    "df['status']='Return'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# output_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature ID        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "ref_fiturid_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\ref_fiturid.csv\"\n",
    "id_calltype_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_calltype_uat.csv\"\n",
    "\n",
    "ref_fiturid = pd.read_csv(ref_fiturid_path)\n",
    "id_calltype = pd.read_csv(id_calltype_path)\n",
    "\n",
    "# Convert calltype and NAME columns to string type\n",
    "ref_fiturid['calltype'] = ref_fiturid['calltype'].astype(str)\n",
    "id_calltype['NAME'] = id_calltype['NAME'].astype(str)\n",
    "\n",
    "# Merge the dataframes to add the Call_Type_ID column based on the calltype column\n",
    "ref_fiturid = ref_fiturid.merge(id_calltype[['NAME', 'ID']], left_on='calltype', right_on='NAME', how='left')\n",
    "\n",
    "# Rename the ID column to Call_Type_ID\n",
    "ref_fiturid.rename(columns={'ID': 'Call_Type_ID'}, inplace=True)\n",
    "\n",
    "# Drop the unnecessary NAME column from the merge\n",
    "ref_fiturid.drop(columns=['NAME'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\"\n",
    "# ref_fiturid=ref_fiturid.iloc[62:63]\n",
    "ref_fiturid.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Updated CSV file saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see the characteristics of all files and save it as excel files. FILE: EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to find the longest length of values in each column\n",
    "def find_longest_lengths_per_column(df):\n",
    "    lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    max_lengths = lengths.max()\n",
    "    return max_lengths\n",
    "\n",
    "# Set the folder path\n",
    "# folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\"\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\knowledge\\new_WI_12aug\"\n",
    "\n",
    "# Get all Excel files in the folder\n",
    "file_names = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "# Load the files dynamically\n",
    "dfs = []\n",
    "rows_count = []\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    dfs.append((file_name, df))\n",
    "    rows_count.append((file_name, len(df)))\n",
    "\n",
    "# Calculate the longest length for each column in each file\n",
    "longest_lengths = {}\n",
    "for file_name, df in dfs:\n",
    "    longest_lengths[file_name] = find_longest_lengths_per_column(df)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "results = pd.DataFrame(longest_lengths).transpose()\n",
    "\n",
    "# Create DataFrame for rows count\n",
    "rows_count_df = pd.DataFrame(rows_count, columns=[\"File Name\", \"Total Rows\"])\n",
    "\n",
    "# Print the results\n",
    "results = results.transpose()\n",
    "results.to_excel('allcolumns_artikel2aug.xlsx')\n",
    "\n",
    "# Save rows count to a new Excel file\n",
    "rows_count_df.to_excel('allcolumns_artikel_rows_count.xlsx', index=False)\n",
    "\n",
    "# Find the longest column and its length for each file\n",
    "longest_columns_info = []\n",
    "for file_name, df in dfs:\n",
    "    lengths = find_longest_lengths_per_column(df)\n",
    "    longest_col = lengths.idxmax()\n",
    "    longest_length = lengths.max()\n",
    "    longest_columns_info.append({\"File\": file_name, \"Column\": longest_col, \"Length\": longest_length})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "longest_columns_df = pd.DataFrame(longest_columns_info)\n",
    "longest_columns_df.to_excel('allcolumns_artikel_col12aug.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see the characteristics of all files and save it as excel files. FILE: CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\2989052216.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\2989052216.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\2989052216.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\2989052216.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\2989052216.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to find the longest length of values in each column\n",
    "def find_longest_lengths_per_column(df):\n",
    "    lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    max_lengths = lengths.max()\n",
    "    return max_lengths\n",
    "\n",
    "# Set the folder path\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\"\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "file_names = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Load the files dynamically\n",
    "dfs = []\n",
    "rows_count = []\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    dfs.append((file_name, df))\n",
    "    rows_count.append((file_name, len(df)))\n",
    "\n",
    "# Calculate the longest length for each column in each file\n",
    "longest_lengths = {}\n",
    "for file_name, df in dfs:\n",
    "    longest_lengths[file_name] = find_longest_lengths_per_column(df)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "results = pd.DataFrame(longest_lengths).transpose()\n",
    "\n",
    "# Create DataFrame for rows count\n",
    "rows_count_df = pd.DataFrame(rows_count, columns=[\"File Name\", \"Total Rows\"])\n",
    "\n",
    "# Print the results\n",
    "results = results.transpose()\n",
    "results.to_excel('allcolumns_artike29aug.xlsx')\n",
    "\n",
    "# Save rows count to a new Excel file\n",
    "rows_count_df.to_excel('allcolumns_artikel_rows_count29aug.xlsx', index=False)\n",
    "\n",
    "# Find the longest column and its length for each file\n",
    "longest_columns_info = []\n",
    "for file_name, df in dfs:\n",
    "    lengths = find_longest_lengths_per_column(df)\n",
    "    longest_col = lengths.idxmax()\n",
    "    longest_length = lengths.max()\n",
    "    longest_columns_info.append({\"File\": file_name, \"Column\": longest_col, \"Length\": longest_length})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "longest_columns_df = pd.DataFrame(longest_columns_info)\n",
    "longest_columns_df.to_excel('allcolumns_artikel_col29aug.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_32596\\2514165187.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to find the longest length of values in each column\n",
    "def find_longest_lengths_per_column(df):\n",
    "    lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    max_lengths = lengths.max()\n",
    "    return max_lengths\n",
    "\n",
    "# Set the folder path\n",
    "# folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_wise\\sample_wise\"\n",
    "\n",
    "folder_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\"\n",
    "\n",
    "# List of file names\n",
    "file_names = [\n",
    "    \"sample_wi_100.xlsx\",\n",
    "    \"sample_wise_produk_58.xlsx\",\n",
    "    \"sample_wise_program_41.xlsx\",\n",
    "    \"sample_wise_promo_90.xlsx\"\n",
    "]\n",
    "\n",
    "# Load the files dynamically\n",
    "dfs = []\n",
    "rows_count = []\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    dfs.append((file_name, df))\n",
    "    rows_count.append((file_name, len(df)))\n",
    "\n",
    "# Calculate the longest length for each column in each file\n",
    "longest_lengths = {}\n",
    "for file_name, df in dfs:\n",
    "    longest_lengths[file_name] = find_longest_lengths_per_column(df)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "results = pd.DataFrame(longest_lengths).transpose()\n",
    "\n",
    "# Create DataFrame for rows count\n",
    "rows_count_df = pd.DataFrame(rows_count, columns=[\"File Name\", \"Total Rows\"])\n",
    "\n",
    "# Print the results\n",
    "results = results.transpose()\n",
    "results.to_excel('allcolumns_artikel2.xlsx')\n",
    "\n",
    "# Save rows count to a new Excel file\n",
    "rows_count_df.to_excel('allcolumns_artikel_rows_count.xlsx', index=False)\n",
    "\n",
    "# Optional: Find the longest column and its length for each file\n",
    "# longest_columns_info = []\n",
    "# for file_name, lengths in results.iterrows():\n",
    "#     longest_col = lengths.idxmax()\n",
    "#     longest_length = lengths.max()\n",
    "#     longest_columns_info.append({\"File\": file_name, \"Column\": longest_col, \"Length\": longest_length})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "# longest_columns_df = pd.DataFrame(longest_columns_info)\n",
    "# longest_columns_df.to_excel('longest_columns_info.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\681736892.py:29: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[text_columns] = df[text_columns].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the Excel file\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\knowledge\\new_WI_6aug\\knowledge_wi.xlsx\"\n",
    "# df = pd.read_excel(path)\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\knowledge_wi_29_agustus_2024.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Replace \\n and \\t with actual new lines and tabs\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        # Remove trailing and leading spaces\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns = df.select_dtypes(include=[object]).columns\n",
    "df[text_columns] = df[text_columns].applymap(clean_text)\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"WI-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df))]\n",
    "\n",
    "# Check the 'calltype' column and replace \"-\" or empty values with \"No Call Type\"\n",
    "df['calltype'] = df['calltype'].apply(lambda x: 'No Call Type' if pd.isnull(x) or x.strip() == '-' else x)\n",
    "\n",
    "df['Article_Type'] = 'Working Instruction'\n",
    "df['record_type'] = 'Working Instructions'\n",
    "\n",
    "# Function to limit words in the title to 20 words\n",
    "def limit_words(text, word_limit=20):\n",
    "    if pd.isnull(text) or text.strip() == '':\n",
    "        return 'No Title'\n",
    "    words = text.split()\n",
    "    limited_text = ' '.join(words[:word_limit])\n",
    "    return limited_text\n",
    "\n",
    "# Add the new 'title' column, handle missing values in concatenation\n",
    "df['title'] = df['calltype'].fillna('No Call Type') + ' - ' + df['kategori'].fillna('') + ' - ' + df['sub_kategori'].fillna('')\n",
    "df['title'] = df['title'].apply(limit_words)\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df.to_csv('cleaned_wise_wi_29aug.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path= r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\knowledge\\new_WI_6aug\\cleaned_wise_promo_90_6aug.csv\"\n",
    "df =pd.read_csv(path)\n",
    "\n",
    "df=df.iloc[:1]\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\knowledge\\new_WI_6aug\\cleaned_wise_promo_90_6aug_1line.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\2709541421.py:45: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'produk' at row 12 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel29aug\\json_table_12_produk.xlsx'.\n",
      "Column 'bunga' at row 4 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel29aug\\json_table_4_bunga.xlsx'.\n",
      "Column 'limits' at row 4 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel29aug\\json_table_4_limits.xlsx'.\n",
      "Column 'limits' at row 6 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel29aug\\json_table_6_limits.xlsx'.\n",
      "Column 'biaya' at row 4 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel29aug\\json_table_4_biaya.xlsx'.\n",
      "Column 'biaya' at row 6 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel29aug\\json_table_6_biaya.xlsx'.\n",
      "Column 'biaya' at row 44 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel29aug\\json_table_44_biaya.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from prettytable import PrettyTable\n",
    "import openpyxl\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.drawing.image import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the new Excel file\n",
    "# file_path_new = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\knowledge\\new_WI_6aug\\knowledge_produk.xlsx\"\n",
    "# df_program = pd.read_excel(file_path_new)\n",
    "\n",
    "file_path_new = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\knowledge_produk_29_agustus_2024.csv\"\n",
    "df_program = pd.read_csv(file_path_new)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df_program = df_program.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Add the new 'title' column\n",
    "df_program.insert(0, 'title', df_program['jenis_produk'] + ' - ' + df_program['produk'] + ' - ' + df_program['varian_produk'])\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"PD-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df_program['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_program))]\n",
    "df_program['calltype'] = 'No Call Type'\n",
    "df_program['Article_Type'] = 'Product'\n",
    "df_program['record_type'] = 'Product'\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns_program = df_program.select_dtypes(include=[object]).columns\n",
    "df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
    "\n",
    "# Function to save DataFrame to Excel with auto-fit columns and borders\n",
    "def save_df_to_excel(df, file_path):\n",
    "    writer = pd.ExcelWriter(file_path, engine='openpyxl')\n",
    "    df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # Auto-fit columns\n",
    "    for col in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column = col[0].column_letter\n",
    "        for cell in col:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(cell.value)\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = (max_length + 2)\n",
    "        worksheet.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "    # Add borders to cells\n",
    "    thin_border = openpyxl.styles.Border(\n",
    "        left=openpyxl.styles.Side(style='thin'),\n",
    "        right=openpyxl.styles.Side(style='thin'),\n",
    "        top=openpyxl.styles.Side(style='thin'),\n",
    "        bottom=openpyxl.styles.Side(style='thin')\n",
    "    )\n",
    "\n",
    "    for row in worksheet.iter_rows():\n",
    "        for cell in row:\n",
    "            cell.border = thin_border\n",
    "\n",
    "    workbook.save(file_path)\n",
    "\n",
    "# Function to generate a picture of the DataFrame\n",
    "def generate_df_picture(df, file_path):\n",
    "    fig, ax = plt.subplots(figsize=(12, len(df) // 2))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.2)\n",
    "    plt.savefig(file_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Function to check if a string is valid JSON and convert it to readable format\n",
    "def process_json(text, row_idx, col_name, save_path, generate_pictures):\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, list) and 'table_data' in data[0]:\n",
    "            # Save the JSON data to a separate Excel file\n",
    "            table_data = data[0]['table_data']\n",
    "            rows = []\n",
    "\n",
    "            for row in table_data:\n",
    "                row_values = [item['value'] for item in row]\n",
    "                rows.append(row_values)\n",
    "\n",
    "            json_df = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "            json_file_path = os.path.join(save_path, f'json_table_{row_idx}_{col_name}.xlsx')\n",
    "            save_df_to_excel(json_df, json_file_path)\n",
    "\n",
    "            if generate_pictures:\n",
    "                # Generate picture of the table\n",
    "                img_file_path = json_file_path.replace('.xlsx', '.png')\n",
    "                generate_df_picture(json_df, img_file_path)\n",
    "\n",
    "            table = PrettyTable()\n",
    "            table.align = \"l\"  # Align all text to the left\n",
    "\n",
    "            headers = [item['value'] for item in table_data[0]]\n",
    "            table.field_names = headers\n",
    "            \n",
    "            # Calculate the maximum width for each column\n",
    "            max_widths = {header: len(header) for header in headers}\n",
    "            for row in table_data[1:]:\n",
    "                for i, item in enumerate(row):\n",
    "                    value_length = len(str(item['value']).strip())\n",
    "                    if value_length > max_widths[headers[i]]:\n",
    "                        max_widths[headers[i]] = value_length\n",
    "            \n",
    "            for row in table_data[1:]:\n",
    "                row_values = [item['value'].strip() if isinstance(item['value'], str) else item['value'] for item in row]\n",
    "                table.add_row(row_values)\n",
    "            \n",
    "            # Adjust column widths\n",
    "            for field in table.field_names:\n",
    "                table.max_width[field] = max_widths[field]\n",
    "            \n",
    "            return str(table)\n",
    "        return text\n",
    "    except (ValueError, TypeError, KeyError):\n",
    "        return text\n",
    "\n",
    "# Process and clean JSON-like text in all columns, saving JSON data to separate files\n",
    "def process_all_json_texts(df, save_path, generate_pictures):\n",
    "    json_columns_rows = []\n",
    "    text_columns_program = df.select_dtypes(include=[object]).columns\n",
    "    for column in text_columns_program:\n",
    "        for idx, value in enumerate(df[column]):\n",
    "            if isinstance(value, str) and value.strip().startswith('[') and value.strip().endswith(']'):\n",
    "                df.at[idx, column] = process_json(value, idx, column, save_path, generate_pictures)\n",
    "                json_columns_rows.append((column, idx))\n",
    "    return json_columns_rows\n",
    "\n",
    "# Define the path where to save JSON files and whether to generate pictures\n",
    "save_path = r\"D:\\dataquality2\\product_artikel29aug\"\n",
    "generate_pictures = False\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Process all JSON texts in the DataFrame\n",
    "json_columns_rows = process_all_json_texts(df_program, save_path, generate_pictures)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file_path = 'cleaned_product_29aug.csv'\n",
    "df_program.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display lines 39 to 42 for user reference\n",
    "# lines_39_to_42 = df_program.iloc[38:42]\n",
    "# print(lines_39_to_42['Biaya'].values)\n",
    "\n",
    "# Add comments indicating columns and rows with JSON texts\n",
    "for column, row in json_columns_rows:\n",
    "    print(f\"Column '{column}' at row {row} contains JSON text. Saved to '{os.path.join(save_path, f'json_table_{row}_{column}.xlsx')}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to handle empty values in column id_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\693236902.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['id_produk'].fillna(df['URL_Name'], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\maste\\\\Downloads\\\\dataloader_v60.0.2\\\\server\\\\artikel\\\\knowledge\\\\new_29aug\\\\cleaned_product_29aug.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\cleaned_product_29aug.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Fill empty values in 'id_produk' with values from 'URL_Name'\n",
    "df['id_produk'].fillna(df['URL_Name'], inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\cleaned_product_29aug.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # Load the new Excel file\n",
    "# file_path_new = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\\sample_wise_produk_75.xlsx\"\n",
    "# df_new = pd.read_excel(file_path_new)\n",
    "\n",
    "# # Drop the 'Unnamed: 0' column if it exists\n",
    "# df_new = df_new.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# # Function to clean text by removing HTML tags and unwanted characters\n",
    "# def clean_text(text):\n",
    "#     if isinstance(text, str):\n",
    "#         # Remove HTML tags\n",
    "#         clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "#         # Replace \\n and \\t with actual new lines and tabs\n",
    "#         clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "#         # Remove trailing and leading spaces\n",
    "#         clean_text = clean_text.strip()\n",
    "#         return clean_text\n",
    "#     return text\n",
    "\n",
    "# # Apply the clean_text function to all text columns\n",
    "# text_columns_new = df_new.select_dtypes(include=[object]).columns\n",
    "# df_new[text_columns_new] = df_new[text_columns_new].applymap(clean_text)\n",
    "\n",
    "# # Convert the date format\n",
    "# # df_new['modified_time'] = pd.to_datetime(df_new['modified_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# # Add the new 'title' column\n",
    "# df_new.insert(0, 'title', df_new['jenis_produk'] + ' - ' + df_new['produk'] + ' - ' + df_new['varian_produk'] + ' - ' + df_new['nama_knowledge'])\n",
    "\n",
    "# # Function to generate sequential URL names\n",
    "# def generate_url_name(index):\n",
    "#     return f\"PD-{index:07d}\"\n",
    "\n",
    "# # Apply the function to generate URL names based on the row index + 1\n",
    "# df_new['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_new))]\n",
    "\n",
    "# # Save the cleaned DataFrame to a CSV file\n",
    "# df_new['calltype'] = 'No Call Type'\n",
    "# df_new['Article_Type']='Product'\n",
    "# df_new['record_type']='Product'\n",
    "# # df_new=df_new.iloc[:1]\n",
    "# df_new.to_csv('cleaned_wise_produk_75.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\1595748788.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the new Excel file\n",
    "file_path_program = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\knowledge_program_29_agustus_2024.csv\"\n",
    "df_program = pd.read_csv(file_path_program)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df_program = df_program.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Replace \\n and \\t with actual new lines and tabs\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        # Remove trailing and leading spaces\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns_program = df_program.select_dtypes(include=[object]).columns\n",
    "df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
    "\n",
    "# Convert the date format for 'modified_time', 'start', and 'end' columns\n",
    "# df_program['modified_time'] = pd.to_datetime(df_program['modified_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_program['start'] = pd.to_datetime(df_program['start'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "df_program['end'] = pd.to_datetime(df_program['end'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Add the new 'title' column\n",
    "# df_program.insert(0, 'title', df_program['nama_program'])\n",
    "\n",
    "df_program.insert(0, 'title', df_program['nama'])\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"PG-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df_program['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_program))]\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df_program['calltype'] = 'No Call Type'\n",
    "df_program['Article_Type']='Program'\n",
    "df_program['record_type']='Program'\n",
    "# df_program=df_program.iloc[:1]\n",
    "df_program.to_csv('cleaned_wise_program_29aug.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6260\\967382301.py:29: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_promo[text_columns_promo] = df_promo[text_columns_promo].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the new Excel file\n",
    "# file_path_promo = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\\sample_wise_promo_107.xlsx\"\n",
    "# df_promo = pd.read_excel(file_path_promo)\n",
    "\n",
    "\n",
    "file_path_promo = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\knowledge_promo_29_agustus_2024.csv\"\n",
    "df_promo = pd.read_csv(file_path_promo)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df_promo = df_promo.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Replace \\n and \\t with actual new lines and tabs\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        # Remove trailing and leading spaces\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns_promo = df_promo.select_dtypes(include=[object]).columns\n",
    "df_promo[text_columns_promo] = df_promo[text_columns_promo].applymap(clean_text)\n",
    "\n",
    "# Convert the date format for 'modified_time', 'start', and 'end' columns\n",
    "# df_promo['modified_time'] = pd.to_datetime(df_promo['modified_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_promo['start'] = pd.to_datetime(df_promo['start'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_promo['end'] = pd.to_datetime(df_promo['end'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add the new 'title' column\n",
    "df_promo.insert(0, 'title', df_promo['kategori'])\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"PM-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df_promo['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_promo))]\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df_promo['calltype'] = 'No Call Type'\n",
    "df_promo['Article_Type']='Promotion'\n",
    "df_promo['record_type']='Promotion'\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "# df_promo=df_promo.iloc[:1]\n",
    "df_promo.to_csv('cleaned_wise_promo_29aug.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to handle duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\maste\\\\Downloads\\\\dataloader_v60.0.2\\\\server\\\\artikel\\\\knowledge\\\\new_29aug\\\\cleaned_wise_promo_29aug_done.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\cleaned_wise_promo_29aug.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a dictionary to track occurrences of 'id_promo'\n",
    "id_counts = {}\n",
    "\n",
    "def modify_duplicates(id_promo):\n",
    "    # Increase count each time an id_promo is seen\n",
    "    if id_promo in id_counts:\n",
    "        id_counts[id_promo] += 1\n",
    "        # Append a number to duplicates without an underscore\n",
    "        return f\"{id_promo}{id_counts[id_promo]}\"\n",
    "    else:\n",
    "        id_counts[id_promo] = 0  # First occurrence is not modified\n",
    "        return id_promo\n",
    "\n",
    "# Apply the function to 'id_promo' column\n",
    "df['id_promo'] = df['id_promo'].apply(modify_duplicates)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\artikel\\knowledge\\new_29aug\\cleaned_wise_promo_29aug_done.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>Call Type Info</th>\n",
       "      <th>Type</th>\n",
       "      <th>User</th>\n",
       "      <th>Title</th>\n",
       "      <th>Detail Bricare</th>\n",
       "      <th>Deskripsi</th>\n",
       "      <th>Empati</th>\n",
       "      <th>Konfirmasi</th>\n",
       "      <th>...</th>\n",
       "      <th>Verifikasi</th>\n",
       "      <th>Gali Informasi</th>\n",
       "      <th>Pembuatan Laporan</th>\n",
       "      <th>Konfirmasi Ulang</th>\n",
       "      <th>Percepatan Komplain</th>\n",
       "      <th>Solusi, Informasi</th>\n",
       "      <th>Edukasi &amp; Cross Selling</th>\n",
       "      <th>Closing</th>\n",
       "      <th>Article Type</th>\n",
       "      <th>URL_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>Nasabah Menanyakan Informasi Pengajuan Terkait...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1000 - Nasabah Menanyakan Informasi Pengajuan ...</td>\n",
       "      <td>nomor ponsel nasabah yang bisa dihubungi\\nID C...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...</td>\n",
       "      <td>Jika Nasabah menanyakan Cara &amp; Syarat Pengajua...</td>\n",
       "      <td>...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nVerifikas...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nGali Info...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nBuat lapo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nInformasi...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1002</td>\n",
       "      <td>Nasabah Menyakan Terkait Promo dan Program CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1002 - Nasabah Menyakan Terkait Promo dan Prog...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\nJika...</td>\n",
       "      <td>Konfirmasi :\\nJika Nasabah menanyakan promo ce...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi\\nTidak dilakukan Verifikasi\\n</td>\n",
       "      <td>Gali Informasi\\t\\n\\t  i.          Jenis promo/...</td>\n",
       "      <td>Pembuatan Laporan\\t\\n\\ti.     Nomor laporan ti...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\t\\n\\t  i.              \\...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1003</td>\n",
       "      <td>Nasabah Mengajukan Pelunasan Awal Cicilan CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1003 - Nasabah Mengajukan Pelunasan Awal Cicil...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\n\"Bap...</td>\n",
       "      <td>Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...</td>\n",
       "      <td>Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi\\n1.\\tCicilan telah berjalan...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1005</td>\n",
       "      <td>Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1005 - Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>Nasabah mengajukan pemblokiran Sementara Akun ...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati\\nJika nasabah infokan ingin melakukan p...</td>\n",
       "      <td>Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...</td>\n",
       "      <td>Pembuatan Laporan\\nNasabah mengajukan pembloki...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\n1.\\tAgent Konfirmasi ke...</td>\n",
       "      <td>Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...</td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1005-Nasabah-Mengajukan-Pemblokiran-CERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1008</td>\n",
       "      <td>Nasabah Mengajukan Pengaktifan Akun Ceria Terb...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1008 - Nasabah Mengajukan Pengaktifan Akun Cer...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 Call Type                                     Call Type Info  \\\n",
       "0            1      1000  Nasabah Menanyakan Informasi Pengajuan Terkait...   \n",
       "1            2      1002   Nasabah Menyakan Terkait Promo dan Program CERIA   \n",
       "2            3      1003    Nasabah Mengajukan Pelunasan Awal Cicilan CERIA   \n",
       "3            4      1005               Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4            5      1008  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...   \n",
       "..         ...       ...                                                ...   \n",
       "437                                                                           \n",
       "438                                                                           \n",
       "439                                                                           \n",
       "440                                                                           \n",
       "441                                                                           \n",
       "\n",
       "      Type                  User  \\\n",
       "0    CERIA  Agent Contact Center   \n",
       "1    CERIA  Agent Contact Center   \n",
       "2    CERIA  Agent Contact Center   \n",
       "3    CERIA  Agent Contact Center   \n",
       "4    CERIA  Agent Contact Center   \n",
       "..     ...                   ...   \n",
       "437                                \n",
       "438                                \n",
       "439                                \n",
       "440                                \n",
       "441                                \n",
       "\n",
       "                                                 Title  \\\n",
       "0    1000 - Nasabah Menanyakan Informasi Pengajuan ...   \n",
       "1    1002 - Nasabah Menyakan Terkait Promo dan Prog...   \n",
       "2    1003 - Nasabah Mengajukan Pelunasan Awal Cicil...   \n",
       "3          1005 - Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4    1008 - Nasabah Mengajukan Pengaktifan Akun Cer...   \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Detail Bricare  \\\n",
       "0    nomor ponsel nasabah yang bisa dihubungi\\nID C...   \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Nasabah mengajukan pemblokiran Sementara Akun ...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                             Deskripsi  \\\n",
       "0    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "1    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "2    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "3    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                                Empati  \\\n",
       "0    Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...   \n",
       "1    Empati sesuai kondisi (pilih salah satu)\\nJika...   \n",
       "2    Empati sesuai kondisi (pilih salah satu)\\n\"Bap...   \n",
       "3    Empati\\nJika nasabah infokan ingin melakukan p...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                            Konfirmasi  ...  \\\n",
       "0    Jika Nasabah menanyakan Cara & Syarat Pengajua...  ...   \n",
       "1    Konfirmasi :\\nJika Nasabah menanyakan promo ce...  ...   \n",
       "2    Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...  ...   \n",
       "3    Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...  ...   \n",
       "4                                                       ...   \n",
       "..                                                 ...  ...   \n",
       "437                                                     ...   \n",
       "438                                                     ...   \n",
       "439                                                     ...   \n",
       "440                                                     ...   \n",
       "441                                                     ...   \n",
       "\n",
       "                                            Verifikasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nVerifikas...   \n",
       "1             Verifikasi\\nTidak dilakukan Verifikasi\\n   \n",
       "2    Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...   \n",
       "3    Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Gali Informasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nGali Info...   \n",
       "1    Gali Informasi\\t\\n\\t  i.          Jenis promo/...   \n",
       "2    Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...   \n",
       "3    Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                     Pembuatan Laporan Konfirmasi Ulang  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nBuat lapo...                    \n",
       "1    Pembuatan Laporan\\t\\n\\ti.     Nomor laporan ti...                    \n",
       "2    Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...                    \n",
       "3    Pembuatan Laporan\\nNasabah mengajukan pembloki...                    \n",
       "4                                                                         \n",
       "..                                                 ...              ...   \n",
       "437                                                                       \n",
       "438                                                                       \n",
       "439                                                                       \n",
       "440                                                                       \n",
       "441                                                                       \n",
       "\n",
       "    Percepatan Komplain                                  Solusi, Informasi  \\\n",
       "0                        1\\tCara & Syarat pengajuan Aplikasi\\nInformasi...   \n",
       "1                        Informasi & Solusi \\t\\t\\n\\t  i.              \\...   \n",
       "2                        Informasi & Solusi\\n1.\\tCicilan telah berjalan...   \n",
       "3                        Informasi & Solusi \\t\\n1.\\tAgent Konfirmasi ke...   \n",
       "4                                                                            \n",
       "..                  ...                                                ...   \n",
       "437                                                                          \n",
       "438                                                                          \n",
       "439                                                                          \n",
       "440                                                                          \n",
       "441                                                                          \n",
       "\n",
       "                               Edukasi & Cross Selling  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                               Closing         Article Type  \\\n",
       "0    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "1    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "2    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "3    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "4                                                       Working Instruction   \n",
       "..                                                 ...                  ...   \n",
       "437                                                     Working Instruction   \n",
       "438                                                     Working Instruction   \n",
       "439                                                     Working Instruction   \n",
       "440                                                     Working Instruction   \n",
       "441                                                     Working Instruction   \n",
       "\n",
       "                                              URL_name  \n",
       "0    1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...  \n",
       "1    1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...  \n",
       "2    1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...  \n",
       "3            1005-Nasabah-Mengajukan-Pemblokiran-CERIA  \n",
       "4    1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...  \n",
       "..                                                 ...  \n",
       "437                                                     \n",
       "438                                                     \n",
       "439                                                     \n",
       "440                                                     \n",
       "441                                                     \n",
       "\n",
       "[375 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare(WI).csv\"\n",
    "df=pd.read_csv(path, encoding='ISO-8859-1')\n",
    "df = df.fillna('')\n",
    "df['Article Type']='Working Instruction'\n",
    "\n",
    "output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\artikel_all.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Change 'Brilink' to 'BRILink' in 'Type' column\n",
    "df['Type'] = df['Type'].replace('Brilink', 'BRILink')\n",
    "\n",
    "# Remove rows with 'Wholesale' in 'Type' column\n",
    "df = df[df['Type'] != 'Wholesale']\n",
    "\n",
    "# df['Type'].unique()\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('artikel2.csv', index= False)\n",
    "# df.iloc[:1].to_csv(output, index= False)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df= pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424041601915.csv\")\n",
    "df.to_csv('error.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare 2.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path)\n",
    "df=df.iloc[:1]\n",
    "# df['Gali Informasi']\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "df.to_csv('artikel_bullet.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate 1k roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/extract_role.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the uploaded CSV file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/extract_role.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m roles_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the content of the CSV file to understand its structure\u001b[39;00m\n\u001b[0;32m      8\u001b[0m roles_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/extract_role.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '/mnt/data/extract_role.csv'\n",
    "roles_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the content of the CSV file to understand its structure\n",
    "roles_df.head()\n",
    "\n",
    "\n",
    "# Number of additional roles needed to reach a total of 1000\n",
    "num_additional_roles_needed = 1000 - len(expanded_roles_df)\n",
    "\n",
    "# Duplicate the existing roles to meet the required number of roles\n",
    "additional_roles_more = expanded_roles_df.sample(num_additional_roles_needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "# Generate unique names for the additional roles\n",
    "additional_roles_more['NAME'] = additional_roles_more['NAME'] + \"_more_\" + (additional_roles_more.index + 1).astype(str)\n",
    "\n",
    "# Combine the original roles with the additional roles\n",
    "expanded_roles_df_more = pd.concat([expanded_roles_df, additional_roles_more], ignore_index=True)\n",
    "\n",
    "# Ensure we have a total of 1000 roles\n",
    "expanded_roles_df_more = expanded_roles_df_more.head(1000)\n",
    "\n",
    "# Save the expanded roles to a new CSV file\n",
    "output_file_path_more = 'expanded_roles_1000.csv'\n",
    "expanded_roles_df_more.to_csv(output_file_path_more, index=False)\n",
    "\n",
    "output_file_path_more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Extraction Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check data before 2023 = 27 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns in DataFrame: {'Tgl_Assigned'}\n",
      "Extra columns in DataFrame: {'TglAssigned', 'cifno', 'Details'}\n"
     ]
    }
   ],
   "source": [
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_27_kolom.csv\"\n",
    "path2=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_79_kolom.csv\"\n",
    "# df= pd.read_csv(path, delimiter=';')\n",
    "df= pd.read_csv(path2, delimiter=';')\n",
    "\n",
    "\n",
    "# column_list_set = set(column_list)\n",
    "column_list_set = set(column_names)\n",
    "df_columns_set = set(df.columns)\n",
    "\n",
    "missing_columns = column_list_set - df_columns_set\n",
    "extra_columns = df_columns_set - column_list_set\n",
    "\n",
    "if not missing_columns and not extra_columns:\n",
    "    print(\"The column names match.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "    if extra_columns:\n",
    "        print(f\"Extra columns in DataFrame: {extra_columns}\")\n",
    "\n",
    "################################################\n",
    "# column_list_set2 = set(column_names)\n",
    "# df_columns_set2 = set(df2.columns)\n",
    "\n",
    "# missing_columns2 = column_list_set2 - df_columns_set2\n",
    "# extra_columns2 = df_columns_set2 - column_list_set2\n",
    "\n",
    "# if not missing_columns and not extra_columns:\n",
    "#     print(\"The column names match.\")\n",
    "# else:\n",
    "#     if missing_columns2:\n",
    "#         print(f\"Missing columns in DataFrame: {missing_columns2}\")\n",
    "#     if extra_columns2:\n",
    "#         print(f\"Extra columns in DataFrame: {extra_columns2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data dengan 27 kolom:\n",
    "- jumlah kolom sudah ok\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- attachment\n",
    "- penambahan nama file diakhir \"27\"\n",
    "\n",
    "\n",
    "\n",
    "Data dengan 79 kolom:\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- penambahan nama file diakhir \"79\"\n",
    "\n",
    "\n",
    "\n",
    "Data Zendesk:\n",
    "- harus hilangkan double quotes \"\"\n",
    "- delimiternya ;\n",
    "- nama file zendesk\n",
    "\n",
    "\n",
    "Data Omni:\n",
    "- SMS: harus format csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Unable to create/update fields: LastModifiedDa...\n",
       "1    Unable to create/update fields: LastModifiedDa...\n",
       "2    Unable to create/update fields: LastModifiedDa...\n",
       "3    Unable to create/update fields: LastModifiedDa...\n",
       "4    Unable to create/update fields: LastModifiedDa...\n",
       "5    Unable to create/update fields: LastModifiedDa...\n",
       "6    Unable to create/update fields: LastModifiedDa...\n",
       "7    Unable to create/update fields: LastModifiedDa...\n",
       "8    Unable to create/update fields: LastModifiedDa...\n",
       "9    Unable to create/update fields: LastModifiedDa...\n",
       "Name: ERROR, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060524083553095.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['ERROR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zendesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
      "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
      "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
      "\n",
      "     Requester name                                     Ticket subject  \\\n",
      "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
      "1  Masruroh Sodikin                                                  P   \n",
      "\n",
      "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
      "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
      "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
      "\n",
      "  Ticket solved - Timestamp                 Tickets  \n",
      "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
      "1       2024-01-01T00:46:27  1.00000000000000000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket ID</th>\n",
       "      <th>Ticket channel</th>\n",
       "      <th>Assignee ID</th>\n",
       "      <th>Assignee name</th>\n",
       "      <th>Requester ID</th>\n",
       "      <th>Requester name</th>\n",
       "      <th>Ticket subject</th>\n",
       "      <th>Requester created - Timestamp</th>\n",
       "      <th>Ticket created - Timestamp</th>\n",
       "      <th>Ticket solved - Timestamp</th>\n",
       "      <th>Tickets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3435272</td>\n",
       "      <td>Any channel</td>\n",
       "      <td>405257525413</td>\n",
       "      <td>Agent Sosmed 4</td>\n",
       "      <td>27064719563033</td>\n",
       "      <td>vianovia94</td>\n",
       "      <td>[IGC] Minimal opo boloo... ......#relsvideo #r...</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:30:56</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3435273</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>405257335633</td>\n",
       "      <td>Agent Sosmed 3</td>\n",
       "      <td>27064768687769</td>\n",
       "      <td>Masruroh Sodikin</td>\n",
       "      <td>P</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:46:27</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
       "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
       "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
       "\n",
       "     Requester name                                     Ticket subject  \\\n",
       "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
       "1  Masruroh Sodikin                                                  P   \n",
       "\n",
       "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
       "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
       "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
       "\n",
       "  Ticket solved - Timestamp                 Tickets  \n",
       "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
       "1       2024-01-01T00:46:27  1.00000000000000000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\Data Zendesk 1-15 Januari 2024 - sampel.csv\"\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "# Read the CSV file with proper handling of quotes and custom separator\n",
    "with open(file_path, 'r', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "    rows = [row for row in reader]\n",
    "\n",
    "# Split the combined columns that have commas within them\n",
    "split_rows = []\n",
    "for row in rows:\n",
    "    split_row = row[0].split(',') + row[1:]  # Split the first column and add the rest as they are\n",
    "    split_rows.append(split_row)\n",
    "\n",
    "# Create DataFrame from the processed rows\n",
    "df = pd.DataFrame(split_rows[1:], columns=split_rows[0])\n",
    "\n",
    "# Remove double quotes from column names\n",
    "df.columns = df.columns.str.replace('\"', '')\n",
    "\n",
    "# Remove double quotes from all data in the DataFrame\n",
    "df = df.replace('\"', '', regex=True)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "print(df.head())\n",
    "df\n",
    "\n",
    "# If you need to save the cleaned DataFrame to a new CSV file\n",
    "# cleaned_file_path = '/mnt/data/Cleaned_Zendesk_Data.csv'\n",
    "# df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# df=pd.read_csv(path)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# df\u001b[39;00m\n\u001b[0;32m     10\u001b[0m path2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaste\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataloader_v60.0.2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata check\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnew as per 5 June\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcase.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\bricare_20221213_20240604_case_account.csv\"\n",
    "# df=pd.read_csv(path)\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "path2=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\case.csv\"\n",
    "df=pd.read_csv(path2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy data for Alex7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Account Alex7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for account with quoting (DONE)\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_dummy_data(file_path, file_name, num_rows):\n",
    "    # Define possible values for each column\n",
    "    account_names = ['John Doe', 'Jane Smith', 'Mike Brown', 'Lisa Green', 'Mark Taylor']\n",
    "    account_owners = ['Owner A', 'Owner B', 'Owner C', 'Owner D']\n",
    "    nasabah_types = ['Nasabah', 'Non Nasabah']\n",
    "    account_record_types = ['Personal', 'Non Personal']\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Writing to the CSV file\n",
    "    with open(os.path.join(file_path, file_name), mode='w', newline='') as file:\n",
    "        writer = csv.writer(file, quotechar='\"', quoting=csv.QUOTE_ALL)  # Enforce quoting for all fields\n",
    "        writer.writerow(['Account Name', 'CIF No', 'Account Owner', 'No Telp', 'Email', 'Nasabah Type', 'Account Record Type'])\n",
    "        \n",
    "        for _ in range(num_rows):\n",
    "            account_name = random.choice(account_names)\n",
    "            email = f\"{account_name.split(' ')[0].lower()}.{account_name.split(' ')[1].lower()}@example.com\"\n",
    "            writer.writerow([\n",
    "                account_name,\n",
    "                ''.join([\"{}\".format(random.randint(0, 9)) for _ in range(10)]),  # 10-digit CIF No\n",
    "                random.choice(account_owners),\n",
    "                f'+62{random.randint(1000000000, 9999999999)}',  # Phone number\n",
    "                email,\n",
    "                random.choice(nasabah_types),\n",
    "                random.choice(account_record_types)\n",
    "            ])\n",
    "\n",
    "\n",
    "file_path = 'D:\\dataquality2'  # Adjust the path as needed\n",
    "file_name = 'dummy_data_account.csv'\n",
    "num_rows = 1  # Adjust the number of rows as needed\n",
    "\n",
    "generate_dummy_data(file_path, file_name, num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Case ALex 7\n",
    "\n",
    "\n",
    "num of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dummy_casefor_alex11.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_salesforce_case_dummy_data(num_rows=100, start_ttb=1):\n",
    "    import pandas as pd\n",
    "    import random\n",
    "\n",
    "    # Define the columns and possible values\n",
    "    statuses = [\"New\", \"Working\", \"Escalated\"]\n",
    "    types = [\"Electronic\", \"Electrical\", \"Mechanical\"]\n",
    "    case_reasons = [\"Performance\", \"Breakdown\"]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        \"Status\": [random.choice(statuses) for _ in range(num_rows)],\n",
    "        \"Type\": [random.choice(types) for _ in range(num_rows)],\n",
    "        \"Case Reason\": [random.choice(case_reasons) for _ in range(num_rows)],\n",
    "        \"Subject\": [f\"Subject {i+1}\" for i in range(num_rows)],\n",
    "        \"Description\": [f\"Description of issue {i+1}\" for i in range(num_rows)],\n",
    "        \"Legacy_Ticket_ID\": [f\"TTB{start_ttb + i:06d}\" for i in range(num_rows)],  # Generating 6-digit random numbers in order\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = 'dummy_casefor_alex11.csv'\n",
    "    df['record_type'] = 'Case Migration'\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "# Example usage\n",
    "file_path = create_salesforce_case_dummy_data(num_rows=5, start_ttb=1006)\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_5108\\2256201397.py:5: DtypeWarning: Columns (14,16,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\bricaredatareal_20200101_20200201.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df.iloc[:100]\n",
    "df.to_csv('bricare_100_pakpondah.csv',index=False)\n",
    "\n",
    "# data dummy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_data_casenumber_fix_part1.csv with 200000 rows created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_dummy_files(base_filename, total_rows, start_ttb='TTB0000000001'):\n",
    "    \n",
    "    rows_per_file = 200000\n",
    "    num_files = math.ceil(total_rows / rows_per_file)\n",
    "    \n",
    "    start_number = int(start_ttb[3:])\n",
    "    \n",
    "    for file_index in range(num_files):\n",
    "       \n",
    "        if file_index == num_files - 1:\n",
    "            current_rows = total_rows % rows_per_file or rows_per_file\n",
    "        else:\n",
    "            current_rows = rows_per_file\n",
    "        \n",
    "        data = {\n",
    "            'Status': ['Cancelled'] * current_rows,\n",
    "            'Priority': ['Low'] * current_rows,\n",
    "            'Call Type': ['1000'] * current_rows,\n",
    "            'record_type': ['Case Migration'] * current_rows,\n",
    "            'Create_Date': ['1999-01-01'] * current_rows,\n",
    "            'TanggalClosed': ['1999-01-01'] * current_rows,\n",
    "            'Legacy_ticket_id': [f'TTB{str(i).zfill(12)}' for i in range(start_number + file_index * rows_per_file, start_number + file_index * rows_per_file + current_rows)]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        current_filename = f\"{base_filename}_part{file_index + 1}.csv\"\n",
    "        \n",
    "        df.to_csv(current_filename, index=False)\n",
    "        print(f\"{current_filename} with {current_rows} rows created successfully.\")\n",
    "\n",
    "base_filename = 'dummy_data_casenumber_fix'\n",
    "total_rows = 200000\n",
    "start_ttb = 'TTB000000000001'\n",
    "generate_dummy_files(base_filename, total_rows, start_ttb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:dummy_data_casenumber_fix_part1.csv with 200000 rows created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "def generate_dummy_files(folder_path, base_filename, total_rows, start_ttb='TTB0000000001'):\n",
    "    # Define rows per file\n",
    "    rows_per_file = 300000\n",
    "    # Calculate number of files required\n",
    "    num_files = math.ceil(total_rows / rows_per_file)\n",
    "    \n",
    "    # Convert start TTB to an integer number\n",
    "    start_number = int(start_ttb[3:])\n",
    "    \n",
    "    for file_index in range(num_files):\n",
    "        # Determine number of rows for the current file\n",
    "        if file_index == num_files - 1:\n",
    "            current_rows = total_rows % rows_per_file or rows_per_file\n",
    "        else:\n",
    "            current_rows = rows_per_file\n",
    "        \n",
    "        # Generate data for the current file\n",
    "        data = {\n",
    "            'status': ['Cancelled'] * current_rows,\n",
    "            'Priority': ['Low'] * current_rows,\n",
    "            'Call_Type_ID': ['1000'] * current_rows,\n",
    "            'record_type': ['Case Migration'] * current_rows,\n",
    "            'Create_Date' : ['1999-01-01'] * current_rows,\n",
    "            'TanggalClosed' : ['1999-01-01'] * current_rows,\n",
    "            'Legacy_Ticket_ID': [f'TTB{str(i).zfill(10)}' for i in range(start_number + file_index * rows_per_file, start_number + file_index * rows_per_file + current_rows)]\n",
    "        }\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Create filename for the current file\n",
    "        current_filename = os.path.join(folder_path, f\"{base_filename}_part{file_index + 1}.csv\")\n",
    "        \n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv(current_filename, index=False)\n",
    "        print(f\"{current_filename} with {current_rows} rows created successfully.\")\n",
    "    \n",
    "\n",
    "# Define folder path, base filename, total rows, and start TTB\n",
    "folder_path = r\"D:\"\n",
    "base_filename = 'dummy_data_casenumber_fix'\n",
    "total_rows = 200000\n",
    "start_ttb = 'TTB0000000001'\n",
    "\n",
    "# Generate dummy files\n",
    "generate_dummy_files(folder_path, base_filename, total_rows, start_ttb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\dummy_data_casenumber_fix_part1.csv\"\n",
    "\n",
    "df =pd.read_csv(path)\n",
    "df = df.drop(columns=['Legacy_ticket_id'])\n",
    "df.to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Case ALex 7\n",
    "Join the two files based on Ticket ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Ticket_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m extract_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCASENUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicket_ID\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Merge the dataframes based on 'Ticket_ID'\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data_dummy_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTicket_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Reorder the columns to place 'ID' as the first column\u001b[39;00m\n\u001b[0;32m     31\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m merged_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:794\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_left_right_on(left_on, right_on)\n\u001b[0;32m    788\u001b[0m (\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[0;32m    792\u001b[0m     left_drop,\n\u001b[0;32m    793\u001b[0m     right_drop,\n\u001b[1;32m--> 794\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_drop_labels_or_levels(left_drop)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1310\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m     lk \u001b[38;5;241m=\u001b[39m cast(Hashable, lk)\n\u001b[1;32m-> 1310\u001b[0m     left_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1311\u001b[0m     join_names\u001b[38;5;241m.\u001b[39mappend(lk)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:1911\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1909\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1911\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Ticket_ID'"
     ]
    }
   ],
   "source": [
    "#extract the ID and CaseNumber:\n",
    "# SELECT Id, CaseNumber\n",
    "# FROM Case \n",
    "# WHERE CreatedById = '005MR0000004GJ7YAM'\n",
    "\n",
    "# from the real data to join the ID or Case Number and update the records in Case \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded files\n",
    "extract_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_150_casenumberandID.csv\")\n",
    "# test_data_dummy_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\Test Data Dummy.csv\")\n",
    "test_data_dummy_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error062124052713145.csv\")\n",
    "\n",
    "\n",
    "# Split the 'ID;\"CASENUMBER\"' column into two separate columns\n",
    "extract_df[['ID', 'CASENUMBER']] = extract_df['ID;\"CASENUMBER\"'].str.split(';', expand=True)\n",
    "\n",
    "# Drop the original combined column\n",
    "extract_df.drop(columns=['ID;\"CASENUMBER\"'], inplace=True)\n",
    "\n",
    "# Remove leading and trailing quotes from the columns\n",
    "extract_df['ID'] = extract_df['ID'].str.strip('\"')\n",
    "extract_df['CASENUMBER'] = extract_df['CASENUMBER'].str.strip('\"')\n",
    "\n",
    "# Rename columns for easier merging\n",
    "extract_df.rename(columns={'CASENUMBER': 'Ticket_ID'}, inplace=True)\n",
    "\n",
    "# Merge the dataframes based on 'Ticket_ID'\n",
    "merged_df = test_data_dummy_df.merge(extract_df, on='Ticket_ID', how='left')\n",
    "\n",
    "# Reorder the columns to place 'ID' as the first column\n",
    "columns = ['ID'] + [col for col in merged_df.columns if col != 'ID']\n",
    "merged_df = merged_df[columns]\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "# merged_df=merged_df.iloc[:1]\n",
    "merged_df['Call_Type_ID']='1000'\n",
    "merged_df=merged_df.drop('ID', axis=1)\n",
    "merged_df.to_csv('Updated_Test_Data_Dummy_2row.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17788\\1435779221.py:11: DtypeWarning: Columns (47,48,84,108,124,130,132,189,200,203,204,229,247) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(path, delimiter=';')\n"
     ]
    }
   ],
   "source": [
    "# the extracted data from Salesforce after being updated \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\case_150.csv\"\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_case_uat_120k.csv\"\n",
    "\n",
    "df=pd.read_csv(path, delimiter=';')\n",
    "df2 = df[['ID', 'STATUS','CASENUMBER', 'SCC_LEGACY_TICKET_ID__C']]\n",
    "\n",
    "# df3 = df2[df2['SCC_LEGACY_TICKET_ID__C'].notna()]\n",
    "# df2.iloc[70990:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Supported Objects in Queue Name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First Extract all the Queue ID from Group Object, you may use this query:\n",
    "# SELECT Id\n",
    "# where Type ='Queue'\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_group_queueid.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# add one column 'SObjectType' = Case, see the example below:\n",
    "# QueueId,SObjectType\n",
    "# 00GMR0000000trT2AQ,Case\n",
    "\n",
    "df['SObjectType']='Case'\n",
    "df.to_csv('extract_group_queueid.csv', index=False)\n",
    "\n",
    "# Go to Data Loader 'insert' and choose  'QueueSobject' and insert the file with two columns: Id and SObjectType = Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error062124041536955.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "# df= df.drop('ERROR', axis=1)\n",
    "# df= df.drop('ERROR.1', axis=1)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cifno', 'Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date',\n",
       "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
       "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
       "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
       "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
       "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
       "       'user_login_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_case (2)\\bricare_20200806_20240430_0_27.csv\"\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the file path\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_case (2)\\bricare_20200806_20240430_0_27.csv\"\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_case (2)\\bricare_20200806_20240430_1000001_27.csv\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "# df = df.iloc[:1]\n",
    "# Change the year to 2019\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'])\n",
    "df['Create_Date'] = df['Create_Date'].apply(lambda x: x.replace(year=2019))\n",
    "\n",
    "def generate_random_16_digits():\n",
    "    return ''.join([str(np.random.randint(0, 10)) for _ in range(16)])\n",
    "\n",
    "# Add a new column 'Nomor_Kartu' with random 16-digit numbers\n",
    "df['Nomor_Kartu'] = df.apply(lambda x: generate_random_16_digits(), axis=1)\n",
    "\n",
    "# Remove the .0 from Call_Type_ID by converting it to an integer\n",
    "df['Call_Type_ID'] = '8412'\n",
    "df['Call_Type_ID'] = df['Call_Type_ID'].astype(int)\n",
    "\n",
    "def generate_random_15_to_18_digits():\n",
    "    return ''.join([str(np.random.randint(0, 10)) for _ in range(np.random.randint(15, 19))])\n",
    "\n",
    "# Update the 'No_Rekening' column with random 15 to 18 digit numbers\n",
    "df['No_Rekening'] = df.apply(lambda x: generate_random_15_to_18_digits(), axis=1)\n",
    "\n",
    "# add recordtype\n",
    "df['record_type']='Case Migration'\n",
    "\n",
    "df.to_csv('bricare_27col_100k_2.csv', index=False)\n",
    "\n",
    "\n",
    "# df=df[df['Ticket_ID'] == 'TTB000025269322']\n",
    "# df\n",
    "\n",
    "# Extract just the 'Ticket_ID' column and sort it\n",
    "# df2 = df['Ticket_ID'].sort_values(ascending=True)\n",
    "# df2\n",
    "\n",
    "# Display the sorted Series\n",
    "# df2.to_csv('tiket_bricare.csv', index=False)\n",
    "\n",
    "\n",
    "#dummy 120k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "\n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].str.replace('TTB', 'TICKET')\n",
    "\n",
    "df.to_csv('bricare_120k_ticket.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket_ID values have been successfully replaced and saved to 'updated_bricare_120k_ticket.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifno</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>Nominal</th>\n",
       "      <th>...</th>\n",
       "      <th>email</th>\n",
       "      <th>full_name</th>\n",
       "      <th>no_telepon</th>\n",
       "      <th>approver_login</th>\n",
       "      <th>approver_name</th>\n",
       "      <th>SLAResolution</th>\n",
       "      <th>submitter_login_id</th>\n",
       "      <th>submitter_user_group</th>\n",
       "      <th>user_login_name</th>\n",
       "      <th>record_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LD86852</td>\n",
       "      <td>TICKET000998035</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b2714defcfef8c65d3154367beb604aa</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>23300800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RCAW297</td>\n",
       "      <td>TICKET000998036</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal tarik tunai &amp; terdebet di AT...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b69e3868bc8b6c3231df7d691a5595fe</td>\n",
       "      <td>5073214431798791</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AOY4038</td>\n",
       "      <td>TICKET000998037</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>948421d448fb51f1cf27584b1256a120</td>\n",
       "      <td>6430217575043687</td>\n",
       "      <td>246300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUA0263</td>\n",
       "      <td>TICKET000998038</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal transfer &amp; terdebet jaringan...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>a4aa532c65d1503dcc41ef396d3c42b8</td>\n",
       "      <td>4067829670996616</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOL2556</td>\n",
       "      <td>TICKET000998039</td>\n",
       "      <td>8412</td>\n",
       "      <td>Kartu ATM BRI Tertelan di MESIN ATM</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>95d4f5421b5b387726b5d619dbe24155</td>\n",
       "      <td>272778316775473552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90119221.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cifno        Ticket_ID  Call_Type_ID  \\\n",
       "0  LD86852  TICKET000998035          8412   \n",
       "1  RCAW297  TICKET000998036          8412   \n",
       "2  AOY4038  TICKET000998037          8412   \n",
       "3  AUA0263  TICKET000998038          8412   \n",
       "4  HOL2556  TICKET000998039          8412   \n",
       "\n",
       "                                           Call_Type          Create_Date  \\\n",
       "0  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "1  Nasabah BRI gagal tarik tunai & terdebet di AT...  2019-08-06 16:52:14   \n",
       "2  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "3  Nasabah BRI gagal transfer & terdebet jaringan...  2019-08-06 16:52:14   \n",
       "4                Kartu ATM BRI Tertelan di MESIN ATM  2019-08-06 16:52:14   \n",
       "\n",
       "  gateway            Jenis_Laporan                      Nama_Nasabah  \\\n",
       "0   Phone  Complaint - Transaction  b2714defcfef8c65d3154367beb604aa   \n",
       "1   Phone  Complaint - Transaction  b69e3868bc8b6c3231df7d691a5595fe   \n",
       "2   Phone  Complaint - Transaction  948421d448fb51f1cf27584b1256a120   \n",
       "3   Phone  Complaint - Transaction  a4aa532c65d1503dcc41ef396d3c42b8   \n",
       "4   Phone                  Request  95d4f5421b5b387726b5d619dbe24155   \n",
       "\n",
       "          No_Rekening     Nominal  ... email  \\\n",
       "0     878667067646102  23300800.0  ...   NaN   \n",
       "1    5073214431798791   1000000.0  ...   NaN   \n",
       "2    6430217575043687    246300.0  ...   NaN   \n",
       "3    4067829670996616    100000.0  ...   NaN   \n",
       "4  272778316775473552         0.0  ...   NaN   \n",
       "\n",
       "                          full_name no_telepon approver_login  \\\n",
       "0  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "1  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "2  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "3  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "4  fcc2dc136c7c3d69fb080cdd2b1648f1        NaN            NaN   \n",
       "\n",
       "                      approver_name  SLAResolution submitter_login_id  \\\n",
       "0  d41d8cd98f00b204e9800998ecf8427e           20.0                NaN   \n",
       "1  d41d8cd98f00b204e9800998ecf8427e           10.0                NaN   \n",
       "2  d41d8cd98f00b204e9800998ecf8427e           20.0                NaN   \n",
       "3  d41d8cd98f00b204e9800998ecf8427e           20.0                NaN   \n",
       "4  d41d8cd98f00b204e9800998ecf8427e           20.0         90119221.0   \n",
       "\n",
       "   submitter_user_group                   user_login_name     record_type  \n",
       "0                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "1                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "2                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "3                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "4           LCC-CCTCALL  fcc2dc136c7c3d69fb080cdd2b1648f1  Case Migration  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "extract_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\extract_dummyfromserver_120k.csv.xls\")\n",
    "bricare_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\")\n",
    "\n",
    "# Check the lengths of the CaseNumber and Ticket_ID columns\n",
    "extract_length = len(extract_df['CaseNumber'])\n",
    "bricare_length = len(bricare_df['Ticket_ID'])\n",
    "\n",
    "# Ensure the number of CaseNumber matches the number of Ticket_ID\n",
    "if extract_length >= bricare_length:\n",
    "    # Trim the extract dataframe to match the length of the bricare dataframe\n",
    "    trimmed_extract_df = extract_df.iloc[:bricare_length]\n",
    "    \n",
    "    # Replace Ticket_ID values with CaseNumber values\n",
    "    bricare_df['Ticket_ID'] = trimmed_extract_df['CaseNumber'].values\n",
    "    \n",
    "    # Save the updated bricare dataframe to a new CSV file\n",
    "    output_file_path = 'updated_bricare_120k_ticket.csv'\n",
    "    # bricare_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(\"Ticket_ID values have been successfully replaced and saved to 'updated_bricare_120k_ticket.csv'.\")\n",
    "else:\n",
    "    print(\"Error: The number of CaseNumber entries is less than the number of Ticket_ID entries. Ensure they are one-to-one.\")\n",
    "\n",
    "# Display the first few rows of the updated bricare dataframe\n",
    "bricare_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\updated_bricare_120k_ticket_with_id.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df=df.iloc[:1]\n",
    "df.to_csv('updated_bricare_1_ticket.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket_ID values have been successfully replaced and the Id column has been added and saved to 'updated_bricare_120k_ticket_with_id.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>cifno</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>...</th>\n",
       "      <th>email</th>\n",
       "      <th>full_name</th>\n",
       "      <th>no_telepon</th>\n",
       "      <th>approver_login</th>\n",
       "      <th>approver_name</th>\n",
       "      <th>SLAResolution</th>\n",
       "      <th>submitter_login_id</th>\n",
       "      <th>submitter_user_group</th>\n",
       "      <th>user_login_name</th>\n",
       "      <th>record_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500MR000004Q2wkYAC</td>\n",
       "      <td>LD86852</td>\n",
       "      <td>TICKET000998035</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b2714defcfef8c65d3154367beb604aa</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500MR000004Q2wlYAC</td>\n",
       "      <td>RCAW297</td>\n",
       "      <td>TICKET000998036</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal tarik tunai &amp; terdebet di AT...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b69e3868bc8b6c3231df7d691a5595fe</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500MR000004Q2wmYAC</td>\n",
       "      <td>AOY4038</td>\n",
       "      <td>TICKET000998037</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>948421d448fb51f1cf27584b1256a120</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500MR000004Q2wnYAC</td>\n",
       "      <td>AUA0263</td>\n",
       "      <td>TICKET000998038</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal transfer &amp; terdebet jaringan...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>a4aa532c65d1503dcc41ef396d3c42b8</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500MR000004Q2woYAC</td>\n",
       "      <td>HOL2556</td>\n",
       "      <td>TICKET000998039</td>\n",
       "      <td>8412</td>\n",
       "      <td>Kartu ATM BRI Tertelan di MESIN ATM</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>95d4f5421b5b387726b5d619dbe24155</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90119221.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id    cifno        Ticket_ID  Call_Type_ID  \\\n",
       "0  500MR000004Q2wkYAC  LD86852  TICKET000998035          8412   \n",
       "1  500MR000004Q2wlYAC  RCAW297  TICKET000998036          8412   \n",
       "2  500MR000004Q2wmYAC  AOY4038  TICKET000998037          8412   \n",
       "3  500MR000004Q2wnYAC  AUA0263  TICKET000998038          8412   \n",
       "4  500MR000004Q2woYAC  HOL2556  TICKET000998039          8412   \n",
       "\n",
       "                                           Call_Type          Create_Date  \\\n",
       "0  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "1  Nasabah BRI gagal tarik tunai & terdebet di AT...  2019-08-06 16:52:14   \n",
       "2  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "3  Nasabah BRI gagal transfer & terdebet jaringan...  2019-08-06 16:52:14   \n",
       "4                Kartu ATM BRI Tertelan di MESIN ATM  2019-08-06 16:52:14   \n",
       "\n",
       "  gateway            Jenis_Laporan                      Nama_Nasabah  \\\n",
       "0   Phone  Complaint - Transaction  b2714defcfef8c65d3154367beb604aa   \n",
       "1   Phone  Complaint - Transaction  b69e3868bc8b6c3231df7d691a5595fe   \n",
       "2   Phone  Complaint - Transaction  948421d448fb51f1cf27584b1256a120   \n",
       "3   Phone  Complaint - Transaction  a4aa532c65d1503dcc41ef396d3c42b8   \n",
       "4   Phone                  Request  95d4f5421b5b387726b5d619dbe24155   \n",
       "\n",
       "       No_Rekening  ...  email                         full_name no_telepon  \\\n",
       "0  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "1  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "2  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "3  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "4  878667067646102  ...    NaN  fcc2dc136c7c3d69fb080cdd2b1648f1        NaN   \n",
       "\n",
       "  approver_login                     approver_name SLAResolution  \\\n",
       "0            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "1            NaN  d41d8cd98f00b204e9800998ecf8427e          10.0   \n",
       "2            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "3            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "4            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "\n",
       "  submitter_login_id submitter_user_group                   user_login_name  \\\n",
       "0                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "1                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "2                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "3                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "4         90119221.0          LCC-CCTCALL  fcc2dc136c7c3d69fb080cdd2b1648f1   \n",
       "\n",
       "      record_type  \n",
       "0  Case Migration  \n",
       "1  Case Migration  \n",
       "2  Case Migration  \n",
       "3  Case Migration  \n",
       "4  Case Migration  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "extract_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\extract_dummyfromserver_120k.csv.xls\")\n",
    "bricare_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\")\n",
    "\n",
    "# Check the lengths of the CaseNumber and Ticket_ID columns\n",
    "extract_length = len(extract_df['CaseNumber'])\n",
    "bricare_length = len(bricare_df['Ticket_ID'])\n",
    "\n",
    "# Ensure the number of CaseNumber matches the number of Ticket_ID\n",
    "if extract_length >= bricare_length:\n",
    "    # Trim the extract dataframe to match the length of the bricare dataframe\n",
    "    trimmed_extract_df = extract_df.iloc[:bricare_length]\n",
    "    \n",
    "    # Replace Ticket_ID values with CaseNumber values\n",
    "    bricare_df['Ticket_ID'] = trimmed_extract_df['CaseNumber'].values\n",
    "    \n",
    "    # Add the Id column from the extract dataframe to the bricare dataframe\n",
    "    bricare_df['Id'] = trimmed_extract_df['Id'].values\n",
    "    \n",
    "    # Reorder the columns to place Id as the first column\n",
    "    cols = bricare_df.columns.tolist()\n",
    "    cols = ['Id'] + [col for col in cols if col != 'Id']\n",
    "    bricare_df = bricare_df[cols]\n",
    "    \n",
    "    # Save the updated bricare dataframe to a new CSV file\n",
    "    output_file_path = 'updated_bricare_120k_ticket_with_id.csv'\n",
    "    bricare_df['Nomor_Kartu']='1706269727199000'\n",
    "    bricare_df['No_Rekening']='878667067646102'\n",
    "    \n",
    "    bricare_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(\"Ticket_ID values have been successfully replaced and the Id column has been added and saved to 'updated_bricare_120k_ticket_with_id.csv'.\")\n",
    "else:\n",
    "    print(\"Error: The number of CaseNumber entries is less than the number of Ticket_ID entries. Ensure they are one-to-one.\")\n",
    "\n",
    "# Display the first few rows of the updated bricare dataframe\n",
    "bricare_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-27 04:30:20'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Original date and time in UTC\n",
    "utc_time_str_2 = \"2024-06-26T21:30:20.000+0000\"\n",
    "utc_time_2 = datetime.strptime(utc_time_str_2, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "# Convert to Jakarta time\n",
    "jakarta_tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "jakarta_time_2 = utc_time_2.astimezone(jakarta_tz)\n",
    "\n",
    "# Format the Jakarta time as a string\n",
    "jakarta_time_str_2 = jakarta_time_2.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "jakarta_time_str_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-26T18:10:20.000000+0000'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Jakarta date and time string\n",
    "jakarta_time_str_2 = \"2024-06-27 01:10:20\"\n",
    "\n",
    "# Define Jakarta timezone\n",
    "jakarta_tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "\n",
    "# Parse the Jakarta time string into a datetime object\n",
    "jakarta_time_2 = datetime.strptime(jakarta_time_str_2, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Localize the Jakarta time\n",
    "jakarta_time_2 = jakarta_tz.localize(jakarta_time_2)\n",
    "\n",
    "# Convert to UTC\n",
    "utc_time_2 = jakarta_time_2.astimezone(pytz.utc)\n",
    "\n",
    "# Format the UTC time as a string\n",
    "utc_time_str_2 = utc_time_2.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "utc_time_str_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the bricare files and the 220k file\n",
    "file_220k = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\220K.csv.xls\")\n",
    "bricare_1 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\")\n",
    "bricare_2 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k_2.csv\")\n",
    "\n",
    "# Merge Id and CaseNumber into bricare files\n",
    "bricare_1 = bricare_1.merge(file_220k[['Id', 'CaseNumber']], left_index=True, right_index=True)\n",
    "bricare_2 = bricare_2.merge(file_220k[['Id', 'CaseNumber']], left_index=True, right_index=True)\n",
    "\n",
    "# Replace Ticket_ID with CaseNumber from 220K file\n",
    "bricare_1['Ticket_ID'] = bricare_1['CaseNumber']\n",
    "bricare_2['Ticket_ID'] = bricare_2['CaseNumber']\n",
    "\n",
    "# Drop the temporary 'CaseNumber' column\n",
    "bricare_1.drop(columns=['CaseNumber'], inplace=True)\n",
    "bricare_2.drop(columns=['CaseNumber'], inplace=True)\n",
    "\n",
    "# Combine the Ticket_IDs from both files to identify overlaps\n",
    "combined_ticket_ids = pd.concat([bricare_1['Ticket_ID'], bricare_2['Ticket_ID']])\n",
    "\n",
    "# Identify duplicate Ticket_IDs\n",
    "duplicates = combined_ticket_ids[combined_ticket_ids.duplicated()]\n",
    "\n",
    "# Reassign duplicates in bricare_2 to ensure uniqueness\n",
    "for i, dup in enumerate(duplicates):\n",
    "    bricare_2.loc[bricare_2['Ticket_ID'] == dup, 'Ticket_ID'] += f'_DUP{i}'\n",
    "\n",
    "# Save the transformed files\n",
    "bricare_1.to_csv('bricare_27col_100k_transformed_no_suffix.csv', index=False)\n",
    "bricare_2.to_csv('bricare_27col_100k_2_transformed_no_suffix.csv', index=False)\n",
    "\n",
    "# Check if Ticket_IDs are completely different between the two files\n",
    "ticket_id_1 = set(bricare_1['Ticket_ID'])\n",
    "ticket_id_2 = set(bricare_2['Ticket_ID'])\n",
    "\n",
    "# Ensure no Ticket_ID is shared between the two files\n",
    "unique_across_files = ticket_id_1.isdisjoint(ticket_id_2)\n",
    "\n",
    "\n",
    "unique_across_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in the Ticket_ID column are completely different between the two files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files into pandas DataFrames\n",
    "file1 = pd.read_csv(r\"D:\\dataquality2\\bricare_27col_100k_transformed_no_suffix.csv\")\n",
    "file2 = pd.read_csv(r\"D:\\dataquality2\\bricare_27col_100k_2_transformed_no_suffix.csv\")\n",
    "\n",
    "# Extract the Ticket_ID column from both DataFrames\n",
    "ticket_ids_file1 = file1['Ticket_ID']\n",
    "ticket_ids_file2 = file2['Ticket_ID']\n",
    "\n",
    "# Check if there are any common values\n",
    "common_ids = set(ticket_ids_file1).intersection(set(ticket_ids_file2))\n",
    "\n",
    "if len(common_ids) == 0:\n",
    "    print(\"All values in the Ticket_ID column are completely different between the two files.\")\n",
    "else:\n",
    "    print(\"There are common values in the Ticket_ID column between the two files.\")\n",
    "    print(\"Common values:\", common_ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Group Name between SF and BRicare\n",
    "\n",
    "from pak Suyanto\n",
    "\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Group name ds_ej.csv\"\n",
    "\n",
    "from SF\n",
    "\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\CSV 2 - Group.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file1_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\ds_ej.csv\"\n",
    "file2_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\CSV 2 - Group.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "\n",
    "user_groups_ds_ej = df1['usergrouptxt'].str.strip()\n",
    "queue_group_names = df2['Queue & Group Name'].str.strip()\n",
    "\n",
    "\n",
    "missing_groups = user_groups_ds_ej[~user_groups_ds_ej.isin(queue_group_names)]\n",
    "\n",
    "missing_groups_list = missing_groups.tolist()\n",
    "len(missing_groups_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 345)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV and Excel files\n",
    "csv_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Pak Hadi Green.xlsx\"\n",
    "excel_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Cleaned.xlsx\"\n",
    "\n",
    "# Read the files\n",
    "df_csv = pd.read_excel(csv_file_path)\n",
    "df_excel = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Extract the relevant columns for comparison\n",
    "case_types = set(df_csv['Case Types'].astype(str))\n",
    "call_type_numbers = set(df_excel['Call Type Number'].astype(str))\n",
    "\n",
    "# Find the missing and common case types\n",
    "missing_case_types = case_types - call_type_numbers\n",
    "common_case_types = case_types & call_type_numbers\n",
    "\n",
    "# Save the results to CSV files\n",
    "missing_case_types_df = pd.DataFrame(missing_case_types, columns=[\"Missing Case Types\"])\n",
    "common_case_types_df = pd.DataFrame(common_case_types, columns=[\"Common Case Types\"])\n",
    "\n",
    "missing_case_types_df.to_csv('case_types_missing.csv', index=False)\n",
    "common_case_types_df.to_csv('case_types_common.csv', index=False)\n",
    "\n",
    "# Provide the links to download the files\n",
    "len(missing_case_types_df), len(common_case_types_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call Type Number</th>\n",
       "      <th>Active</th>\n",
       "      <th>Send to Drone?</th>\n",
       "      <th>Additional Details (Formulir Detail)</th>\n",
       "      <th>Customer Segment</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub Product</th>\n",
       "      <th>Case Category</th>\n",
       "      <th>Case Type Description</th>\n",
       "      <th>Sub-Description</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 43</th>\n",
       "      <th>Unnamed: 44</th>\n",
       "      <th>Unnamed: 45</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>Unnamed: 47</th>\n",
       "      <th>Unnamed: 48</th>\n",
       "      <th>Unnamed: 49</th>\n",
       "      <th>Unnamed: 50</th>\n",
       "      <th>Unnamed: 51</th>\n",
       "      <th>external_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>nomor ponsel nasabah yang bisa dihubungi: \\nID CERIA:\\nNO HP yg terdaftar:\\nAlasan:</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Nasabah Menanyakan Informasi Pengajuan Terkait CERIA</td>\n",
       "      <td>1. Cara, Syarat &amp; Ketentuan\\n2. Status\\n3. Pembatalan</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Products / Promotion Inquiry</td>\n",
       "      <td>Nasabah Menyakan Terkait Promo dan Program CERIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Request</td>\n",
       "      <td>Nasabah Mengajukan Pelunasan Awal Cicilan CERIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1005</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Nasabah mengajukan pemblokiran Sementara Akun Ceria\\nNomor ID :\\nAlasan pemblokiran :\\n\\nMohon bantuan tindak lanjut\\nTerima kasih</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Request</td>\n",
       "      <td>Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1008</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Request</td>\n",
       "      <td>Nasabah Mengajukan Pengaktifan Akun Ceria Terblokir karena Fraud Detection System</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>TRB-8</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korporasi</td>\n",
       "      <td>Servicing</td>\n",
       "      <td>Cash Management</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Nasabah Menanyakan Pengajuan Pendaftaran Product BRIVA tidak disetujui pada QLOLA Apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRB-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>TRB-9</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korporasi</td>\n",
       "      <td>Servicing</td>\n",
       "      <td>Cash Management</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Nasabah Menanyakan Pengajuan Pendaftaran Product Corporate Billing Management (BRI CBM) tidak disetujui pada QLOLA Apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRB-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>UMI-1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMKM</td>\n",
       "      <td>Transaction Banking</td>\n",
       "      <td>Merchant Solutions</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Calon Mitra UMI Menanyakan Cara, Syarat, dan Ketentuan mengenai Pengajuan Menjadi Mitra UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMI-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>UMI-2</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMKM</td>\n",
       "      <td>Transaction Banking</td>\n",
       "      <td>Merchant Solutions</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Mitra UMI Menanyakan Informasi Fee Transaksi</td>\n",
       "      <td>1. Disbursment\\n2. Loan quality fee</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMI-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>UMI-3</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMKM</td>\n",
       "      <td>Transaction Banking</td>\n",
       "      <td>Merchant Solutions</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>Mitra UMI Komplain Fee Transaksi Belum Masuk ke Rekening BRI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMI-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Call Type Number Active Send to Drone?  \\\n",
       "0               1000   TRUE          FALSE   \n",
       "1               1002   TRUE          FALSE   \n",
       "2               1003   TRUE          FALSE   \n",
       "3               1005   TRUE          FALSE   \n",
       "4               1008   TRUE          FALSE   \n",
       "..               ...    ...            ...   \n",
       "379            TRB-8   TRUE          FALSE   \n",
       "380            TRB-9   TRUE          FALSE   \n",
       "381            UMI-1   TRUE          FALSE   \n",
       "382            UMI-2   TRUE          FALSE   \n",
       "383            UMI-3   TRUE          FALSE   \n",
       "\n",
       "                                                                                                   Additional Details (Formulir Detail)  \\\n",
       "0                                                   nomor ponsel nasabah yang bisa dihubungi: \\nID CERIA:\\nNO HP yg terdaftar:\\nAlasan:   \n",
       "1                                                                                                                                   NaN   \n",
       "2                                                                                                                                   NaN   \n",
       "3    Nasabah mengajukan pemblokiran Sementara Akun Ceria\\nNomor ID :\\nAlasan pemblokiran :\\n\\nMohon bantuan tindak lanjut\\nTerima kasih   \n",
       "4                                                                                                                                   NaN   \n",
       "..                                                                                                                                  ...   \n",
       "379                                                                                                                                 NaN   \n",
       "380                                                                                                                                 NaN   \n",
       "381                                                                                                                                 NaN   \n",
       "382                                                                                                                                 NaN   \n",
       "383                                                                                                                                 NaN   \n",
       "\n",
       "    Customer Segment              Product         Sub Product  \\\n",
       "0      Individu Umum                Loans       KTA - Digital   \n",
       "1      Individu Umum                Loans       KTA - Digital   \n",
       "2      Individu Umum                Loans       KTA - Digital   \n",
       "3      Individu Umum                Loans       KTA - Digital   \n",
       "4      Individu Umum                Loans       KTA - Digital   \n",
       "..               ...                  ...                 ...   \n",
       "379        Korporasi            Servicing     Cash Management   \n",
       "380        Korporasi            Servicing     Cash Management   \n",
       "381             UMKM  Transaction Banking  Merchant Solutions   \n",
       "382             UMKM  Transaction Banking  Merchant Solutions   \n",
       "383             UMKM  Transaction Banking  Merchant Solutions   \n",
       "\n",
       "                    Case Category  \\\n",
       "0                         Inquiry   \n",
       "1    Products / Promotion Inquiry   \n",
       "2                         Request   \n",
       "3                         Request   \n",
       "4                         Request   \n",
       "..                            ...   \n",
       "379                       Inquiry   \n",
       "380                       Inquiry   \n",
       "381                       Inquiry   \n",
       "382                       Inquiry   \n",
       "383       Complaint - Transaction   \n",
       "\n",
       "                                                                                                       Case Type Description  \\\n",
       "0                                                                       Nasabah Menanyakan Informasi Pengajuan Terkait CERIA   \n",
       "1                                                                           Nasabah Menyakan Terkait Promo dan Program CERIA   \n",
       "2                                                                            Nasabah Mengajukan Pelunasan Awal Cicilan CERIA   \n",
       "3                                                                                       Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4                                          Nasabah Mengajukan Pengaktifan Akun Ceria Terblokir karena Fraud Detection System   \n",
       "..                                                                                                                       ...   \n",
       "379                                   Nasabah Menanyakan Pengajuan Pendaftaran Product BRIVA tidak disetujui pada QLOLA Apps   \n",
       "380  Nasabah Menanyakan Pengajuan Pendaftaran Product Corporate Billing Management (BRI CBM) tidak disetujui pada QLOLA Apps   \n",
       "381                              Calon Mitra UMI Menanyakan Cara, Syarat, dan Ketentuan mengenai Pengajuan Menjadi Mitra UMI   \n",
       "382                                                                             Mitra UMI Menanyakan Informasi Fee Transaksi   \n",
       "383                                                             Mitra UMI Komplain Fee Transaksi Belum Masuk ke Rekening BRI   \n",
       "\n",
       "                                           Sub-Description  ... Unnamed: 43  \\\n",
       "0    1. Cara, Syarat & Ketentuan\\n2. Status\\n3. Pembatalan  ...         NaN   \n",
       "1                                                      NaN  ...         NaN   \n",
       "2                                                      NaN  ...         NaN   \n",
       "3                                                      NaN  ...         NaN   \n",
       "4                                                      NaN  ...         NaN   \n",
       "..                                                     ...  ...         ...   \n",
       "379                                                    NaN  ...         NaN   \n",
       "380                                                    NaN  ...         NaN   \n",
       "381                                                    NaN  ...         NaN   \n",
       "382                    1. Disbursment\\n2. Loan quality fee  ...         NaN   \n",
       "383                                                    NaN  ...         NaN   \n",
       "\n",
       "    Unnamed: 44  Unnamed: 45  Unnamed: 46 Unnamed: 47  Unnamed: 48  \\\n",
       "0           NaN          NaN          NaN         NaN          NaN   \n",
       "1           NaN          NaN          NaN         NaN          NaN   \n",
       "2           NaN          NaN          NaN         NaN          NaN   \n",
       "3           NaN          NaN          NaN         NaN          NaN   \n",
       "4           NaN          NaN          NaN         NaN          NaN   \n",
       "..          ...          ...          ...         ...          ...   \n",
       "379         NaN          NaN          NaN         NaN          NaN   \n",
       "380         NaN          NaN          NaN         NaN          NaN   \n",
       "381         NaN          NaN          NaN         NaN          NaN   \n",
       "382         NaN          NaN          NaN         NaN          NaN   \n",
       "383         NaN          NaN          NaN         NaN          NaN   \n",
       "\n",
       "    Unnamed: 49  Unnamed: 50 Unnamed: 51  external_id  \n",
       "0           NaN          NaN         NaN         1000  \n",
       "1           NaN          NaN         NaN         1002  \n",
       "2           NaN          NaN         NaN         1003  \n",
       "3           NaN          NaN         NaN         1005  \n",
       "4           NaN          NaN         NaN         1008  \n",
       "..          ...          ...         ...          ...  \n",
       "379         NaN          NaN         NaN        TRB-8  \n",
       "380         NaN          NaN         NaN        TRB-9  \n",
       "381         NaN          NaN         NaN        UMI-1  \n",
       "382         NaN          NaN         NaN        UMI-2  \n",
       "383         NaN          NaN         NaN        UMI-3  \n",
       "\n",
       "[384 rows x 53 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Cleaned - TBM.xlsx\"\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Cleaned - TBM.xlsx\"\n",
    "\n",
    "\n",
    "df= pd.read_excel(path)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# df = df[df['Call Type Number']== 'UMI-3'] # Modify this\n",
    "df['Send to Drone?'] = df['Send to Drone?'].replace({'No': 'FALSE', 'Yes': 'TRUE'})\n",
    "df['Active'] = df['Active'].astype(str).str.upper()\n",
    "df['external_id']=df['Call Type Number']\n",
    "\n",
    "\n",
    "# df.to_csv('CallType_umi3.csv', index=False)\n",
    "df.to_csv('CallType_cleaned.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casenumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found and saved to CSV.\n",
      "Gaps found and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_ticket_ids(file_path, case_column, output_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract the numeric part of the ticket IDs\n",
    "    df['Numeric_ID'] = df[case_column].str.extract(r'(\\d+)')\n",
    "    \n",
    "    # Drop rows with NaN values in 'Numeric_ID'\n",
    "    df = df.dropna(subset=['Numeric_ID'])\n",
    "    \n",
    "    # Convert the numeric part to integers\n",
    "    df['Numeric_ID'] = df['Numeric_ID'].astype(int)\n",
    "    \n",
    "    # Sort the DataFrame by the numeric ID\n",
    "    df_sorted = df.sort_values(by='Numeric_ID').reset_index(drop=True)\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = df_sorted[df_sorted.duplicated(subset='Numeric_ID', keep=False)]\n",
    "    \n",
    "    # Find gaps\n",
    "    df_sorted['Next_ID'] = df_sorted['Numeric_ID'].shift(-1)\n",
    "    df_sorted['Gap'] = df_sorted['Next_ID'] - df_sorted['Numeric_ID']\n",
    "    gaps = df_sorted[df_sorted['Gap'] != 1]\n",
    "    \n",
    "    # Detailed gap analysis\n",
    "    gap_details = []\n",
    "    for idx, row in gaps.iterrows():\n",
    "        if not pd.isna(row['Next_ID']):\n",
    "            gap_start = row[case_column]\n",
    "            gap_end = df_sorted.iloc[idx + 1][case_column]\n",
    "            gap_size = row['Gap'] - 1\n",
    "            gap_details.append({'Gap_Start': gap_start, 'Gap_End': gap_end, 'Gap_Size': gap_size})\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    if not duplicates.empty:\n",
    "        duplicates.to_csv(output_path + '_duplicates.csv', index=False)\n",
    "        print(\"Duplicates found and saved to CSV.\")\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "    \n",
    "    if gap_details:\n",
    "        gaps_df = pd.DataFrame(gap_details)\n",
    "        gaps_df.to_csv(output_path + '_gaps.csv', index=False)\n",
    "        print(\"Gaps found and saved to CSV.\")\n",
    "    else:\n",
    "        print(\"No gaps found.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Case_2024-07-03_okta.csv\"\n",
    "case_column = 'CASENUMBER'\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\output\"\n",
    "\n",
    "check_ticket_ids(file_path, case_column, output_path)\n",
    "#select SCC_Legacy_Ticket_ID__c, CaseNumber from case where SCC_Legacy_Ticket_ID__c ='TTB0000088100' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_1.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_2.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_3.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_4.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_5.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_6.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_7.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_8.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_9.csv\n"
     ]
    }
   ],
   "source": [
    "# To split the extracted files, the file containing all Legacy ID having non-empty values\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_csv(input_file, output_directory, output_prefix, rows_per_file=200000):\n",
    "    # Read the CSV file with semicolon as the delimiter\n",
    "    df = pd.read_csv(input_file, delimiter=';')\n",
    "\n",
    "    # Ensure the SCC_LEGACY_TICKET_ID__C column is empty\n",
    "    df['SCC_LEGACY_TICKET_ID__C'] = ''\n",
    "\n",
    "    # Get the total number of rows in the DataFrame\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Calculate the number of files needed\n",
    "    num_files = (total_rows // rows_per_file) + (1 if total_rows % rows_per_file != 0 else 0)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for i in range(num_files):\n",
    "        start_row = i * rows_per_file\n",
    "        end_row = start_row + rows_per_file\n",
    "\n",
    "        # Slice the DataFrame to get the chunk\n",
    "        chunk = df.iloc[start_row:end_row]\n",
    "\n",
    "        # Create a filename for the chunk\n",
    "        output_file = os.path.join(output_directory, f\"{output_prefix}_part_{i+1}.csv\")\n",
    "\n",
    "        # Save the chunk to a CSV file, including the header\n",
    "        chunk.to_csv(output_file, index=False, sep=';')\n",
    "\n",
    "        print(f\"Saved {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\extract.csv\"       \n",
    "    output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\"     \n",
    "    output_prefix = 'output'\n",
    "    rows_per_file = 200000                \n",
    "\n",
    "    split_csv(input_file, output_directory, output_prefix, rows_per_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>record_type</th>\n",
       "      <th>Legacy_ticket_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62995</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62996</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62997</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62998</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62999</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Status Priority  Call Type     record_type Legacy_ticket_id\n",
       "0      Cancelled      Low       1000  Case Migration    TTB0061614201\n",
       "1      Cancelled      Low       1000  Case Migration    TTB0061614202\n",
       "2      Cancelled      Low       1000  Case Migration    TTB0061614203\n",
       "3      Cancelled      Low       1000  Case Migration    TTB0061614204\n",
       "4      Cancelled      Low       1000  Case Migration    TTB0061614205\n",
       "...          ...      ...        ...             ...              ...\n",
       "62995  Cancelled      Low       1000  Case Migration    TTB0061677196\n",
       "62996  Cancelled      Low       1000  Case Migration    TTB0061677197\n",
       "62997  Cancelled      Low       1000  Case Migration    TTB0061677198\n",
       "62998  Cancelled      Low       1000  Case Migration    TTB0061677199\n",
       "62999  Cancelled      Low       1000  Case Migration    TTB0061677200\n",
       "\n",
       "[63000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\Dummy\\dummy_data_casenumber60_part600.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "#\n",
    "df=df.iloc[:63000]\n",
    "# df = df.drop(columns=['SUBJECT','PRIORITY','STATUS'])\n",
    "df.to_csv(path, index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Account files in server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TesseractNotFoundError",
     "evalue": "tesseract is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:255\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 255\u001b[0m     proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msubprocess_args())\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:969\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    967\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:1438\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1438\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1452\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Perform OCR on the image to extract text\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m extracted_text \u001b[38;5;241m=\u001b[39m \u001b[43mpytesseract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Split the text into lines\u001b[39;00m\n\u001b[0;32m     15\u001b[0m lines \u001b[38;5;241m=\u001b[39m extracted_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:423\u001b[0m, in \u001b[0;36mimage_to_string\u001b[1;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBYTES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDICT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:426\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    424\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[0;32m    425\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[1;32m--> 426\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    427\u001b[0m }[output_type]()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:288\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[0;32m    278\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    286\u001b[0m     }\n\u001b[1;32m--> 288\u001b[0m     run_tesseract(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:260\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m: tesseract is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your image file\n",
    "image_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\WhatsApp Image 2024-07-05 at 15.51.41_53d1f445.jpg\"\n",
    "\n",
    "# Load the image using PIL\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Perform OCR on the image to extract text\n",
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Split the text into lines\n",
    "lines = extracted_text.split('\\n')\n",
    "\n",
    "# Filter out non-CSV lines and sort them in ascending order\n",
    "csv_files = sorted([line for line in lines if 'csv' in line])\n",
    "\n",
    "# Create a DataFrame to display the sorted list of CSV files\n",
    "csv_files_df = pd.DataFrame(csv_files, columns=['CSV Files'])\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "sorted_excel_file_path = \"sorted_csv_file_names.xlsx\"\n",
    "csv_files_df.to_excel(sorted_excel_file_path, index=False)\n",
    "\n",
    "print(f\"Excel file with sorted CSV names saved at: {sorted_excel_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the Legacy Ticket Id into blank or NuLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>CASENUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500Mg000005shaOIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500Mg000005shaPIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500Mg000005shaQIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500Mg000005shaRIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500Mg000005shaSIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID SCC_LEGACY_TICKET_ID__C     CASENUMBER\n",
       "0  500Mg000005shaOIAQ                          TTB0000122839\n",
       "1  500Mg000005shaPIAQ                          TTB0000122840\n",
       "2  500Mg000005shaQIAQ                          TTB0000122841\n",
       "3  500Mg000005shaRIAQ                          TTB0000122842\n",
       "4  500Mg000005shaSIAQ                          TTB0000122843"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_legacynonemppty77_cleaned2.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "df['SCC_LEGACY_TICKET_ID__C']=''\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_legacynonemppty77_cleaned2.csv\",index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>CASENUMBER</th>\n",
       "      <th>case_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500Mg000005sZYzIAM</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000088100</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID SCC_LEGACY_TICKET_ID__C     CASENUMBER case_origin\n",
       "0  500Mg000005sZYzIAM                          TTB0000088100           X"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_legacynonempty_1line.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "df['case_origin']='X'\n",
    "df['SCC_LEGACY_TICKET_ID__C']=''\n",
    "df_cleaned_specific = df.dropna(subset=['ID'])\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\cleaned_prod_1line.csv\",index=False)\n",
    "df_cleaned_specific.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bricare_20200806_20240430_0_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_0_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_11000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9900001_bricare_case_account_person_account.csv\n",
      "Extracted filenames saved to extracted_filenames.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\list_acount.txt\"\n",
    "\n",
    "# Regular expression pattern to match the filenames\n",
    "pattern = re.compile(r'bricare_\\d{8}_\\d{8}_\\d+_bricare_case_account_\\w+\\.csv')\n",
    "\n",
    "# List to store the extracted filenames\n",
    "filenames = []\n",
    "\n",
    "# Read the file and extract filenames\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        matches = pattern.findall(line)\n",
    "        filenames.extend(matches)\n",
    "\n",
    "# Display the extracted filenames\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "\n",
    "# Optionally, save the filenames to a new file\n",
    "output_path = 'extracted_filenames.txt'\n",
    "with open(output_path, 'w') as output_file:\n",
    "    for filename in filenames:\n",
    "        output_file.write(f\"{filename}\\n\")\n",
    "\n",
    "print(f\"Extracted filenames saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected emails: ['john.doe@gmail.com', 'invalid-email.com', 'jane.doe@yahoo.com', 'test@invalid-domain.com', 'hilmi@gmail.com master']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import dns.resolver\n",
    "\n",
    "def is_valid_email(email):\n",
    "    regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    if re.match(regex, email) is None:\n",
    "        return False\n",
    "    \n",
    "    domain = email.split('@')[1]\n",
    "    try:\n",
    "        dns.resolver.resolve(domain, 'MX')\n",
    "    except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN, dns.resolver.Timeout):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def correct_email(email):\n",
    "    corrections = {\n",
    "        'gmial.com': 'gmail.com',\n",
    "        'gamil.com': 'gmail.com',\n",
    "        'yahooo.com': 'yahoo.com',\n",
    "        'hotmial.com': 'hotmail.com',\n",
    "        'hotmal.com': 'hotmail.com',\n",
    "    }\n",
    "    \n",
    "    if '@' not in email:\n",
    "        return email\n",
    "    \n",
    "    local_part, domain = email.split('@')\n",
    "    \n",
    "    # Common typo corrections for domains\n",
    "    if domain in corrections:\n",
    "        domain = corrections[domain]\n",
    "    \n",
    "    # Repeated characters correction (e.g., \"john..doe@gmail.com\")\n",
    "    local_part = re.sub(r'\\.{2,}', '.', local_part)\n",
    "    \n",
    "    corrected_email = f\"{local_part}@{domain}\"\n",
    "    \n",
    "    if is_valid_email(corrected_email):\n",
    "        return corrected_email\n",
    "    else:\n",
    "        return email\n",
    "\n",
    "# Example usage\n",
    "emails = [\n",
    "    \"john..doe@gmial.com\",\n",
    "    \"invalid-email.com\",\n",
    "    \"jane.doe@yahooo.com\",\n",
    "    \"test@invalid-domain.com\",\n",
    "    \"hilmi@gmail.com master\"\n",
    "]\n",
    "\n",
    "corrected_emails = [correct_email(email) for email in emails]\n",
    "print(\"Corrected emails:\", corrected_emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Compare the Queue Name and Excalation Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Compare the Queue Name and Excalation Team\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file_path = '/mnt/data/(FINAL) SLA&OLA_NewUserGrouping_Ringkasan (3).xlsx'\n",
    "excel_data = pd.read_excel(excel_file_path, sheet_name='Appendix Team Name')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = '/mnt/data/extract_Que.csv'\n",
    "extract_que_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "appendix_names = excel_data['Full Name / Queue Name'].dropna().unique()\n",
    "extract_names = extract_que_df['NAME'].dropna().unique()\n",
    "\n",
    "# Find matches\n",
    "matches = {name: get_close_matches(name, extract_names, n=1, cutoff=0.6) for name in appendix_names}\n",
    "\n",
    "# Filter out names without matches\n",
    "matches = {k: v for k, v in matches.items() if v}\n",
    "\n",
    "# Convert matches to a DataFrame for better readability\n",
    "matches_df = pd.DataFrame([(k, v[0]) for k, v in matches.items()], columns=['Appendix Team Name', 'Extract Name'])\n",
    "\n",
    "# Define a function to describe the differences between two strings\n",
    "def describe_difference(str1, str2):\n",
    "    if str1 == str2:\n",
    "        return \"Exact match\"\n",
    "    differences = []\n",
    "    if len(str1) != len(str2):\n",
    "        differences.append(\"Different lengths\")\n",
    "    if str1.strip() != str2.strip():\n",
    "        differences.append(\"Whitespace differences\")\n",
    "    for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "        if char1 != char2:\n",
    "            differences.append(f\"Different character at position {i}: '{char1}' vs '{char2}'\")\n",
    "    return \"; \".join(differences) if differences else \"Other differences\"\n",
    "\n",
    "# Add a column describing the differences\n",
    "matches_df['Difference Details'] = matches_df.apply(lambda row: describe_difference(row['Appendix Team Name'], row['Extract Name']), axis=1)\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Matched Names with Differences\", dataframe=matches_df)\n",
    "\n",
    "matches_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\Matched_Names_with_Differences.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df.to_excel('match_queue_name_vs_escalation_name.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all error logs into 2 different files: Business and Person Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business records saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\account_business_records.csv\n",
      "Person Account records saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\account_person_account_records.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\"\n",
    "\n",
    "\n",
    "all_files = os.listdir(folder_path)\n",
    "\n",
    "\n",
    "error_files = [f for f in all_files if \"error\" in f]\n",
    "\n",
    "\n",
    "business_data = pd.DataFrame()\n",
    "person_account_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "for file_name in error_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    data = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "\n",
    "    data['Nama'] = data['Nama'].apply(lambda x: x[-250:] if isinstance(x, str) else x)\n",
    "    \n",
    "    # combine the columns \"Email\" and \"no telp\"\n",
    "    data['Description'] = data[['No_telp', 'Email']].apply(lambda x: ' | '.join(x.dropna().astype(str)), axis=1)\n",
    "    \n",
    "\n",
    "    data = data.drop(columns=['No_telp', 'Email'])\n",
    "    \n",
    "\n",
    "    business_data = pd.concat([business_data, data[data['record_type'] == 'Business']])\n",
    "    person_account_data = pd.concat([person_account_data, data[data['record_type'] == 'Person Account']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "business_file_path = os.path.join(folder_path, 'account_business_records.csv')\n",
    "person_account_file_path = os.path.join(folder_path, 'account_person_account_records.csv')\n",
    "\n",
    "\n",
    "business_data.to_csv(business_file_path, index=False)\n",
    "person_account_data.to_csv(person_account_file_path, index=False)\n",
    "\n",
    "print(f\"Business records saved to: {business_file_path}\")\n",
    "print(f\"Person Account records saved to: {person_account_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To check and analyze the Account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def detect_delimiter(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        header = file.readline()\n",
    "        for delimiter in [',', ';', '\\t', '|']:\n",
    "            if delimiter in header:\n",
    "                return delimiter\n",
    "    return None\n",
    "\n",
    "def analyze_file(file_path, expected_delimiter):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=expected_delimiter)\n",
    "        line_number = 0\n",
    "        inconsistent_lines = []\n",
    "        for row in reader:\n",
    "            line_number += 1\n",
    "            if len(row) != len(reader.fieldnames):\n",
    "                inconsistent_lines.append(line_number)\n",
    "        return inconsistent_lines\n",
    "\n",
    "def main():\n",
    "    file_paths = [\"path_to_your_first_csv_file.csv\", \"path_to_your_second_csv_file.csv\"]  \n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        detected_delimiter = detect_delimiter(file_path)\n",
    "        if detected_delimiter:\n",
    "            print(f\"Detected delimiter for {file_path}: {detected_delimiter}\")\n",
    "            inconsistent_lines = analyze_file(file_path, detected_delimiter)\n",
    "            if inconsistent_lines:\n",
    "                print(f\"Inconsistent lines in {file_path}: {inconsistent_lines}\")\n",
    "            else:\n",
    "                print(f\"No inconsistencies found in {file_path}\")\n",
    "        else:\n",
    "            print(f\"Could not detect delimiter for {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558030.csv\n",
      "Detected delimiter for C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558030.csv: ;\n",
      "Inconsistent lines in C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558030.csv: [1]\n",
      "Processing file: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558031.csv\n",
      "Detected delimiter for C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558031.csv: ;\n",
      "Inconsistent lines in C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558031.csv: [2, 3]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def detect_delimiter(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        header = file.readline()\n",
    "        for delimiter in [',', ';', '\\t', '|']:\n",
    "            if delimiter in header:\n",
    "                return delimiter\n",
    "    return None\n",
    "\n",
    "def analyze_file(file_path, expected_delimiter):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=expected_delimiter)\n",
    "        line_number = 0\n",
    "        inconsistent_lines = []\n",
    "        field_count = len(next(reader))\n",
    "        for row in reader:\n",
    "            line_number += 1\n",
    "            if len(row) != field_count:\n",
    "                inconsistent_lines.append(line_number)\n",
    "        return inconsistent_lines\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                detected_delimiter = detect_delimiter(file_path)\n",
    "                if detected_delimiter:\n",
    "                    print(f\"Detected delimiter for {file_path}: {detected_delimiter}\")\n",
    "                    inconsistent_lines = analyze_file(file_path, detected_delimiter)\n",
    "                    if inconsistent_lines:\n",
    "                        print(f\"Inconsistent lines in {file_path}: {inconsistent_lines}\")\n",
    "                    else:\n",
    "                        print(f\"No inconsistencies found in {file_path}\")\n",
    "                else:\n",
    "                    print(f\"Could not detect delimiter for {file_path}\")\n",
    "\n",
    "def main():\n",
    "    folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\"\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_folder(folder_path)\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\error071124083558031.csv\"\n",
    "output_file = r\"c:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\error071124083558030_output.csv\"\n",
    "\n",
    "# Read the input CSV file\n",
    "with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile, delimiter=';')\n",
    "    rows = list(reader)\n",
    "\n",
    "# Process the rows to add double quotes where necessary\n",
    "processed_rows = []\n",
    "for row in rows:\n",
    "    if row == rows[0]:\n",
    "        # Keep the header row as is\n",
    "        processed_rows.append(row)\n",
    "    else:\n",
    "        # Add double quotes around non-empty values\n",
    "        processed_row = ['' if value == '\"\"' else f'\"{value}\"' if value else '' for value in row]\n",
    "        processed_rows.append(processed_row)\n",
    "\n",
    "# Write the processed rows to the output CSV file\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=';')\n",
    "    writer.writerows(processed_rows)\n",
    "\n",
    "print(\"CSV file has been processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of CSV files saved to csv_shapes.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_shapes_to_excel(folder_path, output_file):\n",
    "    data = []\n",
    "\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        if filename.endswith('.csv'):\n",
    "     \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "      \n",
    "            df = pd.read_csv(file_path)\n",
    "    \n",
    "            rows, columns = df.shape\n",
    "         \n",
    "            data.append([filename, rows, columns])\n",
    "\n",
    " \n",
    "    df_shapes = pd.DataFrame(data, columns=['Name', 'Rows', 'Columns'])\n",
    "\n",
    "\n",
    "    df_shapes.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Shapes of CSV files saved to {output_file}\")\n",
    "\n",
    "\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\"\n",
    "output_file = 'csv_shapes.xlsx'\n",
    "\n",
    "\n",
    "save_shapes_to_excel(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\BRI - User Matrix v1.2.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Role2')\n",
    "\n",
    "df['']\n",
    "\n",
    "df.to_csv('Role2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ATM BANK LAIN', 'Aplikasi', 'EDC', 'CRM', 'ATM', 'ATM BRI'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "\n",
    "df= pd.read_csv(path)\n",
    "\n",
    "df['Chanel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def format_ticket_id(ticket_id):\n",
    "    if pd.notna(ticket_id) and ticket_id.startswith(\"TTB\"):\n",
    "        number_part = ticket_id[3:]\n",
    "        formatted_number = number_part[-10:].zfill(10)\n",
    "        return f\"TTB{formatted_number}\"\n",
    "    return ticket_id\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if \"Ticket_ID\" in df.columns:\n",
    "            df[\"Ticket_ID\"] = df[\"Ticket_ID\"].apply(format_ticket_id)\n",
    "        if \"Legacy_Ticket_ID\" in df.columns:\n",
    "            df[\"Legacy_Ticket_ID\"] = df[\"Legacy_Ticket_ID\"].apply(format_ticket_id)\n",
    "        output_path = file_path.replace(\".csv\", \"_processed.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "        return df\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_all_files_in_folder(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            process_file(file_path)\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\test_case - Copy\"\n",
    "process_all_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed_processed_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def format_ticket_id(ticket_id):\n",
    "    if ticket_id and ticket_id.startswith(\"TTB\"):\n",
    "        number_part = ticket_id[3:]\n",
    "        formatted_number = number_part[-10:].zfill(10)\n",
    "        return f\"TTB{formatted_number}\"\n",
    "    return ticket_id\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "        \n",
    "\n",
    "        for row in rows:\n",
    "            if \"Ticket_ID\" in row:\n",
    "                row[\"Ticket_ID\"] = format_ticket_id(row[\"Ticket_ID\"])\n",
    "            if \"Legacy_Ticket_ID\" in row:\n",
    "                row[\"Legacy_Ticket_ID\"] = format_ticket_id(row[\"Legacy_Ticket_ID\"])\n",
    "        \n",
    "\n",
    "        output_path = file_path.replace(\".csv\", \"_processed.csv\")\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        \n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_all_files_in_folder(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            process_file(file_path)\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\test_case - Copy\"\n",
    "process_all_files_in_folder(folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>record_type</th>\n",
       "      <th>cif</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Details</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>...</th>\n",
       "      <th>Tanggal_Settlement</th>\n",
       "      <th>Tgl_Foward</th>\n",
       "      <th>Tgl_In_Progress</th>\n",
       "      <th>Tgl_Returned</th>\n",
       "      <th>Ticket_Referensi</th>\n",
       "      <th>Tiket_Urgency</th>\n",
       "      <th>Tipe_Remark</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>users</th>\n",
       "      <th>Usergroup_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET001339022</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>STGD 980</td>\n",
       "      <td>TTB001339022</td>\n",
       "      <td>1000</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>2023-01-01 07:07:15</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case_number     record_type       cif     Ticket_ID  Call_Type_ID  \\\n",
       "0  TICKET001339022  Case Migration  STGD 980  TTB001339022          1000   \n",
       "\n",
       "                              Call_Type  \\\n",
       "0  Blokir Kartu ATM karena kartu hilang   \n",
       "\n",
       "                                             Details          Create_Date  \\\n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  2023-01-01 07:07:15   \n",
       "\n",
       "  gateway Jenis_Laporan  ... Tanggal_Settlement  Tgl_Foward  Tgl_In_Progress  \\\n",
       "0   Phone       Request  ...                NaN         NaN              NaN   \n",
       "\n",
       "  Tgl_Returned  Ticket_Referensi  Tiket_Urgency  Tipe_Remark UniqueID  users  \\\n",
       "0          NaN               NaN            NaN        Notes      NaN   Call   \n",
       "\n",
       "  Usergroup_ID  \n",
       "0            4  \n",
       "\n",
       "[1 rows x 82 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved D:\\Case\\file_part_1.csv\n",
      "Saved D:\\Case\\file_part_2.csv\n",
      "Saved D:\\Case\\file_part_3.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_file(input_file, output_dir, rows_per_file):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Read the large file\n",
    "    data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Calculate the number of smaller files needed\n",
    "    num_files = len(data) // rows_per_file + (1 if len(data) % rows_per_file != 0 else 0)\n",
    "    \n",
    "    for i in range(num_files):\n",
    "        # Determine the start and end row for the current chunk\n",
    "        start_row = i * rows_per_file\n",
    "        end_row = min((i + 1) * rows_per_file, len(data))\n",
    "        \n",
    "        # Slice the data for the current chunk\n",
    "        chunk = data.iloc[start_row:end_row]\n",
    "        \n",
    "        # Determine the output file path\n",
    "        output_file = os.path.join(output_dir, f'file_part_{i+1}.csv')\n",
    "        \n",
    "        # Save the chunk to a CSV file with column headers\n",
    "        chunk.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f'Saved {output_file}')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    # input_file = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_3mil_ttb.csv\"\n",
    "    # output_dir = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\Delete TTB prod_300k\"\n",
    "    input_file = r\"D:\\Case\\duplicate_records_to_delete.csv\"\n",
    "    output_dir = r\"D:\\Case\"\n",
    "    rows_per_file = 2000000\n",
    "    \n",
    "    split_file(input_file, output_dir, rows_per_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_directory  = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "dummy_file = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_dummy\\dummy_data_casenumber_fix_part1.csv\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output\"\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Placeholder data from the dummy file\n",
    "dummy_df = pd.read_csv(dummy_file)\n",
    "dummy_row = dummy_df.iloc[0].to_dict()\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "all_ticket_ids_27 = set()\n",
    "all_ticket_ids_79 = set()\n",
    "csv_files_27 = []\n",
    "csv_files_79 = []\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith('_27.csv'):\n",
    "        csv_files_27.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'])\n",
    "        all_ticket_ids_27.update(df['Ticket_ID'].unique())\n",
    "    elif file.endswith('_79.csv'):\n",
    "        csv_files_79.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'])\n",
    "        all_ticket_ids_79.update(df['Ticket_ID'].unique())\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    all_ticket_ids = sorted(all_ticket_ids)\n",
    "    min_id = int(all_ticket_ids[0][3:]) \n",
    "    max_id = int(all_ticket_ids[-1][3:])  \n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = dummy_row.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = generate_legacy_ticket_id(ticket_id)\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file))\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{file.split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27')\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "input_directory  = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "dummy_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_dummy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Collect all dummy data from the dummy files\n",
    "dummy_files = [os.path.join(dummy_directory, file) for file in os.listdir(dummy_directory) if file.endswith('.csv')]\n",
    "dummy_df = pd.concat([pd.read_csv(file) for file in dummy_files], ignore_index=True)\n",
    "dummy_row = dummy_df.iloc[0].to_dict()\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "all_ticket_ids_27 = set()\n",
    "all_ticket_ids_79 = set()\n",
    "csv_files_27 = []\n",
    "csv_files_79 = []\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith('_27.csv'):\n",
    "        csv_files_27.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'])\n",
    "        all_ticket_ids_27.update(df['Ticket_ID'].unique())\n",
    "    elif file.endswith('_79.csv'):\n",
    "        csv_files_79.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'])\n",
    "        all_ticket_ids_79.update(df['Ticket_ID'].unique())\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    all_ticket_ids = sorted(all_ticket_ids)\n",
    "    min_id = int(all_ticket_ids[0][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    max_id = int(all_ticket_ids[-1][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = dummy_row.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = generate_legacy_ticket_id(ticket_id)\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file))\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{file.split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27')\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle other delimiter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\bricare_20230101_20230101_79.csv\"\n",
    "path2=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\bricare_20230101_20230101_79.csv\"\n",
    "\n",
    "df =pd.read_csv(path)\n",
    "df['record_type']='Case Migration'\n",
    "df.to_csv(path2,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory  = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "dummy_directory = r\"D:\\output\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to read and process dummy files in chunks\n",
    "def read_dummy_files(dummy_directory, delimiter):\n",
    "    dummy_row = None\n",
    "    for file in os.listdir(dummy_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(dummy_directory, file), delimiter=delimiter)\n",
    "            if dummy_row is None:\n",
    "                dummy_row = df.iloc[0].to_dict()\n",
    "    return dummy_row\n",
    "\n",
    "# Collect dummy data from the dummy files\n",
    "dummy_row = read_dummy_files(dummy_directory, delimiter)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "all_ticket_ids_27 = set()\n",
    "all_ticket_ids_79 = set()\n",
    "csv_files_27 = []\n",
    "csv_files_79 = []\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith('_27.csv'):\n",
    "        csv_files_27.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_27.update(df['Ticket_ID'].unique())\n",
    "    elif file.endswith('_79.csv'):\n",
    "        csv_files_79.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_79.update(df['Ticket_ID'].unique())\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    all_ticket_ids = sorted(all_ticket_ids)\n",
    "    min_id = int(all_ticket_ids[0][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    max_id = int(all_ticket_ids[-1][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = dummy_row.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(file).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27')\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: D:\\cleaned\\bricare_20200101_20200101_27_part1.csv\n",
      "Processed and saved: D:\\cleaned\\bricare_20230101_20230101_79_part1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_21452\\2828063160.py:30: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_21452\\2828063160.py:30: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv_files(input_folder, output_folder):\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Remove the 'Legacy_Ticket_ID' column if it exists\n",
    "            if 'Legacy_Ticket_ID' in data.columns:\n",
    "                data = data.drop(columns=['Legacy_Ticket_ID'])\n",
    "            \n",
    "            # Rename the 'Legacy_ticket_id' column to 'Legacy_Ticket_ID' if it exists\n",
    "            if 'Legacy_ticket_id' in data.columns:\n",
    "                data = data.rename(columns={'Legacy_ticket_id': 'Legacy_Ticket_ID'})\n",
    "            \n",
    "            # Replace all NaN values with empty strings\n",
    "            data = data.fillna('')\n",
    "            \n",
    "            # Remove '.0' at the end of all values\n",
    "            data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
    "            \n",
    "          \n",
    "            \n",
    "            # Make 'closed_date' one day after 'create_date'\n",
    "            if 'Create_Date' in data.columns:\n",
    "                data['Create_Date'] = pd.to_datetime(data['Create_Date'], errors='coerce')\n",
    "                data['TanggalClosed'] = data['Create_Date'] + pd.DateOffset(days=1)\n",
    "                data['TanggalClosed'] = data['TanggalClosed'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Save the cleaned data to the output folder\n",
    "            cleaned_file_path = os.path.join(output_folder, filename)\n",
    "            data.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Processed and saved: {cleaned_file_path}\")\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "output_folder = r\"D:\\cleaned\"\n",
    "\n",
    "# Call the function to clean all CSV files\n",
    "clean_csv_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE THIS to cleanse Case Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20200101_20200101_27_part1.csv\n",
      "Reading file: bricare_20230101_20230101_79_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory  = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "dummy_directory = r\"D:\\output\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to read and process dummy files in chunks\n",
    "def read_dummy_files(dummy_directory, delimiter):\n",
    "    dummy_row = None\n",
    "    for file in os.listdir(dummy_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(dummy_directory, file), delimiter=delimiter)\n",
    "            if dummy_row is None:\n",
    "                dummy_row = df.iloc[0].to_dict()\n",
    "    return dummy_row\n",
    "\n",
    "# Collect dummy data from the dummy files\n",
    "dummy_row = read_dummy_files(dummy_directory, delimiter)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "all_ticket_ids_27 = set()\n",
    "all_ticket_ids_79 = set()\n",
    "csv_files_27 = []\n",
    "csv_files_79 = []\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith('_27.csv'):\n",
    "        csv_files_27.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_27.update(df['Ticket_ID'].unique())\n",
    "    elif file.endswith('_79.csv'):\n",
    "        csv_files_79.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_79.update(df['Ticket_ID'].unique())\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    all_ticket_ids = sorted(all_ticket_ids)\n",
    "    min_id = int(all_ticket_ids[0][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    max_id = int(all_ticket_ids[-1][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = dummy_row.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(file).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27')\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79')\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part1.csv\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory  = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "dummy_directory = r\"D:\\output\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to read and process dummy files in chunks\n",
    "def read_dummy_files(dummy_directory, delimiter):\n",
    "    dummy_row = None\n",
    "    for file in os.listdir(dummy_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(dummy_directory, file), delimiter=delimiter)\n",
    "            if dummy_row is None:\n",
    "                dummy_row = df.iloc[0].to_dict()\n",
    "    return dummy_row\n",
    "\n",
    "# Collect dummy data from the dummy files\n",
    "dummy_row = read_dummy_files(dummy_directory, delimiter)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "all_ticket_ids_27 = set()\n",
    "all_ticket_ids_79 = set()\n",
    "csv_files_27 = []\n",
    "csv_files_79 = []\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if '_27' in file and file.endswith('.csv'):\n",
    "        csv_files_27.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_27.update(df['Ticket_ID'].unique())\n",
    "    elif '_79' in file and file.endswith('.csv'):\n",
    "        csv_files_79.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_79.update(df['Ticket_ID'].unique())\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    all_ticket_ids = sorted(all_ticket_ids)\n",
    "    min_id = int(all_ticket_ids[0][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    max_id = int(all_ticket_ids[-1][3:])  # Assuming Ticket_IDs are of the format 'TTBxxxxxxx'\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = dummy_row.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(file).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27')\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79')\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: D:\\cleaned\\bricare_20200101_20200101_27_kosong_part1.csv\n",
      "Processed and saved: D:\\cleaned\\bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_36988\\3404566872.py:30: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_36988\\3404566872.py:30: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv_files(input_folder, output_folder):\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Remove the 'Legacy_Ticket_ID' column if it exists\n",
    "            if 'Legacy_Ticket_ID' in data.columns:\n",
    "                data = data.drop(columns=['Legacy_Ticket_ID'])\n",
    "            \n",
    "            # Rename the 'Legacy_ticket_id' column to 'Legacy_Ticket_ID' if it exists\n",
    "            if 'Legacy_ticket_id' in data.columns:\n",
    "                data = data.rename(columns={'Legacy_ticket_id': 'Legacy_Ticket_ID'})\n",
    "            \n",
    "            # Replace all NaN values with empty strings\n",
    "            data = data.fillna('')\n",
    "            \n",
    "            # Remove '.0' at the end of all values\n",
    "            data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
    "            \n",
    "            \n",
    "            # Save the cleaned data to the output folder\n",
    "            cleaned_file_path = os.path.join(output_folder, filename)\n",
    "            data.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Processed and saved: {cleaned_file_path}\")\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "output_folder = r\"D:\\cleaned\"\n",
    "\n",
    "# Call the function to clean all CSV files\n",
    "clean_csv_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part1.csv\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part1.csv\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part2.csv\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part3.csv\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part4.csv\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6796\\2663722302.py:102: DtypeWarning: Columns (2,4,5,6,11,12,13,15,19,22,25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs from TTB000000000001 to the maximum in the current data\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(12)}' for i in range(1, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3450825"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_PROD24.csv\"\n",
    "# path=r\"D:\\Case\\account and case 21, 22 July\\bricare_case_20_21_22_07_2024\\bricare_20240720_20240722_0_79_closed.csv\"\n",
    "\n",
    "df=pd.read_csv(path, sep=';')\n",
    "unique_df = df.drop_duplicates(subset='SCC_LEGACY_TICKET_ID__C', keep='first')\n",
    "len(unique_df)\n",
    "# duplicates = df[df.duplicated(subset='Ticket_ID', keep=False)]\n",
    "# duplicates_df = df[~df['SCC_LEGACY_TICKET_ID__C'].isin(unique_df['SCC_LEGACY_TICKET_ID__C'])]\n",
    "# duplicates = df[df.duplicated(subset='SCC_LEGACY_TICKET_ID__C', keep=False)]\n",
    "# len(duplicates_df)\n",
    "\n",
    "# unique_ids = set()  # Initialize the set to store unique IDs\n",
    "# unique_rows = []  # List to store unique rows\n",
    "# duplicate_rows = []  # List to store duplicate rows\n",
    "\n",
    "# # Read the CSV file in chunks\n",
    "# chunksize = 100\n",
    "# for chunk in pd.read_csv(path, delimiter=';', chunksize=chunksize):\n",
    "#     for _, row in chunk.iterrows():\n",
    "#         if row['SCC_LEGACY_TICKET_ID__C'] in unique_ids:\n",
    "#             duplicate_rows.append(row)\n",
    "#         else:\n",
    "#             unique_ids.add(row['SCC_LEGACY_TICKET_ID__C'])\n",
    "#             unique_rows.append(row)\n",
    "\n",
    "# # Convert the lists to DataFrames\n",
    "# unique_df = pd.DataFrame(unique_rows)\n",
    "# duplicate_df = pd.DataFrame(duplicate_rows)\n",
    "\n",
    "# # Save the DataFrames to CSV files\n",
    "# unique_df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\unique_records.csv\", index=False, sep=';')\n",
    "# duplicate_df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\duplicate_records.csv\", index=False, sep=';')\n",
    "\n",
    "# # Print the number of unique and duplicate records\n",
    "# print(f\"Number of unique records: {len(unique_df)}\")\n",
    "# print(f\"Number of duplicate records: {len(duplicate_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Records': 8300416,\n",
       " 'Unique Records': 3477315,\n",
       " 'Duplicate Records': 4823101}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Case\\extractPROD - Copy.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Identify duplicates based on 'SCC_LEGACY_TICKET_ID__C'\n",
    "duplicate_rows = df[df.duplicated(['SCC_LEGACY_TICKET_ID__C'], keep=False)]\n",
    "\n",
    "# Count unique and duplicate records\n",
    "total_records = len(df)\n",
    "unique_records = total_records - len(duplicate_rows) + len(duplicate_rows['SCC_LEGACY_TICKET_ID__C'].drop_duplicates())\n",
    "duplicate_count = len(duplicate_rows) - len(duplicate_rows['SCC_LEGACY_TICKET_ID__C'].drop_duplicates())\n",
    "\n",
    "\n",
    "duplicate_info = {\n",
    "    \"Total Records\": total_records,\n",
    "    \"Unique Records\": unique_records,\n",
    "    \"Duplicate Records\": duplicate_count\n",
    "}\n",
    "\n",
    "duplicate_rows.to_csv('duplicate_records.csv', index=False)\n",
    "\n",
    "duplicate_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Records': 8300416,\n",
       " 'Unique Records': 3477315,\n",
       " 'Duplicate Records': 4823101}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Case\\extractPROD - Copy.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Identify duplicates based on 'SCC_LEGACY_TICKET_ID__C'\n",
    "duplicate_mask = df.duplicated(['SCC_LEGACY_TICKET_ID__C'], keep=False)\n",
    "duplicate_rows = df[duplicate_mask]\n",
    "\n",
    "# Drop the first occurrence of each duplicated 'SCC_LEGACY_TICKET_ID__C' to keep one unique record\n",
    "# This will leave only the duplicates that need to be deleted\n",
    "duplicates_to_delete = duplicate_rows[duplicate_rows.duplicated(['SCC_LEGACY_TICKET_ID__C'], keep='first')]\n",
    "\n",
    "# Save the records to delete (including IDs) to CSV\n",
    "duplicates_to_delete.to_csv('duplicate_records_to_delete.csv', index=False)\n",
    "\n",
    "# Count unique and duplicate records for reference\n",
    "total_records = len(df)\n",
    "unique_records = len(df.drop_duplicates(subset=['SCC_LEGACY_TICKET_ID__C']))\n",
    "duplicate_count = total_records - unique_records\n",
    "\n",
    "# Prepare duplicate information\n",
    "duplicate_info = {\n",
    "    \"Total Records\": total_records,\n",
    "    \"Unique Records\": unique_records,\n",
    "    \"Duplicate Records\": duplicate_count\n",
    "}\n",
    "\n",
    "duplicate_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Records': 8300416,\n",
       " 'Unique Records': 3477315,\n",
       " 'Duplicate Records': 4823101}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Case\\extractPROD.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Identify duplicates based on 'SCC_LEGACY_TICKET_ID__C'\n",
    "duplicate_mask = df.duplicated(['SCC_LEGACY_TICKET_ID__C'], keep=False)\n",
    "duplicate_rows = df[duplicate_mask]\n",
    "\n",
    "# Drop the first occurrence of each duplicated 'SCC_LEGACY_TICKET_ID__C' to keep one unique record\n",
    "# This will leave only the duplicates that need to be deleted\n",
    "duplicates_to_delete = duplicate_rows[duplicate_rows.duplicated(['SCC_LEGACY_TICKET_ID__C'], keep='first')]\n",
    "\n",
    "# Save the records to delete (including IDs) to CSV\n",
    "duplicates_to_delete.to_csv('duplicate_records_to_delete2.csv', index=False)\n",
    "\n",
    "# Count unique and duplicate records for reference\n",
    "total_records = len(df)\n",
    "unique_records = len(df.drop_duplicates(subset=['SCC_LEGACY_TICKET_ID__C']))\n",
    "duplicate_count = total_records - unique_records\n",
    "\n",
    "# Prepare duplicate information\n",
    "duplicate_info = {\n",
    "    \"Total Records\": total_records,\n",
    "    \"Unique Records\": unique_records,\n",
    "    \"Duplicate Records\": duplicate_count\n",
    "}\n",
    "\n",
    "duplicate_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          False\n",
       "1          False\n",
       "2          False\n",
       "3          False\n",
       "4          False\n",
       "           ...  \n",
       "3477312    False\n",
       "3477313    False\n",
       "3477314    False\n",
       "3477315     True\n",
       "3477316     True\n",
       "Length: 3477317, dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_prod_Un.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "df.duplicated(['SCC_LEGACY_TICKET_ID__C'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (8,11,17,23,30,33,36,47,49,53,56,61,63,66,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (9,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (9,15,17,31,33,45,46,47,48,50,51,53,59,62,63,75,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,33,47,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,36,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,36,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,33,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,31,46,47,51,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,36,47,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,47,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,47,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (47,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,31,47,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,33,47,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15,47,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_46792\\4080367576.py:20: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Ticket_ID\n",
      "0        TTB000050004867\n",
      "1        TTB000049908332\n",
      "2        TTB000049929970\n",
      "3        TTB000049777973\n",
      "4        TTB000049965755\n",
      "...                  ...\n",
      "6651718  TTB000050741769\n",
      "6651719  TTB000050971441\n",
      "6651720  TTB000050776910\n",
      "6651721  TTB000050974895\n",
      "6651722  TTB000050874373\n",
      "\n",
      "[6651723 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Specify the path to your folder containing the CSV files\n",
    "folder_path = r\"D:\\Case\\case_2024\"\n",
    "\n",
    "# Ensure the folder path is correctly formatted\n",
    "if not folder_path.endswith('/'):\n",
    "    folder_path += '/'\n",
    "\n",
    "# Use glob to find all CSV files in the specified folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# List to hold dataframes\n",
    "df_list = []\n",
    "\n",
    "# Loop through the CSV files and read each one into a dataframe\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    if 'Ticket_ID' in df.columns:\n",
    "        df_list.append(df[['Ticket_ID']])  # Keep only the Ticket_ID column\n",
    "\n",
    "# Concatenate all dataframes in the list into a single dataframe\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(combined_df)\n",
    "\n",
    "# Optionally, save the combined dataframe to a new CSV file\n",
    "output_path = os.path.join(folder_path, 'combined_ticket_ids.csv')\n",
    "combined_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000050004867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000049908332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000049929970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000049777973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000049965755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451718</th>\n",
       "      <td>TTB000050684721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451719</th>\n",
       "      <td>TTB000050539111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451720</th>\n",
       "      <td>TTB000050585890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451721</th>\n",
       "      <td>TTB000050615438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451722</th>\n",
       "      <td>TTB000050717726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6134520 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Ticket_ID\n",
       "0        TTB000050004867\n",
       "1        TTB000049908332\n",
       "2        TTB000049929970\n",
       "3        TTB000049777973\n",
       "4        TTB000049965755\n",
       "...                  ...\n",
       "6451718  TTB000050684721\n",
       "6451719  TTB000050539111\n",
       "6451720  TTB000050585890\n",
       "6451721  TTB000050615438\n",
       "6451722  TTB000050717726\n",
       "\n",
       "[6134520 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path=r\"D:\\Case\\case_2024\\combined_ticket_ids.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "# df['Ticket_ID'].unique()\n",
    "\n",
    "# duplicates = df[df['Ticket_ID'].duplicated(keep=False)]\n",
    "df['Ticket_ID'].unique()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-517203\n"
     ]
    }
   ],
   "source": [
    "print(6134520-6651723)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_47088\\949659275.py:5: DtypeWarning: Columns (22,28,29,30,32,34,35,36,37,38,40,41,42,46,47,49,51,52,53,54,55,56,57,58,59,61,63,64,66,67,68,71,72,73,74,75,76,77,78,80) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "path=r\"D:\\Case\\account and case 21, 22 July\\bricare_case_20_21_22_07_2024\\Casedata2122.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df['Call_Type_ID'] = pd.to_numeric(df['Call_Type_ID'], errors='coerce').fillna(0).replace(np.inf, 0).astype(int).astype(str)\n",
    "df.to_csv(r\"D:\\Case\\account and case 21, 22 July\\bricare_case_20_21_22_07_2024\\Casedata2122_cleaned.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To update column details by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_49068\\790842565.py:11: DtypeWarning: Columns (22,28,29,30,32,34,35,36,37,38,40,41,42,46,47,49,51,52,53,54,55,56,57,58,59,61,63,64,66,67,68,71,72,73,74,75,76,77,78,80) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_2_df = pd.read_csv(file_2_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cifno        Ticket_ID  Call_Type_ID  \\\n",
      "0   NaN  TTB000054076437          8442   \n",
      "1   NaN  TTB000054076437          8442   \n",
      "2   NaN  TTB000054076437          8442   \n",
      "3   NaN  TTB000054076437          8442   \n",
      "4   NaN  TTB000054076437          8442   \n",
      "\n",
      "                                           Call_Type          Create_Date  \\\n",
      "0  Nasabah ATM Prima gagal transfer & terdebet ke...  2024-07-20 00:00:17   \n",
      "1  Nasabah ATM Prima gagal transfer & terdebet ke...  2024-07-20 00:00:17   \n",
      "2  Nasabah ATM Prima gagal transfer & terdebet ke...  2024-07-20 00:00:17   \n",
      "3  Nasabah ATM Prima gagal transfer & terdebet ke...  2024-07-20 00:00:17   \n",
      "4  Nasabah ATM Prima gagal transfer & terdebet ke...  2024-07-20 00:00:17   \n",
      "\n",
      "  gateway            Jenis_Laporan Nama_Nasabah No_Rekening  Nominal  ...  \\\n",
      "0   Phone  Complaint - Transaction          EKI         NaN      0.0  ...   \n",
      "1   Phone  Complaint - Transaction          EKI         NaN      0.0  ...   \n",
      "2   Phone  Complaint - Transaction          EKI         NaN      0.0  ...   \n",
      "3   Phone  Complaint - Transaction          EKI         NaN      0.0  ...   \n",
      "4   Phone  Complaint - Transaction          EKI         NaN      0.0  ...   \n",
      "\n",
      "  Tgl_Returned Ticket_Referensi Tiket_Urgency Tipe_Remark UniqueID  users  \\\n",
      "0          NaN              NaN           NaN         NaN      NaN    NaN   \n",
      "1          NaN              NaN           NaN         NaN      NaN    NaN   \n",
      "2          NaN              NaN           NaN         NaN      NaN    NaN   \n",
      "3          NaN              NaN           NaN         NaN      NaN    NaN   \n",
      "4          NaN              NaN           NaN         NaN      NaN    NaN   \n",
      "\n",
      "  Usergroup_ID     record_type Legacy_Ticket_ID                  ID  \n",
      "0          NaN  Case Migration    TTB0054076437  500Mg00000ExdHJIAZ  \n",
      "1          NaN  Case Migration    TTB0054076437  500Mg00000EyVZKIA3  \n",
      "2          NaN  Case Migration    TTB0054076437  500Mg00000Ez9DOIAZ  \n",
      "3          NaN  Case Migration    TTB0054076437  500Mg00000F0uoLIAR  \n",
      "4          NaN  Case Migration    TTB0054076437  500Mg00000FAABxIAP  \n",
      "\n",
      "[5 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded files\n",
    "file_1_path = r\"D:\\Case\\extractPROD - Copy.csv\"\n",
    "file_2_path = r\"D:\\Case\\account and case 21, 22 July\\bricare_case_20_21_22_07_2024\\Casedata2122_cleaned.csv\"\n",
    "\n",
    "# Read the first CSV file with semicolon separator\n",
    "file_1_df = pd.read_csv(file_1_path, delimiter=';')\n",
    "\n",
    "# Read the second CSV file with default comma separator\n",
    "file_2_df = pd.read_csv(file_2_path)\n",
    "\n",
    "# Merge the two DataFrames on the 'Ticket_ID' column\n",
    "merged_df = pd.merge(file_2_df, file_1_df[['ID', 'Ticket_ID']], on='Ticket_ID', how='left')\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(merged_df.head())\n",
    "\n",
    "# If you want to save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check whether all tickets exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Status Priority  Call Type     record_type Ticket_ID\n",
      "1  Working      Low       1000  Case Migration   TTB1234\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the uploaded files\n",
    "file_1_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_ales7.csv\"\n",
    "file_2_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\dummy_data_casenumber_fix_part1.csv\"\n",
    "\n",
    "# Read the first CSV file with semicolon separator\n",
    "file_1_df = pd.read_csv(file_1_path, delimiter=';')\n",
    "\n",
    "# Read the second CSV file with default comma separator\n",
    "file_2_df = pd.read_csv(file_2_path)\n",
    "\n",
    "# Check if all values in Ticket_ID of file 2 exist in file 1\n",
    "ticket_id_missing = file_2_df[~file_2_df['Ticket_ID'].isin(file_1_df['Ticket_ID'])]\n",
    "\n",
    "# Save the missing Ticket_IDs to a new CSV file\n",
    "missing_file_path = 'missing_ticket_ids.csv'\n",
    "ticket_id_missing.to_csv(missing_file_path, index=False)\n",
    "\n",
    "\n",
    "# Display the path to the saved file\n",
    "print(ticket_id_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,36,37,47,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,31,36,37,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (36,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,17,31,36,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,31,36,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (9,15,22,25,31,33,36,37,40,45,46,47,50,51,53,62,63,75,77,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,25,31,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,17,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,17,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,36,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (15,25,31,37,46,47,51,63,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (25,31,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (8,15,36,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\4098341139.py:18: DtypeWarning: Columns (31,36,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files have been combined into D:\\Case\\case_2021\\combined_csv_file.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the folder containing the CSV files\n",
    "folder_path = r\"D:\\Case\\case_2021\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the CSV files and read each one into a dataframe\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        # Try reading the CSV file with utf-8 encoding\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # If utf-8 fails, try reading the CSV file with iso-8859-1 encoding\n",
    "            df = pd.read_csv(file_path, encoding='iso-8859-1')\n",
    "        except UnicodeDecodeError:\n",
    "            # If iso-8859-1 fails, try reading the CSV file with latin1 encoding\n",
    "            df = pd.read_csv(file_path, encoding='latin1')\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Specify the output file path\n",
    "output_file = r'D:\\Case\\case_2021\\combined_csv_file.csv'\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'All CSV files have been combined into {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,36,37,47,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (36,37,47,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (36,37,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,31,36,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (36,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (36,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,31,36,46,47,51,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,17,36,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,31,36,46,47,51,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,36,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,25,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,31,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (31,36,37,47,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (9,15,22,25,31,33,36,37,40,45,46,50,51,53,62,63,75,77,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,25,37,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,17,25,31,37,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,17,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,31,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,31,37,47,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,25,31,37,46,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,17,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,31,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,31,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,17,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,36,37,47,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,36,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (17,25,31,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,31,37,46,47,51,53,63,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,25,46,47,51,63,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (15,36,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (8,36,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (31,36,37,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest Ticket_ID is TTB000007861773 with value 7861773\n",
      "The biggest Ticket_ID is TTB000038245877 with value 38245877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_41916\\3751085341.py:11: DtypeWarning: Columns (36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"D:\\Case\\case_2021\\combined_csv_file.csv\"\n",
    "chunk_size = 100000  # Adjust the chunk size as needed\n",
    "\n",
    "smallest_value = float('inf')\n",
    "biggest_value = float('-inf')\n",
    "smallest_ticket_id = None\n",
    "biggest_ticket_id = None\n",
    "\n",
    "for chunk in pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', chunksize=chunk_size):\n",
    "    chunk['Ticket_ID'] = chunk['Ticket_ID'].astype(str)\n",
    "    chunk['Numeric_ID'] = chunk['Ticket_ID'].str.extract(r'TTB(\\d+)').astype(int)\n",
    "    \n",
    "    chunk_min_value = chunk['Numeric_ID'].min()\n",
    "    chunk_max_value = chunk['Numeric_ID'].max()\n",
    "    \n",
    "    if chunk_min_value < smallest_value:\n",
    "        smallest_value = chunk_min_value\n",
    "        smallest_ticket_id = chunk[chunk['Numeric_ID'] == chunk_min_value]['Ticket_ID'].iloc[0]\n",
    "    \n",
    "    if chunk_max_value > biggest_value:\n",
    "        biggest_value = chunk_max_value\n",
    "        biggest_ticket_id = chunk[chunk['Numeric_ID'] == chunk_max_value]['Ticket_ID'].iloc[0]\n",
    "\n",
    "print(f\"The smallest Ticket_ID is {smallest_ticket_id} with value {smallest_value}\")\n",
    "print(f\"The biggest Ticket_ID is {biggest_ticket_id} with value {biggest_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_42968\\2598159650.py:8: DtypeWarning: Columns (8,9,15,17,22,25,31,33,36,37,40,45,46,47,50,51,53,62,63,75,77,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', parse_dates=['Create_Date'])\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.52 GiB for an array with shape (70, 4832391) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Try to load the data with different parameters to handle possible issues\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCreate_Date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pd\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mParserError:\n\u001b[0;32m     10\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreate_Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2144\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2142\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[1;32m-> 2144\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2294\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2287\u001b[0m new_values: ArrayLike\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   2290\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[0;32m   2292\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[1;32m-> 2294\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2296\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.52 GiB for an array with shape (70, 4832391) and data type object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"D:\\Case\\case_2021\\combined_csv_file.csv\"\n",
    "\n",
    "# Try to load the data with different parameters to handle possible issues\n",
    "try:\n",
    "    df = pd.read_csv(file_path, delimiter=',', encoding='utf-8', on_bad_lines='skip', parse_dates=['Create_Date'])\n",
    "except pd.errors.ParserError:\n",
    "    df = pd.read_csv(file_path, delimiter=',', encoding='latin1', on_bad_lines='skip', parse_dates=['Create_Date'])\n",
    "\n",
    "# Ensure the 'Create_Date' column is parsed as datetime\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaT (Not a Time) values in 'Create_Date' if any\n",
    "df = df.dropna(subset=['Create_Date'])\n",
    "\n",
    "# Find the smallest and biggest dates\n",
    "smallest_date = df['Create_Date'].min()\n",
    "biggest_date = df['Create_Date'].max()\n",
    "\n",
    "print(f\"The smallest Create_Date is {smallest_date}\")\n",
    "print(f\"The biggest Create_Date is {biggest_date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if all values in small_ids are in large_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_15496\\2423444586.py:8: DtypeWarning: Columns (9,10,12,16,18,19,23,24,31,32,34,37,46,47,48,49,50,51,52,54,60,62,63,64,67,76,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1_path)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 937. MiB for an array with shape (73, 1682857) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m file2_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmaste\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdataloader_v60.0.2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mextract_133k_case_2024.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV files\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file2_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check if all values in SCC_LEGACY_TICKET_ID__C from file2 are in file1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2144\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2142\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[1;32m-> 2144\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2298\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2300\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2301\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2302\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2304\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 937. MiB for an array with shape (73, 1682857) and data type object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file1_path = r\"D:\\Python2\\case_2024\\M--\\Case_2024_Clean_1\\combined_csv_file - Copy.csv\"\n",
    "file2_path = r\"C:\\\\Users\\\\maste\\\\Downloads\\\\dataloader_v60.0.2\\\\extract_133k_case_2024.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path, delimiter=';')\n",
    "\n",
    "# Check if all values in SCC_LEGACY_TICKET_ID__C from file2 are in file1\n",
    "all_present = df2['SCC_LEGACY_TICKET_ID__C'].isin(df1['SCC_LEGACY_TICKET_ID__C'])\n",
    "\n",
    "# Print results\n",
    "if all(all_present):\n",
    "    print(\"All values in SCC_LEGACY_TICKET_ID__C from file2 are present in file1.\")\n",
    "else:\n",
    "    missing_values = df2[~all_present]\n",
    "    print(\"The following values in SCC_LEGACY_TICKET_ID__C from file2 are not present in file1:\")\n",
    "    print(missing_values['SCC_LEGACY_TICKET_ID__C'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (34,48,54,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,47,52,54,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (9,12,18,19,23,24,31,34,37,47,48,50,52,54,62,64,67,76,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (32,47,48,52,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (10,16,32,34,37,46,47,48,49,51,52,54,60,63,64,76,80) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,18,37,48,54,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,18,34,37,48,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,18,47,48,52,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,37,48,54,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,47,48,52,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,32,47,52,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,37,47,48,52,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,18,34,37,47,48,52,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,34,37,48,54,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,18,34,37,47,48,52,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,37,47,48,52,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\1491224758.py:19: DtypeWarning: Columns (16,37,48,54,60,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The missing values have been saved to D:\\Python2\\case_2024\\M--\\Case_2024_Clean_1\\missing_values.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file1_path = r\"D:\\Python2\\case_2024\\M--\\Case_2024_Clean_1\\combined_csv_file - Copy.csv\"\n",
    "file2_path = r\"D:\\dataquality2\\Prod_133k_legacyid.csv\"\n",
    "output_path = r\"D:\\Python2\\case_2024\\M--\\Case_2024_Clean_1\\missing_values.csv\"\n",
    "\n",
    "# Read file2 in one go as it's the smaller file\n",
    "df2 = pd.read_csv(file2_path, delimiter=';')\n",
    "\n",
    "# Create a set of SCC_LEGACY_TICKET_ID__C values from file2\n",
    "file2_ids = set(df2['SCC_LEGACY_TICKET_ID__C'])\n",
    "\n",
    "# Initialize an empty set to collect matched IDs\n",
    "matched_ids = set()\n",
    "\n",
    "# Read file1 in chunks with correct delimiter\n",
    "chunksize = 10**5\n",
    "for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
    "    matched_ids.update(file2_ids.intersection(chunk['SCC_LEGACY_TICKET_ID__C']))\n",
    "\n",
    "# Determine missing values\n",
    "missing_values = file2_ids - matched_ids\n",
    "\n",
    "# Save missing values to CSV\n",
    "if not missing_values:\n",
    "    print(\"All values in SCC_LEGACY_TICKET_ID__C from file2 are present in file1.\")\n",
    "else:\n",
    "    missing_df = pd.DataFrame(list(missing_values), columns=['SCC_LEGACY_TICKET_ID__C'])\n",
    "    missing_df.to_csv(output_path, index=False)\n",
    "    print(f\"The missing values have been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file1_path = r\"D:\\Python2\\case_2024\\M--\\Case_2024_Clean_1\\combined_csv_file - Copy.csv\"\n",
    "file2_path = r\"D:\\dataquality2\\Prod_133k_legacyid.csv\"\n",
    "output_path = r\"D:\\Python2\\case_2024\\M--\\Case_2024_Clean_1\\missing_values.csv\"\n",
    "\n",
    "# Read file2 in one go as it's the smaller file\n",
    "df2 = pd.read_csv(file2_path, delimiter=';')\n",
    "\n",
    "# Create a set of SCC_LEGACY_TICKET_ID__C values from file2\n",
    "file2_ids = set(df2['SCC_LEGACY_TICKET_ID__C'])\n",
    "\n",
    "# Initialize an empty set to collect matched IDs\n",
    "matched_ids = set()\n",
    "\n",
    "# Read file1 in chunks with correct delimiter\n",
    "chunksize = 10**5\n",
    "for chunk in pd.read_csv(file1_path, chunksize=chunksize):\n",
    "    matched_ids.update(file2_ids.intersection(chunk['SCC_LEGACY_TICKET_ID__C']))\n",
    "\n",
    "# Determine missing values\n",
    "missing_values = file2_ids - matched_ids\n",
    "\n",
    "# Save missing values to CSV\n",
    "if not missing_values:\n",
    "    print(\"All values in SCC_LEGACY_TICKET_ID__C from file2 are present in file1.\")\n",
    "else:\n",
    "    missing_df = pd.DataFrame(list(missing_values), columns=['SCC_LEGACY_TICKET_ID__C'])\n",
    "    missing_df.to_csv(output_path, index=False)\n",
    "    print(f\"The missing values have been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"D:\\\\Python2\\\\case_2023\\\\extract_2023_19_line_toclose.csv\"\n",
    "df2 = pd.read_csv(path, delimiter=';')\n",
    "# try:\n",
    "#     df2 = pd.read_csv(path, delimiter=';')\n",
    "#     print(df2)\n",
    "# except pd.errors.ParserError as e:\n",
    "#     print(\"ParserError:\", e)\n",
    "\n",
    "# # Alternatively, if you want to inspect the file manually\n",
    "# with open(path, 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "#     for i, line in enumerate(lines):\n",
    "#         print(f\"Line {i+1}: {line}\")\n",
    "df2['STATUS']='Closed'\n",
    "df2.to_csv('2023_19Lines.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28596\\3873219863.py:4: DtypeWarning: Columns (9,10,12,16,18,19,23,24,31,32,34,37,46,47,48,49,50,51,52,54,60,62,63,64,67,76,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cifno', 'SCC_LEGACY_TICKET_ID__C', 'Call_Type_ID',\n",
       "       'Call_Type', 'Create_Date', 'gateway', 'Jenis_Laporan', 'Nama_Nasabah',\n",
       "       'No_Rekening', 'Nominal', 'status', 'TanggalClosed', 'tanggalTransaksi',\n",
       "       'Chanel', 'Fitur', 'Nomor_Kartu', 'user_group', 'assgined_to',\n",
       "       'attachment_done', 'email', 'full_name', 'no_telepon', 'approver_login',\n",
       "       'approver_name', 'SLAResolution', 'submitter_login_id',\n",
       "       'submitter_user_group', 'user_login_name', 'Details', 'Jenis_Produk',\n",
       "       'Last_Modified_By', 'Merchant_ID', 'Modified_Date', 'NOTAS', 'Produk',\n",
       "       'SLA_Status', 'TID', 'tanggalAttachmentDone', 'TglAssigned',\n",
       "       'Tgl_Eskalasi', 'AnalisaSkils', 'Attachment_', 'Bank_BRI',\n",
       "       'Biaya_Admin', 'Suku_Bunga', 'Bunga', 'Butuh_Attachment', 'Cicilan',\n",
       "       'Hasil_Kunjungan', 'Log_Name', 'MMS_Ticket_Id',\n",
       "       'Mass_Ticket_Upload_Flag', 'Nama_Supervisor', 'Nama_TL', 'Nama_Wakabag',\n",
       "       'Nasabah_Prioritas', 'Notify_By', 'Organization', 'Output_Settlement',\n",
       "       'phone_survey', 'Return_Ticket', 'Settlement_By', 'Settlement_ID',\n",
       "       'Settlement', 'Site_User', 'Status_Return', 'Status_Transaksi',\n",
       "       'Submitter_Region', 'Submitter_SiteGroup', 'Submitter_User_group_ID',\n",
       "       'Tanggal_Settlement', 'Tgl_Foward', 'Tgl_In_Progress', 'Tgl_Returned',\n",
       "       'Ticket_Referensi', 'Tiket_Urgency', 'Tipe_Remark', 'UniqueID', 'users',\n",
       "       'Usergroup_ID', 'record_type', 'Legacy_Ticket_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1_path = r\"D:\\Python2\\case_2024\\M--\\Case_2024_Clean_1\\combined_csv_file - Copy.csv\"\n",
    "df1 = pd.read_csv(file1_path)\n",
    "\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To filter the combined file based on Ticket ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,22,31,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,47,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,46,47,51,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,46,47,51,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,46,47,51,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,46,47,51,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,47,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,46,47,51,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,47,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,46,47,51,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,46,47,51,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,46,47,51,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,17,31,46,47,51,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,47,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,47,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (15,31,46,47,51,63,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_39000\\1121489279.py:18: DtypeWarning: Columns (8,11,17,21,22,23,39,56,58,61,66,73,74,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the primary file\n",
    "primary_file = pd.read_csv(r\"D:\\Python2\\case_2023\\2023_19Lines.csv\")\n",
    "\n",
    "# Directory containing the other files\n",
    "directory = r\"D:\\Python2\\case_2023\\Case_2023_Clean_2\"\n",
    "\n",
    "# Initialize an empty list to hold the filtered DataFrames\n",
    "filtered_dfs = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read each file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Filter based on Ticket_ID values\n",
    "        filtered_df = df[df['Ticket_ID'].isin(primary_file['Ticket_ID'])]\n",
    "        \n",
    "        # Append the filtered DataFrame to the list\n",
    "        filtered_dfs.append(filtered_df)\n",
    "\n",
    "# Concatenate all filtered DataFrames\n",
    "\n",
    "all_filtered_data = pd.concat(filtered_dfs, ignore_index=True)\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "all_filtered_data.to_csv(r'D:\\Python2\\case_2023\\filtered_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python2\\\\case_2024\\\\133k_2024\\\\error073124121623093_truncated.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "path = r\"D:\\Python2\\case_2024\\133k_2024\\error073124121623093.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Ensure the column exists\n",
    "if 'PHONE_SURVEY' in df.columns:\n",
    "    # Truncate the values in the 'PHONE_SURVEY' column to a maximum of 40 characters\n",
    "    df['PHONE_SURVEY'] = df['PHONE_SURVEY'].str.slice(0, 40)\n",
    "\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "output_path = r\"D:\\Python2\\case_2024\\133k_2024\\error073124121623093_truncated.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been split into 'file_Person Account.csv' and 'file_Business.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Fill the empty values in column names with No name\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"D:\\Python2\\case_2024\\5-6 aug\\bricare_20240805_20240806_0_case_account.csv\", delimiter=';')\n",
    "df['Nama'] = df['Nama'].replace(['', '-'], 'No Name')\n",
    "df['Nama'] = df['Nama'].apply(lambda x: x[:250] if len(x) > 250 else x)\n",
    "# Check for duplicate 'cifno'\n",
    "# duplicate_cifno = df.duplicated('cifno')\n",
    "# if duplicate_cifno.any():\n",
    "#     print(\"There are duplicate 'cifno' values.\")\n",
    "# else:\n",
    "#     print(\"All 'cifno' values are unique.\")\n",
    "\n",
    "# Fill empty 'Nama' fields with \"No Name\"\n",
    "# df['Nama'].fillna('No Name', inplace=True)\n",
    "# df['Nama'].replace('', 'No Name', inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "# df.to_csv('updated_yourfile.csv', index=False, sep=';')\n",
    "record_type_values = df['record_type'].unique()\n",
    "\n",
    "# Ensure there are only two unique values in the record_type column\n",
    "if len(record_type_values) != 2:\n",
    "    raise ValueError(\"The 'record_type' column should have exactly two unique values.\")\n",
    "\n",
    "# Create two separate DataFrames based on the unique values\n",
    "df1 = df[df['record_type'] == record_type_values[0]]\n",
    "df2 = df[df['record_type'] == record_type_values[1]]\n",
    "\n",
    "# Save the DataFrames to separate CSV files\n",
    "df1.to_csv(f'file_{record_type_values[0]}.csv', index=False)\n",
    "df2.to_csv(f'file_{record_type_values[1]}.csv', index=False)\n",
    "\n",
    "print(f\"File has been split into 'file_{record_type_values[0]}.csv' and 'file_{record_type_values[1]}.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the USER GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed and updated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the master user group data file\n",
    "master_user_group_path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx - branch_unit.csv\"\n",
    "\n",
    "# Load the master user group data\n",
    "master_user_group_df = pd.read_csv(master_user_group_path)\n",
    "\n",
    "# Create a mapping from Branch Code without leading zeros to Name in the master data\n",
    "master_user_group_df['Branch Code'] = master_user_group_df['Branch Code'].astype(str).str.lstrip('0')\n",
    "user_group_mapping = dict(zip(master_user_group_df['Branch Code'], master_user_group_df['Name']))\n",
    "\n",
    "# Folder containing the CSV files to be updated\n",
    "input_folder = r\"D:\\Python2\\case_2024\\5-6 aug\\caseowner\\cabang\"\n",
    "output_folder = r\"D:\\Python2\\case_2024\\5-6 aug\\caseowner\\cabang\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to update the user_group column in a given dataframe\n",
    "def update_user_group(df):\n",
    "    def map_user_group(value):\n",
    "        numeric_part = ''.join(filter(str.isdigit, str(value))).lstrip('0')\n",
    "        return user_group_mapping.get(numeric_part, value) if numeric_part else value\n",
    "    \n",
    "    df['user_group'] = df['user_group'].apply(map_user_group)\n",
    "    return df\n",
    "\n",
    "# Process each CSV file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "        output_file_path = os.path.join(output_folder, f'Updated_{filename}')\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(input_file_path, delimiter=';', on_bad_lines='skip')\n",
    "        \n",
    "        # Update the user_group column\n",
    "        updated_df = update_user_group(df)\n",
    "        \n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        updated_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print('All files have been processed and updated.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change user group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Python2\\case_2024\\change_owner\\Prod\\batch6_working\\New folder\\extract - Cabang.csv has been processed and updated.\n",
      "All files have been processed and updated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_27896\\1192719588.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['user_group'] = df['user_group'].apply(map_user_group)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the master user group data file\n",
    "master_user_group_path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx - branch_unit.csv\"\n",
    "\n",
    "# Load the master user group data\n",
    "master_user_group_df = pd.read_csv(master_user_group_path)\n",
    "\n",
    "# Create a mapping from Branch Code without leading zeros to Name in the master data\n",
    "master_user_group_df['Branch Code'] = master_user_group_df['Branch Code'].astype(str).str.lstrip('0')\n",
    "user_group_mapping = dict(zip(master_user_group_df['Branch Code'], master_user_group_df['Name']))\n",
    "\n",
    "# Folder containing the CSV files to be updated\n",
    "input_folder = r\"D:\\Python2\\case_2024\\change_owner\\Prod\\batch6_working\\New folder\"\n",
    "output_folder = r\"D:\\Python2\\case_2024\\change_owner\\Prod\\batch6_working\\New folder\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to update the user_group column in a given dataframe\n",
    "def update_user_group(df):\n",
    "    # Filter rows where user_group values start with numbers\n",
    "    df = df[df['user_group'].astype(str).str.match(r'^\\d')]\n",
    "    \n",
    "    # Mapping logic\n",
    "    def map_user_group(value):\n",
    "        numeric_part = ''.join(filter(str.isdigit, str(value))).lstrip('0')\n",
    "        return user_group_mapping.get(numeric_part, value) if numeric_part else value\n",
    "    \n",
    "    df['user_group'] = df['user_group'].apply(map_user_group)\n",
    "    return df\n",
    "\n",
    "# Function to process a single CSV file\n",
    "def process_csv(input_file_path, output_file_path):\n",
    "    try:\n",
    "        # Read the entire CSV file with error handling\n",
    "        df = pd.read_csv(input_file_path, delimiter=';', on_bad_lines='skip', encoding='utf-8')\n",
    "        \n",
    "        # Update the user_group column\n",
    "        updated_df = update_user_group(df)\n",
    "        \n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        updated_df.to_csv(output_file_path, index=False)\n",
    "        print(f'{input_file_path} has been processed and updated.')\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f'Error processing {input_file_path}: {e}')\n",
    "\n",
    "# Process each CSV file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "        output_file_path = os.path.join(output_folder, f'Updated_{filename}')\n",
    "        process_csv(input_file_path, output_file_path)\n",
    "\n",
    "print('All files have been processed and updated.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>user_group</th>\n",
       "      <th>UNIT_KERJA_2__C</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500Mg00000G889vIAB</td>\n",
       "      <td>5780 - UNIT TALANG PADANG PRINGSEWU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTB000032693377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                           user_group  UNIT_KERJA_2__C  \\\n",
       "0  500Mg00000G889vIAB  5780 - UNIT TALANG PADANG PRINGSEWU              NaN   \n",
       "\n",
       "  SCC_LEGACY_TICKET_ID__C  \n",
       "0         TTB000032693377  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Salesforce\\test\\Updated_extract_usergroup2021_done - Copy.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the updated user group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ID, user_group, UNIT_KERJA_2__C, SCC_LEGACY_TICKET_ID__C]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>user_group</th>\n",
       "      <th>UNIT_KERJA_2__C</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, user_group, UNIT_KERJA_2__C, SCC_LEGACY_TICKET_ID__C]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read your data into a DataFrame named 'df'\n",
    "path=r\"D:\\Salesforce\\usergroup2021\\Updated_extract_user_group_2021.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# List of ticket IDs to filter\n",
    "ticket_ids = [\n",
    "    'TTB000032745470',\n",
    "    'TTB000032845281',\n",
    "    'TTB000032822488',\n",
    "    'TTB000032866989',\n",
    "    'TTB000032896091'\n",
    "]\n",
    "\n",
    "# Filtering the DataFrame\n",
    "filtered_df = df[df['SCC_LEGACY_TICKET_ID__C'].isin(ticket_ids)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(filtered_df)\n",
    "\n",
    "# If you want to save the filtered DataFrame to a new CSV file\n",
    "# filtered_df.to_csv('filtered_data.csv', index=False)\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue the Tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'next_1000_tickets.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to generate the next ticket IDs\n",
    "def generate_tickets(start_ticket, count):\n",
    "    tickets = []\n",
    "    prefix = \"TICKET\"\n",
    "    current_number = int(start_ticket.replace(prefix, \"\"))\n",
    "    \n",
    "    number_of_digits = len(start_ticket) - len(prefix)\n",
    "    \n",
    "    for _ in range(count):\n",
    "        current_ticket = f\"{prefix}{current_number:0{number_of_digits}}\"\n",
    "        tickets.append(current_ticket)\n",
    "        current_number += 1\n",
    "    \n",
    "    return tickets\n",
    "\n",
    "# Starting ticket ID\n",
    "start_ticket_id = \"TICKET0012345678910\"\n",
    "\n",
    "# Generate 1000 ticket IDs\n",
    "next_1000_tickets = generate_tickets(start_ticket_id, 1000)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_tickets = pd.DataFrame(next_1000_tickets, columns=['Ticket ID'])\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = \"next_1000_tickets.csv\"\n",
    "df_tickets.to_csv(csv_file_path, index=False)\n",
    "\n",
    "csv_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update escalated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the text file\n",
    "with open(r\"D:\\Python2\\case_2024\\escalated\\extract_to_escalated2.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process lines and convert to DataFrame\n",
    "# Assuming each line in the text file is comma-separated\n",
    "data = []\n",
    "for line in lines:\n",
    "    values = line.strip().split(';')\n",
    "    data.append(values)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(r\"D:\\Python2\\case_2024\\escalated\\extract_to_escalated2.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Load the text file into a DataFrame\n",
    "df = pd.read_csv(r\"D:\\Python2\\case_2024\\escalated\\extract_to_escalated2.txt\", delimiter=';')\n",
    "\n",
    "# Save as CSV with minimal quoting\n",
    "df.to_csv(r\"D:\\Python2\\case_2024\\escalated\\extract_to_escalated2.csv\", index=False, quoting=csv.QUOTE_MINIMAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update status from Esacalated to Closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_28744\\4068740087.py:4: DtypeWarning: Columns (15,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file1 = pd.read_csv(r\"D:\\Python2\\case_2024\\7 aug\\update to closed\\bricare_all_update_status_clean\\bricare_20240510_20240806_0_all.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifno</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>Nominal</th>\n",
       "      <th>...</th>\n",
       "      <th>Tgl_Returned</th>\n",
       "      <th>Ticket_Referensi</th>\n",
       "      <th>Tiket_Urgency</th>\n",
       "      <th>Tipe_Remark</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>users</th>\n",
       "      <th>Usergroup_ID</th>\n",
       "      <th>record_type</th>\n",
       "      <th>Legacy_Ticket_ID</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5188280205773204</td>\n",
       "      <td>TTB000052595425</td>\n",
       "      <td>1101</td>\n",
       "      <td>Sanggahan Pengajuan Kartu Kredit</td>\n",
       "      <td>2024-05-10 01:06:06</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Fraud / Suspicious Transactions</td>\n",
       "      <td>EDI HERMAWAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052595425</td>\n",
       "      <td>500Mg00000EoJEOIA3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCBF095</td>\n",
       "      <td>TTB000052696714</td>\n",
       "      <td>8602</td>\n",
       "      <td>BRIMO</td>\n",
       "      <td>2024-05-15 10:40:22</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Platform &amp; Device Support</td>\n",
       "      <td>SUMARIYANTO</td>\n",
       "      <td>601301000250501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-07-16 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052696714</td>\n",
       "      <td>500Mg00000EoJErIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXG2301</td>\n",
       "      <td>TTB000052710500</td>\n",
       "      <td>8708</td>\n",
       "      <td>Penipuan Kepada Nasabah BRI dan Transaksi dila...</td>\n",
       "      <td>2024-05-15 19:16:48</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Fraud / Suspicious Transactions</td>\n",
       "      <td>ANTON KALEME</td>\n",
       "      <td>520601032951537</td>\n",
       "      <td>11645678.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052710500</td>\n",
       "      <td>500Mg00000EoJEwIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LA70603</td>\n",
       "      <td>TTB000052712166</td>\n",
       "      <td>8708</td>\n",
       "      <td>Penipuan Kepada Nasabah BRI dan Transaksi dila...</td>\n",
       "      <td>2024-05-15 20:50:00</td>\n",
       "      <td>Phone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LINDA ISLAMI</td>\n",
       "      <td>574201025946539</td>\n",
       "      <td>26000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052712166</td>\n",
       "      <td>500Mg00000F3sq7IAB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMND036</td>\n",
       "      <td>TTB000052728166</td>\n",
       "      <td>8708</td>\n",
       "      <td>Penipuan Kepada Nasabah BRI dan Transaksi dila...</td>\n",
       "      <td>2024-05-16 14:02:35</td>\n",
       "      <td>SMS</td>\n",
       "      <td>Fraud / Suspicious Transactions</td>\n",
       "      <td>ABDURRAHMAN</td>\n",
       "      <td>470401030333534</td>\n",
       "      <td>50750000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052728166</td>\n",
       "      <td>500Mg00000EoJF3IAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20527</th>\n",
       "      <td>SFLGQ91</td>\n",
       "      <td>TTB000054433087</td>\n",
       "      <td>8416</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC Ban...</td>\n",
       "      <td>2024-08-06 23:55:02</td>\n",
       "      <td>BRIMO</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>SYAIFUL RACHMAN</td>\n",
       "      <td>106001016629500</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NBMB718820065417</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054433087</td>\n",
       "      <td>500Mg00000GgEYpIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20528</th>\n",
       "      <td>AZXI564</td>\n",
       "      <td>TTB000054433088</td>\n",
       "      <td>8434</td>\n",
       "      <td>Pengisian Pulsa HP gagal, Saldo Berkurang</td>\n",
       "      <td>2024-08-06 23:56:35</td>\n",
       "      <td>BRIMO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASTINA Y BATALIPU</td>\n",
       "      <td>523101015490533</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NBMB718820306932</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054433088</td>\n",
       "      <td>500Mg00000GgEYqIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20529</th>\n",
       "      <td>MRRM771</td>\n",
       "      <td>TTB000054433081</td>\n",
       "      <td>8940</td>\n",
       "      <td>Pengajuan Aplikasi Briguna Digital</td>\n",
       "      <td>2024-08-06 23:59:01</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Platform &amp; Device Support</td>\n",
       "      <td>MULYANA</td>\n",
       "      <td>091901006677501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>12801.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054433081</td>\n",
       "      <td>500Mg00000GgEYrIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20530</th>\n",
       "      <td>AZXI564</td>\n",
       "      <td>TTB000054433091</td>\n",
       "      <td>8434</td>\n",
       "      <td>Pengisian Pulsa HP gagal, Saldo Berkurang</td>\n",
       "      <td>2024-08-06 23:59:17</td>\n",
       "      <td>BRIMO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASTINA Y BATALIPU</td>\n",
       "      <td>523101015490533</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NBMB718820714364</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054433091</td>\n",
       "      <td>500Mg00000GgEYsIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20531</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TTB000054432935</td>\n",
       "      <td>2700</td>\n",
       "      <td>Informasi dan Pendaftaran Autodebet  Kartu Kredit</td>\n",
       "      <td>2024-08-06 23:59:56</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>Didi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054432935</td>\n",
       "      <td>500Mg00000GgEYtIAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20532 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cifno        Ticket_ID  Call_Type_ID  \\\n",
       "0      5188280205773204  TTB000052595425          1101   \n",
       "1               SCBF095  TTB000052696714          8602   \n",
       "2               AXG2301  TTB000052710500          8708   \n",
       "3               LA70603  TTB000052712166          8708   \n",
       "4               AMND036  TTB000052728166          8708   \n",
       "...                 ...              ...           ...   \n",
       "20527           SFLGQ91  TTB000054433087          8416   \n",
       "20528           AZXI564  TTB000054433088          8434   \n",
       "20529           MRRM771  TTB000054433081          8940   \n",
       "20530           AZXI564  TTB000054433091          8434   \n",
       "20531               NaN  TTB000054432935          2700   \n",
       "\n",
       "                                               Call_Type          Create_Date  \\\n",
       "0                       Sanggahan Pengajuan Kartu Kredit  2024-05-10 01:06:06   \n",
       "1                                                  BRIMO  2024-05-15 10:40:22   \n",
       "2      Penipuan Kepada Nasabah BRI dan Transaksi dila...  2024-05-15 19:16:48   \n",
       "3      Penipuan Kepada Nasabah BRI dan Transaksi dila...  2024-05-15 20:50:00   \n",
       "4      Penipuan Kepada Nasabah BRI dan Transaksi dila...  2024-05-16 14:02:35   \n",
       "...                                                  ...                  ...   \n",
       "20527  Nasabah BRI Gagal Transaksi Belanja di EDC Ban...  2024-08-06 23:55:02   \n",
       "20528          Pengisian Pulsa HP gagal, Saldo Berkurang  2024-08-06 23:56:35   \n",
       "20529                 Pengajuan Aplikasi Briguna Digital  2024-08-06 23:59:01   \n",
       "20530          Pengisian Pulsa HP gagal, Saldo Berkurang  2024-08-06 23:59:17   \n",
       "20531  Informasi dan Pendaftaran Autodebet  Kartu Kredit  2024-08-06 23:59:56   \n",
       "\n",
       "      gateway                    Jenis_Laporan       Nama_Nasabah  \\\n",
       "0       Phone  Fraud / Suspicious Transactions       EDI HERMAWAN   \n",
       "1       Phone        Platform & Device Support        SUMARIYANTO   \n",
       "2       Phone  Fraud / Suspicious Transactions       ANTON KALEME   \n",
       "3       Phone                              NaN       LINDA ISLAMI   \n",
       "4         SMS  Fraud / Suspicious Transactions        ABDURRAHMAN   \n",
       "...       ...                              ...                ...   \n",
       "20527   BRIMO          Complaint - Transaction    SYAIFUL RACHMAN   \n",
       "20528   BRIMO                              NaN  ASTINA Y BATALIPU   \n",
       "20529   Phone        Platform & Device Support            MULYANA   \n",
       "20530   BRIMO                              NaN  ASTINA Y BATALIPU   \n",
       "20531   Phone                          Request               Didi   \n",
       "\n",
       "           No_Rekening     Nominal  ...         Tgl_Returned Ticket_Referensi  \\\n",
       "0                  NaN         0.0  ...                  NaN              NaN   \n",
       "1      601301000250501         0.0  ...  2024-07-16 00:00:00              NaN   \n",
       "2      520601032951537  11645678.0  ...                  NaN              NaN   \n",
       "3      574201025946539  26000000.0  ...                  NaN              NaN   \n",
       "4      470401030333534  50750000.0  ...                  NaN              NaN   \n",
       "...                ...         ...  ...                  ...              ...   \n",
       "20527  106001016629500     70000.0  ...                  NaN              NaN   \n",
       "20528  523101015490533     20000.0  ...                  NaN              NaN   \n",
       "20529  091901006677501         0.0  ...                  NaN              NaN   \n",
       "20530  523101015490533      1500.0  ...                  NaN              NaN   \n",
       "20531              NaN         0.0  ...                  NaN              NaN   \n",
       "\n",
       "      Tiket_Urgency Tipe_Remark          UniqueID     users Usergroup_ID  \\\n",
       "0               NaN       Notes               NaN      Call          5.0   \n",
       "1               NaN       Notes               NaN      Call          4.0   \n",
       "2               NaN       Notes               NaN      Call          5.0   \n",
       "3               NaN       Notes               NaN      Call          5.0   \n",
       "4               NaN       Notes               NaN  Non Call          5.0   \n",
       "...             ...         ...               ...       ...          ...   \n",
       "20527           NaN       Notes  NBMB718820065417  Non Call          1.0   \n",
       "20528           NaN       Notes  NBMB718820306932  Non Call          1.0   \n",
       "20529           NaN       Notes               NaN      Call      12801.0   \n",
       "20530           NaN       Notes  NBMB718820714364  Non Call          1.0   \n",
       "20531           NaN       Notes               NaN      Call         15.0   \n",
       "\n",
       "          record_type Legacy_Ticket_ID                  ID  \n",
       "0      Case Migration    TTB0052595425  500Mg00000EoJEOIA3  \n",
       "1      Case Migration    TTB0052696714  500Mg00000EoJErIAN  \n",
       "2      Case Migration    TTB0052710500  500Mg00000EoJEwIAN  \n",
       "3      Case Migration    TTB0052712166  500Mg00000F3sq7IAB  \n",
       "4      Case Migration    TTB0052728166  500Mg00000EoJF3IAN  \n",
       "...               ...              ...                 ...  \n",
       "20527  Case Migration    TTB0054433087  500Mg00000GgEYpIAN  \n",
       "20528  Case Migration    TTB0054433088  500Mg00000GgEYqIAN  \n",
       "20529  Case Migration    TTB0054433081  500Mg00000GgEYrIAN  \n",
       "20530  Case Migration    TTB0054433091  500Mg00000GgEYsIAN  \n",
       "20531  Case Migration    TTB0054432935  500Mg00000GgEYtIAN  \n",
       "\n",
       "[20532 rows x 83 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first file into a DataFrame\n",
    "file1 = pd.read_csv(r\"D:\\Python2\\case_2024\\7 aug\\update to closed\\bricare_all_update_status_clean\\bricare_20240510_20240806_0_all.csv\")\n",
    "\n",
    "# Load the second file into a DataFrame\n",
    "file2 = pd.read_csv(r\"D:\\Python2\\case_2024\\7 aug\\update to closed\\bricare_all_update_status_clean\\extract_escalated_to_closed.csv\", delimiter=';')\n",
    "\n",
    "# Merge the DataFrames on the 'Ticket_ID' column\n",
    "merged_df = pd.merge(file1, file2[['Ticket_ID', 'ID']], on='Ticket_ID', how='left')\n",
    "\n",
    "# Save the resulting DataFrame to a new file\n",
    "merged_df.to_csv(r\"D:\\Python2\\case_2024\\7 aug\\update to closed\\bricare_all_update_status_clean\\merged.csv\", index=False)\n",
    "\n",
    "# print(\"Merge completed and saved to merged_file.csv\")\n",
    "\n",
    "merged_df['ID']\n",
    "\n",
    "\n",
    "# Display the first few rows and the columns of both DataFrames to ensure they are loaded correctly\n",
    "# file1_info = file1.head(), file1.columns\n",
    "# file2_info = file2.head(), file2.columns\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CIFNO', 'TICKET_ID', 'CALL_TYPE_ID', 'CALL_TYPE', 'CREATE_DATE',\n",
      "       'GATEWAY', 'JENIS_LAPORAN', 'NAMA_NASABAH', 'NO_REKENING', 'NOMINAL',\n",
      "       'STATUS', 'TANGGALCLOSED', 'TANGGALTRANSAKSI', 'CHANEL', 'FITUR',\n",
      "       'NOMOR_KARTU', 'USER_GROUP', 'ASSGINED_TO', 'ATTACHMENT_DONE', 'EMAIL',\n",
      "       'FULL_NAME', 'NO_TELEPON', 'APPROVER_LOGIN', 'APPROVER_NAME',\n",
      "       'SLARESOLUTION', 'SUBMITTER_LOGIN_ID', 'SUBMITTER_USER_GROUP',\n",
      "       'USER_LOGIN_NAME', 'DETAILS', 'JENIS_PRODUK', 'LAST_MODIFIED_BY',\n",
      "       'MERCHANT_ID', 'MODIFIED_DATE', 'NOTAS', 'PRODUK', 'SLA_STATUS', 'TID',\n",
      "       'TANGGALATTACHMENTDONE', 'TGLASSIGNED', 'TGL_ESKALASI', 'ANALISASKILS',\n",
      "       'ATTACHMENT_', 'BANK_BRI', 'BIAYA_ADMIN', 'SUKU_BUNGA', 'BUNGA',\n",
      "       'BUTUH_ATTACHMENT', 'CICILAN', 'HASIL_KUNJUNGAN', 'LOG_NAME',\n",
      "       'MMS_TICKET_ID', 'MASS_TICKET_UPLOAD_FLAG', 'NAMA_SUPERVISOR',\n",
      "       'NAMA_TL', 'NAMA_WAKABAG', 'NASABAH_PRIORITAS', 'NOTIFY_BY',\n",
      "       'ORGANIZATION', 'OUTPUT_SETTLEMENT', 'PHONE_SURVEY', 'RETURN_TICKET',\n",
      "       'SETTLEMENT_BY', 'SETTLEMENT_ID', 'SETTLEMENT', 'SITE_USER',\n",
      "       'STATUS_RETURN', 'STATUS_TRANSAKSI', 'SUBMITTER_REGION',\n",
      "       'SUBMITTER_SITEGROUP', 'SUBMITTER_USER_GROUP_ID', 'TANGGAL_SETTLEMENT',\n",
      "       'TGL_FOWARD', 'TGL_IN_PROGRESS', 'TGL_RETURNED', 'TICKET_REFERENSI',\n",
      "       'TIKET_URGENCY', 'TIPE_REMARK', 'UNIQUEID', 'USERS', 'USERGROUP_ID',\n",
      "       'RECORD_TYPE', 'LEGACY_TICKET_ID', 'ERROR'],\n",
      "      dtype='object')\n",
      "No 'email' column found in the dataset.\n",
      "Cleaned data saved to: D:\\Python2\\case_2024\\5-6 aug\\data bricare close new clean\\error080724045250191_account.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\case_2024\\5-6 aug\\data bricare close new clean\\error080724045250191.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the column names to find the correct column for email\n",
    "print(df.columns)\n",
    "\n",
    "# Assuming the email column is named 'email', we will filter out the unwanted email\n",
    "if 'email' in df.columns:\n",
    "    df_cleaned = df[df['email'] != 'user@bri.co.id']\n",
    "else:\n",
    "    df_cleaned = df  # No changes needed if 'email' column does not exist\n",
    "    print(\"No 'email' column found in the dataset.\")\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "output_file_path = r\"D:\\Python2\\case_2024\\5-6 aug\\data bricare close new clean\\error080724045250191_account.csv\"\n",
    "df_cleaned['RECORD_TYPE']='Person Account'\n",
    "df_cleaned.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Confirm the process\n",
    "print(f\"Cleaned data saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first file into a DataFrame\n",
    "file1 = pd.read_csv(r\"D:\\Python2\\case_2024\\5-6 aug\\data bricare close new clean\\bricare_20240110_20240804_0_all.csv\")\n",
    "\n",
    "# Load the second file into a DataFrame\n",
    "file2 = pd.read_csv(r\"D:\\Python2\\case_2024\\5-6 aug\\data bricare close new clean\\extract_to_escalated.csv\")\n",
    "\n",
    "\n",
    "# Load the second file into a DataFrame with the correct delimiter\n",
    "file2 = pd.read_csv(file2_path, delimiter=';')\n",
    "\n",
    "# Ensure the 'Ticket_ID' and 'ID' columns are present\n",
    "file2 = file2[['Ticket_ID', 'ID']]\n",
    "\n",
    "# Merge the DataFrames on the 'Ticket_ID' column\n",
    "merged_df = pd.merge(file1, file2, on='Ticket_ID', how='left')\n",
    "\n",
    "# Save the resulting DataFrame to a new file\n",
    "output_path = '/mnt/data/merged_file_escalated.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n",
    "\n",
    "\n",
    "# Load the second file into a DataFrame with the correct delimiter\n",
    "file2 = pd.read_csv(file2_path, delimiter=';')\n",
    "\n",
    "# Ensure the 'Ticket_ID' and 'ID' columns are present\n",
    "file2 = file2[['Ticket_ID', 'ID']]\n",
    "\n",
    "# Merge the DataFrames on the 'Ticket_ID' column\n",
    "merged_df = pd.merge(file1, file2, on='Ticket_ID', how='left')\n",
    "\n",
    "# Save the resulting DataFrame to a new file\n",
    "output_path = '/mnt/data/merged_file_escalated.csv'\n",
    "\n",
    "\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Merged DataFrame\", dataframe=merged_df)\n",
    "\n",
    "output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Account error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cifno', 'Nama_Nasabah', 'no_telepon', 'email'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerson Account\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Filter the columns and rows based on the specified conditions\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJenis_Nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Jenis_Nasabah is empty\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtipe_nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndividu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# tipe_nasabah is Individu\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecord_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPerson Account\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# record_type is Person Account\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNama_Nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_telepon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memail\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJenis_Nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtipe_nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecord_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Selected columns\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Ensure full numbers are retained in the CSV output\u001b[39;00m\n\u001b[0;32m     21\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mfloat_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['cifno', 'Nama_Nasabah', 'no_telepon', 'email'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded file\n",
    "file_path = r\"D:\\Python2\\case_2024\\missing_ttb_from attachment\\error080724030058935.csv\"\n",
    "data = pd.read_csv(file_path, dtype={'cifno': str})  # Ensure 'cifno' is read as a string\n",
    "\n",
    "# Add the columns 'Jenis_Nasabah', 'tipe_nasabah', and 'record_type' with default values\n",
    "data['Jenis_Nasabah'] = None  # Assuming empty means None\n",
    "data['tipe_nasabah'] = 'Individu'\n",
    "data['record_type'] = 'Person Account'\n",
    "\n",
    "# Filter the columns and rows based on the specified conditions\n",
    "filtered_data = data.loc[\n",
    "    (data['Jenis_Nasabah'].isna()) &  # Jenis_Nasabah is empty\n",
    "    (data['tipe_nasabah'] == 'Individu') &  # tipe_nasabah is Individu\n",
    "    (data['record_type'] == 'Person Account'),  # record_type is Person Account\n",
    "    ['cifno', 'Nama_Nasabah', 'no_telepon', 'email', 'Jenis_Nasabah', 'tipe_nasabah', 'record_type']  # Selected columns\n",
    "]\n",
    "\n",
    "# Ensure full numbers are retained in the CSV output\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "output_path = r\"D:\\Python2\\case_2024\\missing_ttb_from attachment\\error080724030058935_done.csv\"\n",
    "filtered_data.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the filtered data to the user\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the DataFrame: ['CIFNO,\"TICKET_ID\",\"CALL_TYPE_ID\",\"CALL_TYPE\",\"CREATE_DATE\",\"GATEWAY\",\"JENIS_LAPORAN\",\"NAMA_NASABAH\",\"NO_REKENING\",\"NOMINAL\",\"STATUS\",\"TANGGALCLOSED\",\"TANGGALTRANSAKSI\",\"CHANEL\",\"FITUR\",\"NOMOR_KARTU\",\"USER_GROUP\",\"ASSGINED_TO\",\"ATTACHMENT_DONE\",\"EMAIL\",\"FULL_NAME\",\"NO_TELEPON\",\"APPROVER_LOGIN\",\"APPROVER_NAME\",\"SLARESOLUTION\",\"SUBMITTER_LOGIN_ID\",\"SUBMITTER_USER_GROUP\",\"USER_LOGIN_NAME\",\"DETAILS\",\"JENIS_PRODUK\",\"LAST_MODIFIED_BY\",\"MERCHANT_ID\",\"MODIFIED_DATE\",\"NOTAS\",\"PRODUK\",\"SLA_STATUS\",\"TID\",\"TANGGALATTACHMENTDONE\",\"TGLASSIGNED\",\"TGL_ESKALASI\",\"ANALISASKILS\",\"ATTACHMENT_\",\"BANK_BRI\",\"BIAYA_ADMIN\",\"SUKU_BUNGA\",\"BUNGA\",\"BUTUH_ATTACHMENT\",\"CICILAN\",\"HASIL_KUNJUNGAN\",\"LOG_NAME\",\"MMS_TICKET_ID\",\"MASS_TICKET_UPLOAD_FLAG\",\"NAMA_SUPERVISOR\",\"NAMA_TL\",\"NAMA_WAKABAG\",\"NASABAH_PRIORITAS\",\"NOTIFY_BY\",\"ORGANIZATION\",\"OUTPUT_SETTLEMENT\",\"PHONE_SURVEY\",\"RETURN_TICKET\",\"SETTLEMENT_BY\",\"SETTLEMENT_ID\",\"SETTLEMENT\",\"SITE_USER\",\"STATUS_RETURN\",\"STATUS_TRANSAKSI\",\"SUBMITTER_REGION\",\"SUBMITTER_SITEGROUP\",\"SUBMITTER_USER_GROUP_ID\",\"TANGGAL_SETTLEMENT\",\"TGL_FOWARD\",\"TGL_IN_PROGRESS\",\"TGL_RETURNED\",\"TICKET_REFERENSI\",\"TIKET_URGENCY\",\"TIPE_REMARK\",\"UNIQUEID\",\"USERS\",\"USERGROUP_ID\",\"RECORD_TYPE\",\"LEGACY_TICKET_ID\",\"ERROR\"', 'P045610,\"TTB000052765512\",\"8907\",\"Taspen\",\"2024-05-17 14:30:58\",\"SMS\",\"Request\",\"PAINTEN\",\"401027503502.0\",\"0.0\",\"Escalated\",\"\",\"1970-01-01 07:00:00\",\"\",\"Penyelesaian Taspen\",\"\",\"04010 -- UNIT TUNGGILIS BANJAR\",\"\",\"\",\"user@bri.co.id\",\"BRICare\",\"\",\"\",\"\",\"10.0\",\"mii_jordan\",\"\",\"\",\"Mohon diperhatikan bahwa tiket diberikan kepada unit kerja yang menjadi juru bayar pensiunan terkait.', 'Diharapkan kepada Unit Kerja Penerima Tiket untuk melakukan hal-hal sebagai berikut:', '1. Melakukan kunjungan nasabah pensiunan sesuai dengan data pada Tiket.', '2. mendokumentasikan kegiatan kunjungan berupa foto/video bersama dengan nasabah pensiunan ybs. Jika saat kunjungan gagal menemui nasabah ybs, maka diharapkan tetap melakukan dokumentasi bukti kunjungan seperti foto bersama dengan anggota keluarga yg ditemui atau foto di depan rumah nasabah.', '3. Apabila unit kerja memiliki nomor HP nasabah yang aktif, kegiatan kunjungan dapat dilakukan melalui media Video Call dengan nasabah. Pastikan nasabah yang dihubungi sesuai dan capture bukti video call tersebut sebagai dokumentasi.', '4. Mengisi Lembar Kunjungan Nasabah / LKPP dengan lengkap, dan ditandatangani oleh Nasabah ybs. Jika saat kunjungan nasabah diketahui sudah pindah alamat, maka Lembar Kunjungan ditandatangani oleh minimal RT setempat.', '5. Melakukan upload hasil kunjungan serta dokumentasi pada tiket, lalu mengisi jenis hasil kunjungan, serta remarks. Setelah lengkap, unit kerja diperkenankan untuk melakukan  close tiket dengan memilih button “In Progress” lalu pilih button “Close”', '6. Petugas yang melakukan kunjungan melakukan sosialisasi kepada Pensiunan terkait otentikasi menggunakan Smartphone khususnya bagi pensiunan yang sudah melakukan enrollment. Namun demikian, apabila terdapat kendala saat melakukan otentikasi melalui smartphone, maka unit kerja dapat melakukan otentikasi pada aplikasi BRIPENS. Prosedur yang dilakukan sesuai dengan aturan pelaksanaan otentikasi.', '7. Bagi unit kerja yang merasa tiket tersebut salah unit kerja, diharapkan untuk melakukan pengecekan unit kerja pemilik rekening lalu melakukan “Forward” tiket ke unit kerja terkait.\",\"Taspen\",\"\",\"\",\"2024-08-05 00:52:39\",\"P5500056570\",\"Servicing\",\"In Progress\",\"\",\"2024-07-01 15:28:09\",\"2024-07-03 00:00:00\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"5490.0\",\"Case Migration\",\"TTB0052765512\",\"INVALID_FIELD:Foreign key external ID: p045610 not found for field SCC_cifno__c in entity Account\"', 'FDT4682,\"TTB000053358878\",\"1009\",\"CERIA - Permintaan Buka Blokir R CERIA\",\"2024-06-14 16:31:53\",\"Sabrina\",\"\",\"FITRI PRIYATI\",\"479101029845535.0\",\"0.0\",\"Escalated\",\"\",\"\",\"\",\"Permintaan Buka Blokir R Kartu Kredit\",\"1895121000297147.2\",\"LCC-FBI\",\"\",\"\",\"\",\"Dicky Zulhan\",\"081212962892\",\"\",\"\",\"10.0\",\"Dicky Boer\",\"DCE\",\"Dicky Zulhan\",\"Nasabah menanyakan Aplikasi CERIA tidak bisa digunakan untuk bertransaksi', 'ID CERIA : 1895121000297147', 'No Hp yang terdaftar : 081212962892', 'Tanggal :211023', 'Nominal :-', 'Merchant/lokasi :-', 'Respon Transaksi : Tarik tunai gagal akun bermasalah\",\"\",\"\",\"\",\"2024-08-05 00:54:44\",\"\",\"\",\"In Progress\",\"\",\"2024-06-18 19:58:32\",\"2024-06-20 09:11:18\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"Mesita Widia Cahyaning\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"DDB\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"10102.0\",\"\",\"2024-06-18 19:58:48\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053358878\",\"INVALID_FIELD:Foreign key external ID: fdt4682 not found for field SCC_cifno__c in entity Account\"', 'MADVI54,\"TTB000053364875\",\"8915\",\"Gagal Setor Tunai di CRM BRI Via BRImo / Kartu Debit\",\"2024-06-15 00:08:21\",\"BRIMO\",\"\",\"MUHAMMAD SAFII\",\"181301005002504.0\",\"100000.0\",\"Escalated\",\"\",\"2024-06-14 00:00:00\",\"CRM\",\"Setor Tunai - gagal dengan Kartu Debit\",\"5221840959976292.0\",\"LCC-ON US\",\"\",\"\",\"\",\"\",\"081361511915\",\"\",\"\",\"8.0\",\"\",\"\",\"\",\"Lokasi atau Nomor ID CDM/CRM: 100000,', 'Nomor Rekening: 181301005002504,', 'Keterangan Tambahan: Belum masuk\",\"\",\"\",\"\",\"2024-08-05 00:54:47\",\"\",\"\",\"In Progress\",\"\",\"2024-07-03 16:52:32\",\"2024-06-20 00:00:00\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Indri Edriati\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-06-20 08:41:22\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB697443656285\",\"Non Call\",\"6.0\",\"Case Migration\",\"TTB0053364875\",\"INVALID_FIELD:Foreign key external ID: madvi54 not found for field SCC_cifno__c in entity Account\"', 'AUXY475,\"TTB000053390049\",\"8915\",\"Gagal Setor Tunai di CRM BRI Via BRImo / Kartu Debit\",\"2024-06-16 17:35:39\",\"BRIMO\",\"Complaint - Transaction\",\"ALVIN VIKI MAULANA\",\"623901014026500.0\",\"400000.0\",\"Escalated\",\"\",\"2024-06-16 17:37:00\",\"CRM\",\"Setor Tunai - gagal via BRImo\",\"5221843121258912.0\",\"LCC-ON US\",\"\",\"\",\"\",\"\",\"081914701439\",\"\",\"\",\"8.0\",\"\",\"\",\"\",\"Lokasi atau Nomor ID CDM/CRM: 190324,', 'Nomor Rekening: 623901014026500,', 'Keterangan Tambahan: SUDAH SETOR TUNAI DI MESIN CRM UNIT YOSORATI UANG MASUK TAPI SALDO SAYA TIDAK NAMBAH DI BRIMO\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:54:51\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-06-21 14:06:01\",\"2024-06-24 00:00:00\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Indri Edriati\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-06-24 15:30:11\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB698093331852\",\"Non Call\",\"6.0\",\"Case Migration\",\"TTB0053390049\",\"INVALID_FIELD:Foreign key external ID: auxy475 not found for field SCC_cifno__c in entity Account\"', '5475820103734402,\"TTB000053478350\",\"3202\",\"Permintaan Penutupan Kartu Kredit tanpa Anti Attrition, tidak ada tagihan\",\"2024-06-21 15:16:14\",\"Media Umum Lainya\",\"Request\",\"IWAN KUSWARA\",\"\",\"0.0\",\"Escalated\",\"\",\"\",\"\",\"Penutupan Kartu\",\"5475820103734402.0\",\"KKD-CRI-DATAM\",\"\",\"\",\"\",\"Asep Panji Nugraha\",\"0123456789\",\"\",\"\",\"1.0\",\"90125703\",\"\",\"Asep Panji Nugraha\",\"Mohon bantuannya untuk penutupan kartu, blok A', 'terima kasih\",\"Kartu Kredit\",\"\",\"\",\"2024-08-05 00:55:10\",\"\",\"Loans\",\"In Progress\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"Sri Astuti\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"00229 -- Kas Kanpus\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"\",\"\",\"\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"15.0\",\"Case Migration\",\"TTB0053478350\",\"INVALID_FIELD:Foreign key external ID: 5475820103734402 not found for field SCC_cifno__c in entity Account\"', 'FO33214,\"TTB000053519862\",\"8425\",\"Pen-delete-an Status Registrasi Layanan yang Ada di BRI\",\"2024-06-24 11:15:24\",\"Phone\",\"\",\"FITRA TATA LARANGGA\",\"107901050414500.0\",\"0.0\",\"Escalated\",\"\",\"2024-06-24 00:00:00\",\"UKO\",\"e-channel\",\"5221843134094965.0\",\"LCC-CCTCALL\",\"\",\"\",\"\",\"Yusef Budiman Efendi\",\"\",\"\",\"\",\"10.0\",\"90054105\",\"LCC-FBI\",\"Yusef Budiman Efendi\",\"Telah dilakukan blokir (hold amount) kepada rekening-rekening berikut karena terdeteksi merupakan penampung pembayaran judi online.', '1107901050414500FITRA TATA LARANGGA', '2530901010362508EDO DARMAWAN', '3106301000805568GURUH ARIF S', '4384801018804508DEBI JUANAH', '5026601088318502JAYLANI PRATAMA ARIF', '6486101057355537PATURAHMAN', '7713801008253504INDRA HALIM CAHNUARI', '8022801072956504Hani Aprilia Siregar', '9118001018218503SALSABILA HOERUNIDA', '10529101014567505LUKY FERNANDO WIJAYA', '11033901120757508RIDHO MAULANA ABDI', '12044201062021503Yuniar Anisa Putri', '13036701102047506RIKY PERDANA', '14005501142218502RIVALDA ANGGRAINI', '15530501031951538GUANDI RIADY', '16041501019907509JOSH VERREL ANGGANI', '17118301009834500ADRIAN RAHARTO', '18026101138151509SIGIT ARUM PRACIPTO', '19031901001403569IAN SURYA YOSUA N', '20321901026640539IBNAN KHOLISH', '21173101006369501MUHAMAD JUAN', '22736701005422503ACHMAD HARIS', '23112901023944503MARSYA MAHARANI', '24379601007058503SONI SAPUTRA', '25408801048555530M KAHFI', '26038701048170502RIO', '27044501071482502MOHAMMAD ANDRIANTO MANALU', '28010501023554537AGUS MULYANA', '29051701043844509MUHAMMAD PASLAH MAULIDNA', '30229301026344500HENDRY RIES TANTI', '31019401101011503RIA A RITO', '32005301150630504FITRIANI LUBIS', '33184801004008506GABRIEL ZEFANYA TULUNG', '34763201012979538SITI AISYAH', '35109001027866500LIAWATI', '36720001002456509FAJAR WIDIYANTO', '37042101069132509Dede Rusdian', '38201101004457531RIZK** FEBR**', '39531701002702507HANXXX AFRIANXXXX', '40090901074209533SUYATNO', '41013701002248566MIKO HAMDANI', '42105501014098506IRVAN SUGIYANTO', '43010701126303508ADI SETIADI', '44053301022396507UJANG SUPRIATNA', '45330501005510506DINA MARYANI', '46163301004610508RIA SUSANTI', '47120701004642500HARI MULYA', '48001901081587509VIRA SAVADILA', '49065801024912507MIE FUNG', '50479901020956501SUDIELI ZEBUA', '51066501000413564Irwansyah', '52009401090231505SONI SONALI', '53078701043727534SUSI FATMALA', '54213601000541560AWAL ALDIEN TSABIT', '55034001120321508NURBAITY', '56012201160363508MUHAMMAD GIBRAN YUSUF', '57063501038370503SRI REZEKI', '58118701009694508DWI DANA YANI', '59752401001376508ULAN SUCIATI', '60345301056154536ONIH ROHANI', 'NGSIH', '61078601029421534NOVIA ANGGRAENI', '62028501120486506ANGGITASARI', 'remarks di TT sumber info dari Pak Toto ORD by WA tgl 24062024\",\"\",\"\",\"\",\"2024-08-05 00:55:23\",\"\",\"\",\"In Progress\",\"\",\"2024-06-24 16:10:34\",\"\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Ade Mulki Erlangga .\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"LCC-FBI\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"5.0\",\"\",\"\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"4.0\",\"Case Migration\",\"TTB0053519862\",\"INVALID_FIELD:Foreign key external ID: fo33214 not found for field SCC_cifno__c in entity Account\"', 'IWV5456,\"TTB000053710210\",\"8411\",\"Salah Transfer antar BRI\",\"2024-07-02 21:33:08\",\"BRIMO\",\"Request\",\"INDAH KHARISMA\",\"809001021061533.0\",\"2000000.0\",\"Escalated\",\"\",\"2024-07-02 21:21:09\",\"ATM BRI\",\"Transfer\",\"5221840842783442.0\",\"LCC-CRC\",\"90152005.0\",\"\",\"indahhkharisma29@gmail.com\",\"\",\"082385017427\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 541501018886535,', 'Nama Rekening: ERDAWATI,', 'Waktu Transaksi: 2024-07-02 21:21:09,', 'Remark: NBMB INDAH KHARISMA TO ERDAWATI,', 'Keterangan Tambahan: Salah transfer\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:56:40\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-07-31 18:08:39\",\"2024-07-03 08:46:07\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-03 10:51:52\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB704539090952\",\"Non Call\",\"3.0\",\"Case Migration\",\"TTB0053710210\",\"INVALID_FIELD:Foreign key external ID: iwv5456 not found for field SCC_cifno__c in entity Account\"', 'RLGV004,\"TTB000053772985\",\"8416\",\"Nasabah BRI Gagal Transaksi Belanja di EDC Bank Lain atau melalui QRIS BRImo, Saldo Rekening Berkurang\",\"2024-07-05 15:50:07\",\"BRIMO\",\"Complaint - Transaction\",\"RIZKI ARI SANDI\",\"60201014476504.0\",\"500000.0\",\"Escalated\",\"2024-07-10 16:54:35\",\"2024-07-05 11:17:51\",\"EDC BANK LAIN\",\"Pembayaran\",\"5221840994235258.0\",\"LCC-ISSUER\",\"\",\"\",\"\",\"\",\"083866397012\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Nama Merchant: yopi potret kehidupan ,', 'Waktu Transaksi: 2024-07-05 11:17:51,', 'Remark: QRIS705551393453#9360000210078800129,', 'Keterangan Tambahan: Saya belum menerima produk yang saya beli dan saya merasa telah di tipu\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:57:23\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-07-09 17:32:58\",\"2024-07-10 00:00:00\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-11 13:43:57\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB705666797168\",\"Non Call\",\"1.0\",\"Case Migration\",\"TTB0053772985\",\"INVALID_FIELD:Foreign key external ID: rlgv004 not found for field SCC_cifno__c in entity Account\"', 'RNH2779,\"TTB000053831016\",\"8419\",\"sanggahan transaksi/tidak merasa bertransaksi ( tanpa kartu )\",\"2024-07-08 21:41:37\",\"BRIMO\",\"\",\"ROMI ERITA\",\"335901022421535.0\",\"9000.0\",\"Escalated\",\"\",\"2024-06-16 20:29:58\",\"Internet Banking / BRImo\",\"Transaksi Normal\",\"5221840955938890.0\",\"LCC-FBI\",\"90104744.0\",\"\",\"\",\"\",\"081371389272\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Nama Merchant: Firmansyah Voucher Gaming,', 'Waktu Transaksi: 2024-06-16 20:29:58,', 'Remark: QRIS698159099193#9360000210066870033,', 'Keterangan Tambahan: Saya tidak melakukan pembelian qris tersebut\",\"\",\"\",\"\",\"2024-08-05 00:58:26\",\"\",\"\",\"In Progress\",\"\",\"2024-07-25 12:00:00\",\"2024-07-25 12:00:00\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-25 12:00:25\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB707020421776\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053831016\",\"INVALID_FIELD:Foreign key external ID: rnh2779 not found for field SCC_cifno__c in entity Account\"', 'RNH2779,\"TTB000053831027\",\"8419\",\"sanggahan transaksi/tidak merasa bertransaksi ( tanpa kartu )\",\"2024-07-08 21:43:41\",\"BRIMO\",\"Fraud / Suspicious Transactions\",\"ROMI ERITA\",\"335901022421535.0\",\"15000.0\",\"Escalated\",\"\",\"2024-05-01 15:39:51\",\"Internet Banking / BRImo\",\"Transaksi Normal\",\"5221840955938890.0\",\"LCC-FBI\",\"90104744.0\",\"\",\"\",\"\",\"081371389272\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Nama Merchant: Axio Store Gaming Voucher ,', 'Waktu Transaksi: 2024-05-01 15:39:51,', 'Remark: QRIS679162156454#9360000210066870033,', 'Keterangan Tambahan: Saya tidak pernah melakukan pembayaran qris tersebut\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:58:26\",\"\",\"Servicing\",\"In Progress\",\"\",\"2024-07-25 12:00:56\",\"2024-07-25 12:00:56\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-25 12:01:15\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB707021067131\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053831027\",\"INVALID_FIELD:Foreign key external ID: rnh2779 not found for field SCC_cifno__c in entity Account\"', 'RNH2779,\"TTB000053831033\",\"8419\",\"sanggahan transaksi/tidak merasa bertransaksi ( tanpa kartu )\",\"2024-07-08 21:45:11\",\"BRIMO\",\"Fraud / Suspicious Transactions\",\"ROMI ERITA\",\"335901022421535.0\",\"25000.0\",\"Escalated\",\"\",\"2024-01-13 02:51:56\",\"Internet Banking / BRImo\",\"Transaksi Normal\",\"5221840955938890.0\",\"LCC-FBI\",\"90104744.0\",\"\",\"\",\"\",\"081371389272\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Nama Merchant: arenapulsaku,', 'Waktu Transaksi: 2024-01-13 02:51:56,', 'Remark: QRIS635004285254#9360000210066870033,', 'Keterangan Tambahan: Saya tidak pernah melakukan pembayaran qris tersebut\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:58:26\",\"\",\"Servicing\",\"In Progress\",\"\",\"2024-07-25 12:01:51\",\"2024-07-25 12:01:51\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-25 12:02:13\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB707021531892\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053831033\",\"INVALID_FIELD:Foreign key external ID: rnh2779 not found for field SCC_cifno__c in entity Account\"', 'IWW6276,\"TTB000053876554\",\"8915\",\"Gagal Setor Tunai di CRM BRI Via BRImo / Kartu Debit\",\"2024-07-11 00:20:56\",\"BRIMO\",\"Complaint - Transaction\",\"IRFAN KASUMA\",\"503001037728530.0\",\"200000.0\",\"Escalated\",\"\",\"2024-07-11 01:20:00\",\"CRM\",\"Setor Tunai - gagal dengan Kartu Debit\",\"6013011234152998.0\",\"LCC-ON US\",\"\",\"\",\"\",\"\",\"0895330101983\",\"\",\"\",\"8.0\",\"\",\"\",\"\",\"Lokasi atau Nomor ID CDM/CRM: 100000,', 'Nomor Rekening: 503001037728530,', 'Keterangan Tambahan: Saldo belum masuk,transaksi dalam proses sudah beberah jam\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:59:09\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-07-26 14:08:04\",\"2024-07-12 00:00:00\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Indri Edriati\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-12 09:04:23\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB707865899275\",\"Non Call\",\"6.0\",\"Case Migration\",\"TTB0053876554\",\"INVALID_FIELD:Foreign key external ID: iww6276 not found for field SCC_cifno__c in entity Account\"', 'MLJN458,\"TTB000053904780\",\"8602\",\"BRImo Permasalahan Sistem Error\",\"2024-07-12 10:43:10\",\"Email\",\"\",\"MUHAMAD SIDIK\",\"741201006530530.0\",\"0.0\",\"Closed\",\"2024-07-31 09:34:39\",\"\",\"Aplikasi\",\"Lambat/koneksi terputus\",\"\",\"LCC-CCTCALL\",\"\",\"\",\"bella231173@gmail.com\",\"Fitriana Isnaeni Ismant\",\"083124631599\",\"60019.0\",\"Mentari\",\"1.0\",\"60826\",\"CCT-CDS-SMG\",\"Fitriana Isnaeni Ismant\",\"Nasabah gagal Login BRImo keterangan koneksi internet kurang baik', 'Nomor terdaftar:083124631599', 'Email terdaftar : bella231173@gmail.com', 'Tipe ponsel:f9', 'Versi Aplikasi BRImo 2.63.0', 'Berapa kali percobaan: lebih dari 10kali', 'Sejak seminggu yang lalu', 'Nama: MUHAMAD SIDIK', 'Nomor rekening:741201006530530', 'Respon yang di tampilkan: kualitas internet kurang baik, padahal sudah cukup baik.\",\"\",\"\",\"\",\"2024-08-01 02:00:03\",\"\",\"\",\"Missed\",\"\",\"2024-07-21 13:30:30\",\"\",\"\",\"CDS\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Desak Putu W\",\"Mentari\",\"\",\"\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"LCC-CCTCALL\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14503.0\",\"\",\"\",\"\",\"2024-07-21 13:30:30\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"4.0\",\"Case Migration\",\"TTB0053904780\",\"INVALID_FIELD:Foreign key external ID: mljn458 not found for field SCC_cifno__c in entity Account\"', 'JEY3866,\"TTB000053921870\",\"8708\",\"Penipuan Kepada Nasabah BRI dan Transaksi dilakukan Sendiri oleh Nasabah\",\"2024-07-13 08:06:31\",\"BRIMO\",\"\",\"JIMMY RYNALDY HARTAN\",\"62201048609507.0\",\"60000000.0\",\"Escalated\",\"\",\"2024-07-13 00:30:34\",\"ATM/CRM BRI\",\"Investasi\",\"5221842178581812.0\",\"LCC-FBI\",\"90169643.0\",\"\",\"\",\"\",\"081361623563\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 093201000009562,', 'Nama Rekening: ALDIO HAIDAR SATRIO,', 'Waktu Transaksi: 2024-07-13 00:30:34,', 'Remark: NBMB JIMMY RYNALDY  TO ALDIO HAIDAR SATR,', 'Keterangan Tambahan: Saya merupakan korban penipuan online. Karena saya diminta transaksi berkali2 dalam jumlah yang besar.\",\"\",\"\",\"\",\"2024-08-05 00:59:55\",\"\",\"\",\"In Progress\",\"\",\"2024-07-15 18:47:12\",\"2024-07-15 08:31:49\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-15 09:33:12\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB708706617898\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053921870\",\"INVALID_FIELD:Foreign key external ID: jey3866 not found for field SCC_cifno__c in entity Account\"', 'JEY3866,\"TTB000053921877\",\"8708\",\"Penipuan Kepada Nasabah BRI dan Transaksi dilakukan Sendiri oleh Nasabah\",\"2024-07-13 08:07:51\",\"BRIMO\",\"\",\"JIMMY RYNALDY HARTAN\",\"62201048609507.0\",\"56637500.0\",\"Escalated\",\"\",\"2024-07-13 00:31:33\",\"ATM/CRM BRI\",\"Investasi\",\"5221842178581812.0\",\"LCC-FBI\",\"90169643.0\",\"\",\"\",\"\",\"081361623563\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 093201000009562,', 'Nama Rekening: ALDIO HAIDAR SATRIO,', 'Waktu Transaksi: 2024-07-13 00:31:33,', 'Remark: NBMB JIMMY RYNALDY  TO ALDIO HAIDAR SATR,', 'Keterangan Tambahan: Saya merupakan korban penipuan online yang dilakukan krena diminta transfer berkali kali\",\"\",\"\",\"\",\"2024-08-05 00:59:55\",\"\",\"\",\"In Progress\",\"\",\"2024-07-15 18:47:39\",\"2024-07-15 08:31:49\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-15 09:35:01\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB708706955202\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053921877\",\"INVALID_FIELD:Foreign key external ID: jey3866 not found for field SCC_cifno__c in entity Account\"', 'JEY3866,\"TTB000053922003\",\"8708\",\"Penipuan Kepada Nasabah BRI dan Transaksi dilakukan Sendiri oleh Nasabah\",\"2024-07-13 08:13:11\",\"BRIMO\",\"Fraud / Suspicious Transactions\",\"JIMMY RYNALDY HARTAN\",\"62201048609507.0\",\"90615000.0\",\"Escalated\",\"\",\"2024-07-12 21:22:49\",\"ATM/CRM BRI\",\"Info Fiktif - Mengaku Seseorang\",\"5221842178581812.0\",\"LCC-FBI\",\"90169643.0\",\"\",\"\",\"\",\"081361623563\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK MANDIRI,', 'Nomor Rekening: 1080027615120,', 'Nama Rekening: RIZIEQ MALIK                  ,', 'Waktu Transaksi: 2024-07-12 21:22:49,', 'Remark: BFST1080027615120     NBMB:BMRIIDJA,', 'Keterangan Tambahan: Saya merupakan korban penipuan online. Saya diminta transfer berkali kali dalam jumlah besar.\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:59:55\",\"\",\"Servicing\",\"In Progress\",\"\",\"2024-07-15 18:48:07\",\"2024-07-15 08:31:50\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-15 08:51:37\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB708708324787\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053922003\",\"INVALID_FIELD:Foreign key external ID: jey3866 not found for field SCC_cifno__c in entity Account\"', 'JEY3866,\"TTB000053922022\",\"8708\",\"Penipuan Kepada Nasabah BRI dan Transaksi dilakukan Sendiri oleh Nasabah\",\"2024-07-13 08:14:40\",\"BRIMO\",\"Fraud / Suspicious Transactions\",\"JIMMY RYNALDY HARTAN\",\"62201048609507.0\",\"52500000.0\",\"Escalated\",\"\",\"2024-07-12 20:07:15\",\"ATM/CRM BRI\",\"Info Fiktif - Mengaku Seseorang\",\"5221842178581812.0\",\"LCC-FBI\",\"90169643.0\",\"\",\"\",\"\",\"081361623563\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 093201000009562,', 'Nama Rekening: ALDIO HAIDAR SATRIO,', 'Waktu Transaksi: 2024-07-12 20:07:15,', 'Remark: NBMB JIMMY RYNALDY  TO ALDIO HAIDAR SATR,', 'Keterangan Tambahan: Saya merupakan korban penipuan online. Saya diminta transfer berkali2 dalam jumlah besar.\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:59:55\",\"\",\"Servicing\",\"In Progress\",\"\",\"2024-07-15 18:48:38\",\"2024-07-15 08:31:50\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-15 09:35:01\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB708708704334\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053922022\",\"INVALID_FIELD:Foreign key external ID: jey3866 not found for field SCC_cifno__c in entity Account\"', 'JEY3866,\"TTB000053922033\",\"8708\",\"Penipuan Kepada Nasabah BRI dan Transaksi dilakukan Sendiri oleh Nasabah\",\"2024-07-13 08:15:53\",\"BRIMO\",\"Fraud / Suspicious Transactions\",\"JIMMY RYNALDY HARTAN\",\"62201048609507.0\",\"62895000.0\",\"Escalated\",\"\",\"2024-07-12 20:33:37\",\"ATM/CRM BRI\",\"Info Fiktif - Mengaku Seseorang\",\"5221842178581812.0\",\"LCC-FBI\",\"90023326.0\",\"\",\"\",\"\",\"081361623563\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 093201000009562,', 'Nama Rekening: ALDIO HAIDAR SATRIO,', 'Waktu Transaksi: 2024-07-12 20:33:37,', 'Remark: NBMB JIMMY RYNALDY  TO DHIMAS PRASETIA T,', 'Keterangan Tambahan: Saya merupakan korban penipuan online. Saya diminta transfer berkali-kali dalam jumlah yang besar.\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 00:59:56\",\"\",\"Servicing\",\"In Progress\",\"\",\"2024-07-15 18:49:06\",\"2024-07-15 08:32:09\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-15 09:35:01\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB708709022229\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053922033\",\"INVALID_FIELD:Foreign key external ID: jey3866 not found for field SCC_cifno__c in entity Account\"', '1895121000355378,\"TTB000053951423\",\"1027\",\"CERIA - Permintaan  Surat Lunas Kolektibilitas\",\"2024-07-15 10:23:10\",\"Surat\",\"Request\",\"HENDIEK SOEHARIYO JAYA\",\"\",\"0.0\",\"Escalated\",\"\",\"\",\"\",\"Permintaan\",\"1895121000355378.0\",\"LCC-FBI\",\"90116924.0\",\"\",\"\",\"Taufan Rachman Putra\",\"081330408190\",\"\",\"\",\"10.0\",\"90069116\",\"KKD-OPS-LKK\",\"Taufan Rachman Putra\",\"terima dio dari Service & Contact Center Division / Customer Protection Department', 'Perihal : Pengaduan Perbaikan iDeb SLIK a.n Hendiek Soehariyo Jaya', 'Laporan Pengaduan No. P240701029 a.n. Hendiek Soehariyo Jaya, Tanggal 10 Juli 2024', '1. berdasarkan No telp 081330408190, ybs memiliki pinjaman ceria dengan ID ceria : 1895121000355378 ,rekening BRI : 202901002477506', '2. untuk NIK 6372051709750002 : tidak ada CC BRI tidak ada Ceria', 'dear team DDB untuk lakukan pengecekan bener apa enggak ybs mengajukan ceria. terima kasih', '==============================================================================', 'mhn untuk di proses. thanks\",\"KTA - Digital\",\"1972-11-08 18:11:56\",\"\",\"2024-08-05 09:08:26\",\"\",\"Loans\",\"In Progress\",\"\",\"2024-08-05 09:06:21\",\"2024-07-31 11:02:34\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Taufan Rachman Putra .\",\"\",\"False\",\"Ratna Purwaningsih\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"CDD\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"23.0\",\"\",\"2024-08-05 09:08:26\",\"2024-07-15 15:29:17\",\"2024-07-23 14:42:19\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053951423\",\"INVALID_FIELD:Foreign key external ID: 1895121000355378 not found for field SCC_cifno__c in entity Account\"', '1895121000355378,\"TTB000053951566\",\"6100\",\"Permintaan  Surat Lunas Kolektibilitas Lancar untuk Kartu Kredit\",\"2024-07-15 10:26:05\",\"Surat\",\"\",\"Hendiek Soehariyo Jaya\",\"\",\"0.0\",\"Escalated\",\"\",\"\",\"\",\"Permintaan\",\"1895121000355378.0\",\"LCC-FBI\",\"90116924.0\",\"\",\"\",\"Taufan Rachman Putra\",\"081330408190\",\"\",\"\",\"4.0\",\"90069116\",\"KKD-OPS-LKK\",\"Taufan Rachman Putra\",\"terima dio dari Service & Contact Center Division / Customer Protection Department', 'Perihal : Pengaduan Perbaikan iDeb SLIK a.n Hendiek Soehariyo Jaya', 'Laporan Pengaduan No. P240701029 a.n. Hendiek Soehariyo Jaya, Tanggal 10 Juli 2024', 'Menindaklanjuti laporan Sdr. Hendiek Soehariyo Jaya pada laman Aplikasi Portal Perlindungan Konsumen (APPK) yang merupakan sistem', 'layanan konsumen berkaitan dengan penanganan pengaduan nasabah dan penyelesaian sengketa yang dikelola oleh Direktorat Pelayanan', 'Konsumen, Pemeriksaan Pengaduan dan EPK Regional Otoritas Jasa Keuangan perihal pengaduan data iDeb SLIK, berikut ini adalah substansi', 'pengaduan yang disampaikan oleh Pelapor:', '1. Sdr. Hendiek Soehariyo Jaya merupakan pemilik NIK 3578021709750004.', '2. Berdasarkan dokumen iDeb SLIK yang dilampirkan oleh Sdr. Hendiek Soehariyo Jaya ditemukan pinjaman Debitur BRI a.n. Hendiek', 'Soehariyo Jaya dengan nomor rekening 09705401502990862322 (BRI Kas KPO) yang terelasi dengan NIK 6372051709750002.', '3. Sdr. Hendiek Soehariyo Jaya merasa keberatan dan meminta untuk dilakukan rahabilitasi iDeb SLIK.', 'Sehubungan dengan hal tersebut, maka kami mohon bantuan Divisi terkait yang mencakup:', '1. Melakukan investigasi pengaduan dan tindak lanjut sesuai ketentuan guna memastikan terdapat penyalahgunaan data pribadi/ kesalahan', 'penginputan data.', '2. Melakukan tindak lanjut perbaikan iDeb SLIK Sdr. Hendiek Soehariyo Jaya apabila benar ditemukan adanya kesalahan penginputan data', 'oleh Bank.', '3. Menyusun kronologis dan hasil tindak lanjut penyelesaian pengaduan yang telah dilakukan.', '4. Memberikan penjelasan serta solusi atas permasalahan yang disampaikan dan memberikan surat tanggapan penyelesaian pengaduan', 'kepada Pelapor.', '5. Mengirimkan dokumen pendukung penyelesaian pengaduan dan bukti hasil investigasi lainnya', 'Mengingat adanya batas waktu yang diberikan kepada Bank BRI untuk memberikan tanggapan, maka kami mohon bantuan Divisi Terkait untuk', 'dapat menindaklanjuti permohonan yang dimaksud serta mengirimkan informasi dan dokumen tindak lanjut pengaduan kepada Dispute', 'Resolution Team (orgeh 70429403) selambatnya tanggal 17 Juli 2024.\",\"\",\"\",\"\",\"2024-08-05 01:00:30\",\"\",\"\",\"In Progress\",\"\",\"2024-07-30 13:37:36\",\"2024-07-31 11:02:17\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"Ratna Purwaningsih\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"CDD\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"23.0\",\"\",\"2024-07-31 11:28:07\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0053951566\",\"INVALID_FIELD:Foreign key external ID: 1895121000355378 not found for field SCC_cifno__c in entity Account\"', 'TPH4066,\"TTB000053955104\",\"8410\",\"Kegagalan TAC / Transfer Lainnya Lewat BRI\",\"2024-07-15 12:07:28\",\"BRIMO\",\"Complaint - Transaction\",\"TOGI LILIYUS PANJAITAN\",\"107901002769561.0\",\"7500000.0\",\"Escalated\",\"\",\"2024-07-14 18:44:49\",\"ATM BRI\",\"Transfer\",\"5326590002763593.0\",\"LCC-ISSUER\",\"\",\"\",\"\",\"\",\"085375263341\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 412101032348507,', 'Nama Rekening: AWALUDIN,', 'Waktu Transaksi: 2024-07-14 18:44:49,', 'Remark: NBMB TOGI LILIYUS P TO AWALUDIN,', 'Keterangan Tambahan: Penipuan pembelian yang bersangkutan mengaku suami adik saya Dengan berkomunikasi melalui wa dengan no hp 0812 9287 7324. Yaitu menawarkan saya barang elektronik barang sitaan orang pajak.\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 01:00:34\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-07-23 15:25:07\",\"2024-07-24 10:05:49\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-25 00:00:00\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB709506717005\",\"Non Call\",\"1.0\",\"Case Migration\",\"TTB0053955104\",\"INVALID_FIELD:Foreign key external ID: tph4066 not found for field SCC_cifno__c in entity Account\"', 'KRZ1814,\"TTB000053978349\",\"8410\",\"Kegagalan TAC / Transfer Lainnya Lewat BRI\",\"2024-07-16 12:49:35\",\"BRIMO\",\"Complaint - Transaction\",\"KRISTINA WENILAWATI RAKAM\",\"94801024621539.0\",\"120000.0\",\"Escalated\",\"\",\"2024-07-16 12:42:29\",\"ATM BRI\",\"Transfer\",\"6013010232474230.0\",\"LCC-ISSUER\",\"\",\"\",\"\",\"\",\"08161489742\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 538801009378532,', 'Nama Rekening: ROSMAULI SIHOMBING,', 'Waktu Transaksi: 2024-07-16 12:42:29,', 'Remark: NBMB KRISTINA WENIL TO ROSMAULI SIHOMBIN,', 'Keterangan Tambahan: Saya salah melakukan transfer ke rekening tersebut (salah input no rekening).  Mohon untuk ditangguhkan dan ditarik kembali sejumlah dana yang saya transfer sebesar Rp.120.000\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 01:01:08\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-07-17 12:44:05\",\"2024-07-17 12:44:05\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-17 13:08:37\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB709947291983\",\"Non Call\",\"1.0\",\"Case Migration\",\"TTB0053978349\",\"INVALID_FIELD:Foreign key external ID: krz1814 not found for field SCC_cifno__c in entity Account\"', 'W693666,\"TTB000053978555\",\"8411\",\"Salah Transfer antar BRI\",\"2024-07-16 12:57:13\",\"BRIMO\",\"\",\"WURTI HERLINA\",\"376101006678539.0\",\"390000.0\",\"Escalated\",\"\",\"2024-07-16 12:11:07\",\"ATM BRI\",\"Transfer\",\"6013012046066103.0\",\"03782 -- UNIT SUBANG KOTA\",\"90152005.0\",\"\",\"\",\"\",\"081391365482\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 378701003037506,', 'Nama Rekening: ARVINDA SASMALIANI,', 'Waktu Transaksi: 2024-07-16 12:11:07,', 'Remark: EDC  WURTI HERLINA   TO ELSA NUR APRIYA,', 'Keterangan Tambahan: Kesalahan memasukan 1 digit nomor rekening seharusnya 378701003037506\",\"\",\"\",\"\",\"2024-08-05 01:01:08\",\"\",\"\",\"In Progress\",\"\",\"2024-07-22 14:20:00\",\"2024-07-23 08:24:04\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"2024-07-23 10:57:33\",\"2024-07-23 10:57:33\",\"2024-07-22 14:20:00\",\"\",\"\",\"Notes\",\"NBMB709950276359\",\"Non Call\",\"5263.0\",\"Case Migration\",\"TTB0053978555\",\"INVALID_FIELD:Foreign key external ID: w693666 not found for field SCC_cifno__c in entity Account\"', 'DDLU594,\"TTB000053995703\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-16 19:09:59\",\"Phone\",\"\",\"DWI DARWATI\",\"671901028885534.0\",\"200000.0\",\"Closed\",\"2024-07-30 21:00:58\",\"2024-07-13 00:00:00\",\"\",\"\",\"6013010078126993.0\",\"ADMIN QSC\",\"\",\"\",\"dwidarwati008@gmail.com\",\"M. Linaldi Aqmal\",\"083144788505\",\"\",\"\",\"5.0\",\"JKT00070\",\"CCT-MRS-JKT\",\"M. Linaldi Aqmal\",\"Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500800438286', 'Sebesar   : Rp. 200.000', 'Sebanyak         : 1 kali', 'Gagal di trx yang ke', 'Tanggal   : 2024-07-13', 'Pukul   : +- 14.30 wib', 'Keterangan / Respon : Sukses', 'Keterangan tambahan : namun tidak masuk brizzi', '( lampirkan 10 mutasi terakhir kartu BRIZZI )', '2024-07-1314:57:24671901028885534TOPD6013500800438286NBMB6013010078126993 ESB:NBMB:0200600T:7088394047672200.0000NA8888284\",\"\",\"\",\"\",\"2024-07-30 21:00:59\",\"\",\"\",\"Missed\",\"\",\"2024-07-22 14:27:47\",\"2024-07-24 00:00:00\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"M. Linaldi Aqmal .\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"00229 -- Kas Kanpus\",\"\",\"Gagal\",\"KAS KANPUS\",\"Kas Kanpus\",\"14601.0\",\"\",\"\",\"2024-07-25 00:00:00\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"2301.0\",\"Case Migration\",\"TTB0053995703\",\"INVALID_FIELD:Foreign key external ID: ddlu594 not found for field SCC_cifno__c in entity Account\"', '6013012278804015,\"TTB000054019679\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-17 12:27:15\",\"Sabrina\",\"Complaint - Transaction\",\"YONO JUNAEDI\",\"138101001209539.0\",\"359.0\",\"Closed\",\"2024-07-31 10:05:32\",\"2024-07-17 00:00:00\",\"IB\",\"Top UP\",\"6013012278804015.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Marlinda Artika Sari\",\"081392700069\",\"\",\"\",\"5.0\",\"00176742\",\"\",\"Marlinda Artika Sari\",\"Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013 5001 3014 8225', 'Sebesar   : Rp. 359.000,-', 'Sebanyak         : 2 kali', 'Gagal di trx yang ke Tanggal   : 17 Juli 2024 Pukul   11.36 dan 11.42 wib', 'Keterangan / Respon : (yang diterima dari mesin EDC,ATM,IB )', 'Keterangan tambahan : Failed\",\"Electronic Money\",\"\",\"\",\"2024-07-31 10:05:32\",\"\",\"Servicing\",\"Missed\",\"\",\"2024-07-19 11:42:09\",\"2024-07-31 07:18:12\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"BRICare Administrator\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"00029 -- Yogyakarta Cik Ditiro\",\"\",\"Gagal\",\"Yogyakarta\",\"Yogyakarta Cik Ditiro\",\"\",\"\",\"\",\"2024-07-31 09:44:55\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054019679\",\"INVALID_FIELD:Foreign key external ID: 6013012278804015 not found for field SCC_cifno__c in entity Account\"', 'MAFG496,\"TTB000054030040\",\"8410\",\"Kegagalan TAC / Transfer Lainnya Lewat BRI\",\"2024-07-17 18:40:03\",\"BRIMO\",\"\",\"MUHAMMAD AL FATIH\",\"340901041627535.0\",\"1000000.0\",\"Escalated\",\"\",\"2024-07-17 14:00:43\",\"ATM BRI\",\"Transfer\",\"6013013636366705.0\",\"LCC-ISSUER\",\"\",\"\",\"\",\"\",\"08985409588\",\"\",\"\",\"10.0\",\"\",\"\",\"\",\"Bank Tujuan: BANK BRI,', 'Nomor Rekening: 564801052883538,', 'Nama Rekening: ANDIKA RAMADANA,', 'Waktu Transaksi: 2024-07-17 14:00:43,', 'Remark: NBMB MUHAMMAD AL FA TO ANDIKA RAMADANA,', 'Keterangan Tambahan: Saya salah kirim kak, baru sadar setelah Cek mutasi rekening.\",\"\",\"\",\"\",\"2024-08-05 01:10:09\",\"\",\"\",\"In Progress\",\"\",\"2024-07-18 11:40:48\",\"2024-07-18 06:47:46\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-19 00:00:00\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB710497671088\",\"Non Call\",\"1.0\",\"Case Migration\",\"TTB0054030040\",\"INVALID_FIELD:Foreign key external ID: mafg496 not found for field SCC_cifno__c in entity Account\"', 'TCW7291,\"TTB000054038160\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-18 09:28:57\",\"SMS\",\"\",\"tumino\",\"668401032591536.0\",\"150000.0\",\"Escalated\",\"\",\"2024-07-13 00:00:00\",\"\",\"\",\"6013012208128725.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Nurul Amalia Zahra\",\"081227437147\",\"\",\"\",\"5.0\",\"00346220\",\"03109 -- UNIT SUMPIUH GOMBONG\",\"Nurul Amalia Zahra\",\"Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500181878027', 'Sebesar   : Rp. 150000', 'Sebanyak         : 1  kali', 'Gagal di trx yang ke Tanggal   : 13 Juli 2024', 'Pukul   : 13:04:40wib', 'Keterangan / Respon : (yang diterima dari mesin EDC,ATM,IB )', 'Keterangan tambahan : gagal to up brizzi namun saldo sudah berkurang', '( lampirkan 10 mutasi terakhir kartu BRIZZI )\",\"\",\"\",\"\",\"2024-08-05 01:10:25\",\"\",\"\",\"In Progress\",\"\",\"\",\"2024-07-19 07:12:07\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"03109 -- UNIT SUMPIUH GOMBONG\",\"\",\"Gagal\",\"Yogyakarta\",\"Gombong\",\"4591.0\",\"\",\"\",\"2024-07-19 09:40:11\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054038160\",\"INVALID_FIELD:Foreign key external ID: tcw7291 not found for field SCC_cifno__c in entity Account\"', 'YY25736,\"TTB000054039280\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-18 10:06:20\",\"SMS\",\"\",\"yosua sinaga\",\"66001017607502.0\",\"400000.0\",\"Escalated\",\"\",\"2024-07-17 00:00:00\",\"IB\",\"Top UP\",\"6013010856545125.0\",\"ADMIN QSC\",\"\",\"\",\"iyossinaga98@gmail.com\",\"Fajar\",\"081239023511\",\"\",\"\",\"5.0\",\"00331682\",\"\",\"Fajar\",\"Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500124557878', 'Sebesar   : Rp.400.000', 'Sebanyak         : 1  kali', 'Gagal di trx yang ke Tanggal   : 17 Juli 2024', 'Pukul   : 01:02 wib', 'Keterangan / Respon : top up berhasil saldo rekening terdebet namun saldo blm bertambah', 'Keterangan tambahan : ch ybs adalah counter yang melayanin transaksi top up kartu BRIZZI di tol jadi ch bukan yang memiliki kartu BRIZZI tersebut.', 'mohon bantuan nya untuk melakukan pengembalian dana ke rekening ch dikarenakan ch sudah melakukan top up 2x ke kartu BRIZZI nya', 'terima kasih\",\"\",\"\",\"\",\"2024-08-05 01:10:28\",\"\",\"\",\"In Progress\",\"\",\"2024-07-31 09:12:47\",\"2024-08-01 06:54:26\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"Email\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"\",\"\",\"\",\"2024-08-01 08:38:32\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054039280\",\"INVALID_FIELD:Foreign key external ID: yy25736 not found for field SCC_cifno__c in entity Account\"', '6013500611334989,\"TTB000054047633\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-18 15:02:06\",\"SMS\",\"\",\"ferdo wijaya\",\"111111111111111.0\",\"500000.0\",\"Escalated\",\"\",\"2024-06-22 00:00:00\",\"\",\"Top UP\",\"6013500611334989.0\",\"ADMIN QSC\",\"\",\"\",\"ferdowijaya899@gmail.com\",\"Dhimas Ujung Prakosa\",\"081369638019\",\"\",\"\",\"5.0\",\"00362207\",\"01111 -- KCP SETURAN PLAZA\",\"Dhimas Ujung Prakosa\",\"nasabah gagal top up Brizzi dengan nomor 6013 5006 1133 4989 melalui aplikasi DANA dengan nominal Rp. 500.000 pada tanggal 22 Juni 2024.', 'pada aplikasi Dana keterangan sudah berhasi dan saldo terdebet tetapi di saldo Brizzi belum bertambah.\",\"\",\"\",\"\",\"2024-08-05 07:14:57\",\"\",\"\",\"In Progress\",\"\",\"\",\"2024-08-05 07:14:57\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"BRICare Administrator\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"\",\"Email\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"01111 -- KCP SETURAN PLAZA\",\"\",\"Gagal\",\"Yogyakarta\",\"Yogyakarta Cik Ditiro\",\"3453.0\",\"\",\"\",\"\",\"2024-08-04 08:21:23\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054047633\",\"INVALID_FIELD:Foreign key external ID: 6013500611334989 not found for field SCC_cifno__c in entity Account\"', '6013501815083091,\"TTB000054067752\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-19 15:01:47\",\"Walk-In\",\"Complaint - Transaction\",\"KORES BE NADEAN\",\"999999999999999.0\",\"300000.0\",\"Escalated\",\"\",\"2024-07-19 00:00:00\",\"\",\"\",\"6013501815083091.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Meilania Julva Ayunsetyani\",\"081363352014\",\"\",\"\",\"5.0\",\"00346251\",\"00251 -- Muntilan\",\"Meilania Julva Ayunsetyani\",\"Nasabah mengajukan pemindahan saldo kartu BRIZZI', 'Dari Nomor kartu brizzi yang lama : 6013501815083091', 'ke nomor kartu brizzi baru : 6013500184961556', 'sebesar  : Rp 300.000', 'Alasan  : BRIZZI RUSAK DAN TIDAK TERBACA', 'Keterangan tambahan : KARTU LAMA NASABAH TIDAK TERBACA DAN UNTUK KARTU BRIZZI YANG LAMA MASIH ADA SALDO SEKITAR Rp 300.000', 'Jika kartu yang lama masih bisa terbaca lampirkan 10 mutasi transaksi kartu brizzi', '(Lampirkan foto/scan dari brizzi lama dan brizzi baru )', '(Apabila nasabah hubungi callcenter, sarankan datang ke uker terdekat untuk mengembalikan/mengganti kartu brizzi lama)\",\"Electronic Money\",\"\",\"\",\"2024-08-05 07:14:57\",\"\",\"Servicing\",\"In Progress\",\"\",\"\",\"2024-08-05 07:14:57\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"BRICare Administrator\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"2751.0\",\"\",\"\",\"\",\"2024-08-04 08:32:21\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054067752\",\"INVALID_FIELD:Foreign key external ID: 6013501815083091 not found for field SCC_cifno__c in entity Account\"', 'DP38433,\"TTB000054104199\",\"8412\",\"Nasabah BRI Gagal Transaksi Belanja di EDC BRI, Saldo Rekening Berkurang\",\"2024-07-22 08:53:16\",\"BRIMO\",\"\",\"DIDI HARYONO\",\"555801012219538.0\",\"1175034.0\",\"Escalated\",\"\",\"2024-07-20 18:18:53\",\"EDC BRI\",\"Transaksi\",\"6013011250767042.0\",\"LCC-ISSUER\",\"\",\"\",\"\",\"\",\"085368775299\",\"\",\"\",\"9.0\",\"\",\"\",\"\",\"Nama Merchant: suproni b irpan,', 'Waktu Transaksi: 2024-07-20 18:18:53,', 'Remark: BRIVA1164520003171378NBMBSUPRONI B IRPAN,', 'Keterangan Tambahan: Saya melakukan transaksi d briva dan salah satu input nomor briva terjadi kesalahan transfer dan saldo saya terpotong mohon bantuan nya #urgent\",\"\",\"\",\"\",\"2024-08-05 01:12:38\",\"\",\"\",\"In Progress\",\"\",\"2024-07-23 07:24:48\",\"2024-07-23 07:24:48\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-23 07:25:29\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB712206416576\",\"Non Call\",\"1.0\",\"Case Migration\",\"TTB0054104199\",\"INVALID_FIELD:Foreign key external ID: dp38433 not found for field SCC_cifno__c in entity Account\"', '4365020286794903,\"TTB000054114428\",\"7703\",\"Komplain Transaksi Kartu Kredit tidak di akui merchant Bank Lain\",\"2024-07-22 14:35:36\",\"Walk-In\",\"Fraud / Suspicious Transactions\",\"FEBRINA\",\"\",\"129000.0\",\"Escalated\",\"\",\"2024-06-30 00:00:00\",\"\",\"Sanggahan\",\"4365020286794903.0\",\"KKD-RMG-CHARGEBACK\",\"90131146.0\",\"\",\"aurelcherine@gmail.com\",\"Monica Angelia\",\"08816139996\",\"\",\"\",\"7.0\",\"00341989\",\"01338 -- KK BUNGALOW\",\"Monica Angelia\",\"Nasabah menyanggah tidak ada merasa melakukan transaksi di Itunes dengan nominal Rp 129.000. Sudah pernah melakukan komplain ke CALL CENTER BRI dan sudah dilakukan pergantian dari nomor kartu yg lama 4365-xxxx-xxxx-3301 namun masih tetap muncul di tagihan yang baru dengan nomor kartu yang baru 4365020286794903', 'Mohon bantuannya untuk ditindaklanjuti. TerimaKASIH\",\"Kartu Kredit\",\"\",\"\",\"2024-08-05 01:12:54\",\"\",\"Loans\",\"In Progress\",\"\",\"\",\"2024-08-02 14:19:01\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"Ade Mulki Erlangga .\",\"\",\"\",\"False\",\"Email\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"01338 -- KK BUNGALOW\",\"\",\"\",\"Medan\",\"Medan Putri Hijau\",\"3665.0\",\"\",\"2024-08-02 14:19:01\",\"\",\"2024-07-23 19:02:36\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"30.0\",\"Case Migration\",\"TTB0054114428\",\"INVALID_FIELD:Foreign key external ID: 4365020286794903 not found for field SCC_cifno__c in entity Account\"', 'FK03399,\"TTB000054171234\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-25 09:53:13\",\"Phone\",\"Complaint - Transaction\",\"FENNY DWI PUSPITA RI\",\"350501037043539.0\",\"50000.0\",\"Closed\",\"2024-08-01 11:21:27\",\"2024-07-25 00:00:00\",\"\",\"\",\"5221842184872858.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Pramelia Arinda Putri\",\"082210070063\",\"\",\"\",\"5.0\",\"SMG60080\",\"CCT-MRS-SMG\",\"Pramelia Arinda Putri\",\"#callterputus', 'if ch call back infokan pengiriman dokumen dan tt', 'Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013502000677531', 'Sebesar   : Rp.50.000', 'Sebanyak         : 2  kali', 'Gagal di trx yang ke Tanggal   : 25.07.2024  gagal trx di 1 dan 2', 'Pukul   : 09.30  wib', 'Keterangan / Respon : trx sukses', 'Keterangan tambahan : ch sudah  ncan nfc namun saldo tdk keluar - tdk menambah saldo brizzi', '( lampirkan 10 mutasi terakhir kartu BRIZZI )', '2024-07-2509:20:42350501037043539TOPU6013502000677531NBMB5221842184872858250.0000NA8888089', '2024-07-2509:20:54350501037043539TOPD6013502000677531NBMB5221842184872858250.0000NA8888569', 'Catatan: Data mutasi rekening ini adalah posisi data 2024-07-25 09:49:25\",\"Electronic Money\",\"\",\"\",\"2024-08-01 11:21:28\",\"\",\"Servicing\",\"Met\",\"\",\"2024-07-29 13:04:45\",\"2024-07-31 07:18:13\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Pramelia Arinda Putri .\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"00229 -- Kas Kanpus\",\"\",\"Gagal\",\"KAS KANPUS\",\"Kas Kanpus\",\"14502.0\",\"\",\"\",\"2024-07-31 10:15:36\",\"2024-08-01 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"2301.0\",\"Case Migration\",\"TTB0054171234\",\"INVALID_FIELD:Foreign key external ID: fk03399 not found for field SCC_cifno__c in entity Account\"', 'SEPEM38,\"TTB000054175764\",\"1020\",\"CERIA - Permintaan Perubahan Data Nasabah\",\"2024-07-25 12:06:49\",\"Phone\",\"Request\",\"SUGENGHARIYADI\",\"36101029937503.0\",\"0.0\",\"Closed\",\"2024-07-30 13:01:06\",\"2024-07-25 00:00:00\",\"\",\"Perubahan data\",\"1895121000359999.2\",\"DCE\",\"\",\"\",\"sugenghariyadi70@gmail.com\",\"Siti Mukodhimah\",\"081119242777\",\"\",\"\",\"10.0\",\"SMG65230\",\"CCT-CDS-SMG\",\"Siti Mukodhimah\",\"ch menghubungi menggunakan 0218518547', '========================', 'Nasabah mengajukan pergantian Nomor rekening auto debet CERIA', 'Dari (rekening lama) : 36101029937503', 'Menjadi (rekening baru) :  036101044883501', 'Alasan : no rekening lama sudah ditutup', 'Alamat email untuk kirim dokumen : sugenghariyadi70@gmail.com', 'Tolong bantuannya untuk tindak lanjut.', 'Dokumen yang dibutuhkan :', '(Screen Capture dari aplikasi CERIA (pada menu PROFILE) Buku tabungan tampak depan, Foto KTP nasabah, Surat pernyataan, dan foto selfie)', '====================', 'csr sudah saranakn send dokumen', 'a. Surat Pernyataan Perubahan', 'b.Buku tabungan tampak depan', 'c.Foto KTP', 'd.Foto Selfie', 'Mohon bantuannya. Terima kasih.\",\"KTA - Digital\",\"\",\"\",\"2024-07-30 13:01:06\",\"\",\"Loans\",\"Met\",\"\",\"2024-07-25 19:10:39\",\"2024-07-29 14:13:57\",\"\",\"CDS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Siti Mukodhimah .\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"00229 -- Kas Kanpus\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14503.0\",\"\",\"2024-07-29 14:39:51\",\"\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"10102.0\",\"Case Migration\",\"TTB0054175764\",\"INVALID_FIELD:Foreign key external ID: sepem38 not found for field SCC_cifno__c in entity Account\"', 'APW8957,\"TTB000054180830\",\"1038\",\"CERIA - Komplain Transaksi Tidak Diakui\",\"2024-07-25 15:05:17\",\"Phone\",\"Fraud / Suspicious Transactions\",\"AGENG RAHMATULLOH\",\"662801044099536.0\",\"0.0\",\"Closed\",\"2024-07-31 13:51:56\",\"2024-07-25 00:00:00\",\"\",\"Sanggahan\",\"1895121000018424.8\",\"DCE\",\"\",\"\",\"\",\"Afrilia Kartika\",\"085162965095\",\"\",\"\",\"10.0\",\"SMG65316\",\"CCT-CDS-SMG\",\"Afrilia Kartika\",\"Nasabah melaporkan mengklik link untuk kenaikan limit pada web, namun terjadi trx yang tidak dilakukan', 'Nominal dana trx : 1.869.605,00', 'Tanggal transaksi  : 2024-07-25', 'No HP Nasabah  : 085162965095', 'Nomor rekening yang terdaftar : 662801044099536', 'ID CERIA (lihat di Library) : 1895121000018425', 'Keterangan tambahan :', 'IC : 0216616372', '=======================================================', '103225072024-07-2512157513912-1721892056016purchaseBLIBLI CERIA90Rp 1.585.619,00Rp 1.869.605,00Liha', '======================================================', 'LAA ID', '077001010322507', 'Jumlah Nominal Bunga', 'Rp 283.986,00', 'Nama Merchant', 'BLIBLI CERIA', 'Jenis Transaksi', 'Purchase', 'Tanggal Transaksi', '2024-07-25', 'Jumlah Early Payoff Fee Saat Ini', 'Rp 47.569,00', 'Nominal Transaksi', 'Rp 1.585.619,00', 'Nominal Cicilan Perbulan', 'Rp 207.733,00', 'Merchant Ref ID', '12157513912-1721892056016', 'Jumlah Tenor dipilih', '9', 'Tanggal Posting', '2024-07-25', '==========================================================', '(Sarankan kirim dokumen : Surat pernyataan, Foto KTP, Foto KTP & Selfie)\",\"KTA - Digital\",\"\",\"\",\"2024-07-31 13:51:56\",\"\",\"Loans\",\"Met\",\"\",\"2024-07-25 15:47:47\",\"2024-07-29 00:00:00\",\"\",\"CDS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Claudia G Kumalasari\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"00229 -- Kas Kanpus\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14503.0\",\"\",\"2024-07-29 14:44:43\",\"2024-07-29 10:04:43\",\"2024-07-31 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"10102.0\",\"Case Migration\",\"TTB0054180830\",\"INVALID_FIELD:Foreign key external ID: apw8957 not found for field SCC_cifno__c in entity Account\"', 'MUEG847,\"TTB000054191421\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-26 08:42:12\",\"Phone\",\"Complaint - Transaction\",\"MUHAMMAD RAFI MAULAN\",\"182201008091501.0\",\"300000.0\",\"Closed\",\"\",\"2024-07-25 00:00:00\",\"\",\"\",\"5221842154094889.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Wahyu Arya Kusumadika\",\"081389931066\",\"\",\"\",\"5.0\",\"SMG60079\",\"CCT-MRS-SMG\",\"Wahyu Arya Kusumadika\",\"Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500418533650', 'Sebesar   : Rp. 300.000', 'Sebanyak         : 1 kali', 'Gagal di trx yang ke Tanggal   : 2024-07-25', 'Pukul   : 19:01  wib', 'Keterangan / Respon : brimo : saldo berkurang, blm masuk ke kartu brizzi', 'Keterangan tambahan :', '( lampirkan 10 mutasi terakhir kartu BRIZZI )', '===============================', '2024-07-2519:01:46182201008091501TOPU6013500418533650NBMB5221842154094889 ESB:NBMB:0200400T:7136177883892300.0000NA888\",\"Electronic Money\",\"\",\"\",\"2024-08-05 10:08:44\",\"\",\"Servicing\",\"Missed\",\"\",\"2024-08-03 16:51:47\",\"2024-08-05 07:14:57\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"BRICare Administrator\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"No Ticket Sudah Ada\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"00229 -- Kas Kanpus\",\"\",\"Gagal\",\"KAS KANPUS\",\"Kas Kanpus\",\"14502.0\",\"\",\"\",\"2024-08-05 09:39:10\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"2301.0\",\"Case Migration\",\"TTB0054191421\",\"INVALID_FIELD:Foreign key external ID: mueg847 not found for field SCC_cifno__c in entity Account\"', '6013501809454142,\"TTB000054192912\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-26 09:30:25\",\"WebChat\",\"\",\"SEPTIAN ARIANSAH\",\"\",\"0.0\",\"Closed\",\"2024-07-31 19:17:11\",\"\",\"\",\"\",\"6013501809454142.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Fadel\",\"082184846966\",\"\",\"\",\"5.0\",\"00341444\",\"00285 -- Teluk Betung\",\"Fadel\",\"Nasabah datang ke kantor bri dengan pengaduan kartu brizzi nasabah setelah di lakukan top up saldo masuk sebesar 150.000 namun kartu brizzi tersebut tidak dapat digunakan untuk tap e-toll nasabah meminta untuk saldo tersebut dipindahkan ke kartu brizzi baru,', 'no kartu brizzi lama: 6013 5018 0945 4142', 'no brizzi baru 6013 5001 8534 0909', 'nominal saldo : Rp.150.600', 'mohon untuk ditindaklanjuti perpindahan dananya terimakasih\",\"\",\"1970-01-05 05:50:44\",\"\",\"2024-07-31 19:17:11\",\"\",\"\",\"Met\",\"\",\"\",\"\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"Fadel Nur Muhammad\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"Tgl transaksi tidak valid\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"\",\"2785.0\",\"\",\"\",\"\",\"2024-07-31 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054192912\",\"INVALID_FIELD:Foreign key external ID: 6013501809454142 not found for field SCC_cifno__c in entity Account\"', 'HCZ4511,\"TTB000054195260\",\"8411\",\"Salah Transfer antar BRI\",\"2024-07-26 10:36:07\",\"SMS\",\"\",\"HENRIYATI BURERAN\",\"498201007055533.0\",\"4200000.0\",\"Escalated\",\"\",\"2024-07-23 00:00:00\",\"ATM BRI\",\"Transfer\",\"6013013066905915.0\",\"04982 -- UNIT ASIKI MERAUKE\",\"90152005.0\",\"\",\"\",\"Anicetus Kadek Agung Krisna\",\"081240600151\",\"\",\"\",\"10.0\",\"00272460\",\"00039 -- Kupang\",\"Anicetus Kadek Agung Krisna\",\"Nasabah melakukan transaksi transfer namun salah memasukkan nomor rekening tujuan :', 'Ke Bank : BRI', 'Nomor rekening yang salah 4982-01-014583-53-5', 'Pemilik rekening DEBI HERLIANTI', 'Sebesar : Rp.4.200.000', 'Sebanyak :1  kali', 'Yang gagal trx ke    : 1_', 'Tanggal : 23 JULI 2024', 'Pukul 13.36 WIB', 'Nomor rekening yang benar : 4982-01-014563-53-5', 'Pemilik rekening : HENRIYATI BAOK', 'Keterangan tambahan : Ybs melakukan transfer di agen BRILINK sebesar Rp.4.200.000, namun pada stelah di transfer nomor rekening nya salah . mohon bantuannya agar uang ybs dapat dikembalikan ke rekening ybs sebesar Rp.4.200.000, dengan NO.REK : 4982-01-007055-53-3 AN HENDRIYATI BURERAN. Mohon bantuannya terimakasih.', '#SANGATURGENT\",\"\",\"\",\"\",\"2024-08-05 01:15:49\",\"\",\"\",\"In Progress\",\"\",\"2024-07-31 11:02:20\",\"2024-07-31 11:04:16\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"Danny\\xa0Surya\\xa0N\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"\",\"2539.0\",\"\",\"2024-08-01 08:50:43\",\"2024-08-01 08:50:43\",\"2024-07-31 11:02:20\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"6458.0\",\"Case Migration\",\"TTB0054195260\",\"INVALID_FIELD:Foreign key external ID: hcz4511 not found for field SCC_cifno__c in entity Account\"', 'T795296,\"TTB000054196035\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-26 10:57:11\",\"Phone\",\"\",\"TOHARI SUTRISNO\",\"342801006295537.0\",\"500000.0\",\"Closed\",\"2024-08-02 15:48:18\",\"2024-07-25 23:00:00\",\"\",\"\",\"5221845039045737.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Fadli Pratama\",\"081253642488\",\"\",\"\",\"5.0\",\"00331886\",\"07129 -- UNIT TELUK LINGGA SANGATTA\",\"Fadli Pratama\",\"Nasabah mengajukan pemindahan saldo kartu BRIZZI', 'Dari Nomor kartu brizzi yang lama : 6013501302225817', 'ke nomor kartu brizzi baru : 6013501301721196', 'sebesar  : 500.000', 'Alasan  : KARTU BRIZZI LAMA RUSAK', 'Keterangan tambahan : BRIZZI LAMA TIDAK BISA DITRANSAKSIKAN LAGI', 'Jika kartu yang lama masih bisa terbaca lampirkan 10 mutasi transaksi kartu brizzi', '(Lampirkan foto/scan dari brizzi lama dan brizzi baru )', '(Apabila nasabah hubungi callcenter, sarankan datang ke uker terdekat untuk mengembalikan/mengganti kartu brizzi lama)\",\"\",\"1970-01-05 07:14:46\",\"\",\"2024-08-02 15:48:18\",\"\",\"\",\"Met\",\"\",\"\",\"2024-07-29 07:13:35\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Olivia Octavia .\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"07129 -- UNIT TELUK LINGGA SANGATTA\",\"\",\"Gagal\",\"Banjarmasin\",\"KANCA SANGATTA\",\"8600.0\",\"\",\"\",\"2024-07-29 10:45:25\",\"2024-08-02 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"2301.0\",\"Case Migration\",\"TTB0054196035\",\"INVALID_FIELD:Foreign key external ID: t795296 not found for field SCC_cifno__c in entity Account\"', 'MSXD959,\"TTB000054196233\",\"8602\",\"BRImo Permasalahan Sistem Error\",\"2024-07-26 11:05:33\",\"Phone\",\"\",\"MUSTOFA AL MUJAHIDIN\",\"330701016164532.0\",\"0.0\",\"Closed\",\"2024-07-30 09:52:30\",\"2024-07-26 00:00:00\",\"Aplikasi\",\"Lambat/koneksi terputus\",\"6013014032478730.0\",\"LCC-CCTCALL\",\"\",\"\",\"\",\"Vida Bakorinda\",\"085817856416\",\"\",\"\",\"1.0\",\"JKT00281\",\"CCT-MRS-JKT\",\"Vida Bakorinda\",\"#BRImo', '#VOIP', 'Ch komplain tidak bisa menggunakan BRImo', 'User id          : mustofaalmujahidin17', 'Sejak             : 2024-07-26', 'Jenis trx         : Update rekening', 'Error code yang diterima  : Tidak ada', 'Sebanyak              : 3 kali sudah mencapai batas dan disarankan mencoba esok hari', 'Status jaringan     : Normal', 'Merk Hp yang digunakan : Samsung', 'Versi aplikasi BRImo yang digunakan : Terupdated', 'Versi Android/iOS yang digunakan : 9', 'Nomor Ponsel yang terdaftar Brimo :       085817856416', 'Email yang terdaftar Brimo               :    mustofaalmujahidin17@gmail.com', 'Keterangan               : Update rekening nya sudah di lakukan sebanyak 3 kali muncul rekening yang baru, tapi ketika balik ke semua rekening mu, rekeningnya tidak muncul di aplikasi.', 'Jenis rekening simpanan yang di miliki yaitu simpedes', 'Sudah informasikan :', '(Copy status IB dari WBS)', '=========================', 'User Profile', 'Status: Enabled', 'Date Registered: 2024-07-26', 'Nama: MUSTOFA AL MUJAHIDIN', 'User ID: mustofaalmujahidin17', 'No Kartu: 601301******8730', 'No. Handphone: 0858****6416 (Active)', 'Smartphone Status: (Registered)', 'E-mail: must******************mail.com', 'Tangal Lahir: 1989-04-17', 'Alamat: JL BIMA CITRA 2 BLOK C3 NO 5 KEL.LAMBANG', 'Nama Gadis Ibu Kandung: RUKHANAH\",\"\",\"\",\"\",\"2024-07-30 09:52:30\",\"\",\"\",\"Missed\",\"\",\"2024-07-26 17:49:15\",\"\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"Vida Bakorinda .\",\"\",\"False\",\"Desak Putu W\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"00229 -- Kas Kanpus\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14601.0\",\"\",\"\",\"\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"4.0\",\"Case Migration\",\"TTB0054196233\",\"INVALID_FIELD:Foreign key external ID: msxd959 not found for field SCC_cifno__c in entity Account\"', 'NT79375,\"TTB000054200839\",\"8412\",\"Nasabah BRI Gagal Transaksi Belanja di EDC BRI, Saldo Rekening Berkurang\",\"2024-07-26 13:54:23\",\"BRIMO\",\"Complaint - Transaction\",\"NURJANNAH\",\"707001015703534.0\",\"350000.0\",\"Escalated\",\"\",\"2024-07-25 09:05:41\",\"EDC BRI\",\"Transaksi\",\"6013012296709535.0\",\"LCC-ISSUER\",\"\",\"\",\"\",\"\",\"085342061755\",\"\",\"\",\"9.0\",\"\",\"\",\"\",\"Nama Merchant: DANA,', 'Waktu Transaksi: 2024-07-25 09:05:41,', 'Remark: BRIVA88810087763291571WBNKDNID NURXXXXX,', 'Keterangan Tambahan: Sy melakukan kesalahan topup dana sebesar Rp. 350.000, yg seharusnya sy top up shopee, mohon bantuan agar dananya dpt dikembalikan, terimakasih\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 01:16:01\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-07-29 06:56:36\",\"2024-07-29 06:56:36\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Dewi Kartika Sari\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"4.0\",\"\",\"\",\"2024-07-29 06:57:31\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"NBMB713910812380\",\"Non Call\",\"1.0\",\"Case Migration\",\"TTB0054200839\",\"INVALID_FIELD:Foreign key external ID: nt79375 not found for field SCC_cifno__c in entity Account\"', 'EQ93386,\"TTB000054204335\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-26 16:00:28\",\"Phone\",\"Complaint - Transaction\",\"EKO AGUSTIAWAN\",\"56301029295507.0\",\"250000.0\",\"Closed\",\"2024-08-01 15:04:23\",\"2024-07-26 00:00:00\",\"\",\"\",\"6013500134828632.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Aulia natasya\",\"082255998382\",\"\",\"\",\"5.0\",\"00341887\",\"00563 -- KANCA SANGATTA\",\"Aulia natasya\",\"Nasabah gagal transaksi dengan kartu BRIZZI namun saldo berkurang', 'Nomor kartu BRIZZI : 6013500134828632', 'Sebesar   : Rp. 250.000', 'Sebanyak         : 1  kali', 'Gagal di trx yang ke Tanggal   : ke 1 tgl 26/07/2024', 'Pukul   : 17:05:55 wib', 'Keterangan / Respon : (yang diterima dari mesin EDC,ATM,IB )', 'Keterangan tambahan : pada saat nasabah melakukan aktivasi saldo deposit melalui brimo terjadi gangguan sehingga saldo di kartu Brizzi hilang. Mohon bantuan dan tindak lanjutnya terima kasih', '( lampirkan 10 mutasi terakhir kartu brizzi )\",\"Electronic Money\",\"1970-01-05 05:58:07\",\"\",\"2024-08-01 15:04:23\",\"\",\"Servicing\",\"Met\",\"\",\"\",\"2024-07-29 07:14:26\",\"\",\"\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"AULIA NATASYA .\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"\",\"\",\"Gagal\",\"\",\"\",\"3004.0\",\"\",\"\",\"2024-07-29 13:50:38\",\"2024-08-01 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"2301.0\",\"Case Migration\",\"TTB0054204335\",\"INVALID_FIELD:Foreign key external ID: eq93386 not found for field SCC_cifno__c in entity Account\"', 'AVEC739,\"TTB000054206135\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-26 17:28:09\",\"Walk-In\",\"Complaint - Transaction\",\"AGATA NINDA PUTRI\",\"51801020379503.0\",\"1000000.0\",\"Closed\",\"2024-07-30 14:27:49\",\"2024-07-11 00:00:00\",\"\",\"\",\"5221843119645005.0\",\"ADMIN QSC\",\"\",\"\",\"k6376@corp.bri.co.id\",\"Dian Meyla Cahya Biratri\",\"089680524370\",\"\",\"\",\"5.0\",\"00344479\",\"\",\"Dian Meyla Cahya Biratri\",\"Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500443658613', 'Sebesar   : Rp. 1.000.000', 'Sebanyak         : 4 kali', 'Gagal di trx yang ke : 1,2,3', 'Tanggal   :11 JULI 2024', 'Pukul   : 09.05 wib', 'Keterangan / Respon : (yang diterima dari mesin EDC,ATM,IB ) transaksi gagal', 'Keterangan tambahan : nasabah transaksi topup brizzi melalui brimo sejumlah Rp.1.000.000 sebanyak 3 kali dengan keterangan transaksi gagal, saat topup ke 4 kalinya keterangan berhasil akan tetapi saldo terdebet 2 kali', '( lampirkan 10 mutasi terakhir kartu BRIZZI )\",\"Electronic Money\",\"1970-01-05 06:41:19\",\"\",\"2024-07-30 14:27:49\",\"\",\"Servicing\",\"Met\",\"\",\"\",\"2024-07-29 07:14:29\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Dian Meyla Cahya Biratri .\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"Email\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"06376 -- UNIT DAU MALANG SUKARNO H\",\"\",\"Gagal\",\"KANWIL MALANG\",\"KC MALANG SOEKARNO HATTA\",\"\",\"\",\"\",\"2024-07-29 14:15:41\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054206135\",\"INVALID_FIELD:Foreign key external ID: avec739 not found for field SCC_cifno__c in entity Account\"', 'PV75054,\"TTB000054207168\",\"8411\",\"Salah Transfer antar BRI\",\"2024-07-26 18:21:19\",\"SMS\",\"\",\"PENAMPUNGAN DANA EDC\",\"63601015789507.0\",\"100000.0\",\"Closed\",\"2024-08-02 14:09:15\",\"2024-07-26 00:00:00\",\"ATM BRI\",\"Transfer\",\"6013013078494668.0\",\"00359 -- Jakarta Gatot Subroto\",\"90136590.0\",\"\",\"\",\"Ruth Chyntia Manurung\",\"082376951975\",\"\",\"\",\"10.0\",\"00351277\",\"\",\"Ruth Chyntia Manurung\",\"Nasabah melakukan transaksi transfer namun salah memasukkan nomor rekening tujuan : BRIVA DENGAN KODE TUJUAN87875072024024958', 'Ke Bank : BRI', 'Nomor rekening yang salah : 87875072024024956', 'SALAH PEMBAYARAN BRIVA PIJAR YANG SEHARUSNYA KEPAA AN ROSIDA SIMANJUNTAK DGN NO BRIVA 87875072024024958 SALAH TRANSKASI KE PIHAK LAIN AN AYUNDA PRASETIA LUBIS NO BRIVA87875072024024956', 'MOHON UNTUK TINDAK LANJUT #URGENT\",\"\",\"1970-01-05 06:46:41\",\"\",\"2024-08-02 14:09:15\",\"\",\"\",\"Met\",\"\",\"2024-07-30 16:35:47\",\"2024-07-31 10:07:49\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Mileni Arta .\",\"\",\"False\",\"Danny\\xa0Surya\\xa0N\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"00636 -- KC PERDAGANGAN\",\"\",\"\",\"Medan\",\"KC PERDAGANGAN\",\"\",\"\",\"2024-07-31 10:07:49\",\"\",\"2024-07-30 16:35:47\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2855.0\",\"Case Migration\",\"TTB0054207168\",\"INVALID_FIELD:Foreign key external ID: pv75054 not found for field SCC_cifno__c in entity Account\"', 'MUKF026,\"TTB000054207839\",\"8411\",\"Salah Transfer antar BRI\",\"2024-07-26 19:06:48\",\"Walk-In\",\"Request\",\"MUHAMAD IRFAN\",\"621201028987532.0\",\"3000000.0\",\"Closed\",\"2024-07-31 22:48:16\",\"2024-07-25 00:00:00\",\"ATM BRI\",\"Transfer\",\"6013010213881569.0\",\"LCC-CRC\",\"90136590.0\",\"\",\"\",\"ZUHUD FATCHUR ROCHMAN\",\"085745397260\",\"\",\"\",\"10.0\",\"00345568\",\"\",\"ZUHUD FATCHUR ROCHMAN\",\"Nasabah melakukan transaksi transfer namun salah memasukkan nomor rekening tujuan :', 'Ke Bank : BRI', 'Nomor rekening yang salah : 621101018924532', 'Pemilik rekening : RISKI WULANDARI', 'Sebesar : Rp. 3.000.000', 'Sebanyak : 1 kali', 'Yang gagal trx ke    : 1', 'Tanggal : 25 JULI 2024', 'Pukul : 01:12:36 WIB', 'Nomor rekening yang benar : 621201038924532', 'Pemilik rekening : RATNA DEWI SANTI', 'Keterangan tambahan : NASABAH SALAH MEMASUKAN NOMER REKENING YANG SEHARUSNYA MILIK KAKAKNYA AN RATNA DEWI SANTI DENGAN NOMER REKENING 621201038924532 SALAH KE REKENING 621101018924532 DARI RISKI WULANDARI\",\"Transaction Solution\",\"1970-01-05 06:59:28\",\"\",\"2024-07-31 22:48:16\",\"\",\"Transaction Banking\",\"Met\",\"\",\"\",\"2024-07-31 19:23:30\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"True\",\"\",\"\",\"ZUHUD FATCHUR ROCHMAN .\",\"\",\"False\",\"Danny\\xa0Surya\\xa0N\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"06237 -- UNIT SUKOWONO JEMBER\",\"\",\"\",\"KANWIL MALANG\",\"Jember\",\"\",\"\",\"\",\"\",\"2024-07-31 19:21:52\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"3.0\",\"Case Migration\",\"TTB0054207839\",\"INVALID_FIELD:Foreign key external ID: mukf026 not found for field SCC_cifno__c in entity Account\"', '8812345678,\"TTB000054213806\",\"8708\",\"Penipuan Kepada Nasabah BRI dan Transaksi dilakukan Sendiri oleh Nasabah\",\"2024-07-27 07:49:15\",\"Email\",\"Fraud / Suspicious Transactions\",\"Poermintojo\",\"1234567654.0\",\"40000000.0\",\"Closed\",\"2024-08-01 00:00:00\",\"2024-07-02 00:00:00\",\"ATM/CRM BRI\",\"Info Fiktif - Mengaku Seseorang\",\"8812345678.0\",\"LCC-FBI\",\"90135387.0\",\"\",\"\",\"SUMIYATI\",\"\",\"\",\"\",\"10.0\",\"90135202\",\"ADMIN QSC\",\"SUMIYATI\",\"#CRemail', 'menerima surat dari BANK BCA', 'PASS : BCA072024', 'dengan no surat:   18343/DSC/VII/2024', 'Perihal: Permohonan Investigasi dan Pengembalian Dana Atas Rekening Terindikasi Penipuan Via Jaringan BIFast', 'Nama Pengirim : Poermintojo', 'Tgl transaksi: 02/07/2024', 'Nominal: Rp40.000.000', 'Rekening penerima : 002 \\xad 099601015438506   MUHAMMAD IQBAL GHONN\",\"Transaction Solution\",\"\",\"\",\"2024-08-01 00:13:49\",\"\",\"Servicing\",\"Met\",\"\",\"2024-07-31 11:23:10\",\"2024-07-31 08:12:58\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Ade Mulki Erlangga .\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"CRS\",\"\",\"Gagal\",\"KAS KANPUS\",\"Kas Kanpus\",\"2301.0\",\"\",\"\",\"2024-07-31 11:23:10\",\"2024-07-30 00:00:00\",\"TTB000054213742\",\"\",\"Notes\",\"\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0054213806\",\"INVALID_FIELD:Foreign key external ID: 8812345678 not found for field SCC_cifno__c in entity Account\"', '6013500418259819,\"TTB000054217341\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-27 11:09:04\",\"Media Umum Lainya\",\"\",\"DANA\",\"6013500418259819.0\",\"301500.0\",\"Closed\",\"2024-07-30 13:55:00\",\"2024-07-26 00:00:00\",\"\",\"\",\"6013500418259819.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"VIRAS AQMAL LICKO\",\"123\",\"\",\"\",\"5.0\",\"90135204\",\"CCT-BMS\",\"VIRAS AQMAL LICKO\",\"#DANA', 'Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500418259819', 'Sebesar   : Rp. 301.500', 'Sebanyak         : 1  kali', 'Gagal di trx yang ke Tanggal   : 26/07/2024', 'Pukul   :  18:35:37 wib', 'Serial number : 202407260000200166005100588315', 'Keterangan / Respon :  Transaksi gagal saldo terdebet', '601350041825981920240726000020016600510058831526/07/2024 18:35:37301.500\",\"\",\"1972-11-09 12:33:24\",\"\",\"2024-07-30 13:55:00\",\"\",\"\",\"Met\",\"\",\"\",\"2024-07-29 06:37:19\",\"\",\"MBS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"VIRAS AQMAL LICKO .\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"LCC-CCTCALL\",\"\",\"Gagal\",\"KAS KANPUS\",\"Kas Kanpus\",\"14603.0\",\"\",\"\",\"2024-07-29 11:01:07\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054217341\",\"INVALID_FIELD:Foreign key external ID: 6013500418259819 not found for field SCC_cifno__c in entity Account\"', 'SGDG897,\"TTB000054218575\",\"1017\",\"CERIA - Penutupan & Pembatalan Akun CERIA\",\"2024-07-27 12:10:50\",\"Sabrina\",\"\",\"siti nurazizah\",\"600501020406535.0\",\"0.0\",\"Escalated\",\"\",\"\",\"\",\"Penutupan Kartu\",\"\",\"KKD-CRI-DATAM\",\"90099638.0\",\"\",\"\",\"Nadiah Rinanda Putri\",\"082221607621\",\"\",\"\",\"10.0\",\"SMG51363\",\"CCT-SMS-SMG\",\"Nadiah Rinanda Putri\",\"id ceria:1895121000018986', 'nomor rekening:600501020406535', 'atas nama: siti nurazizah', 'nomor ponsel yang terdaftar:082221607621', 'alasan penutupan akun: ingin melakukan perubahan email dan sudah lunas', 'alamat email:ditta72@gmail.com\",\"\",\"\",\"\",\"2024-08-05 01:16:38\",\"\",\"\",\"In Progress\",\"\",\"2024-07-27 12:27:04\",\"2024-07-29 06:47:06\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"LCC-CCTCALL\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14606.0\",\"\",\"\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"15.0\",\"Case Migration\",\"TTB0054218575\",\"INVALID_FIELD:Foreign key external ID: sgdg897 not found for field SCC_cifno__c in entity Account\"', '179901000394508,\"TTB000054221979\",\"8602\",\"BRImo Permasalahan Sistem Error\",\"2024-07-27 16:02:45\",\"Phone\",\"Platform & Device Support\",\"RUTH CORY LUSIANNA\",\"6013010845955435.0\",\"0.0\",\"Escalated\",\"\",\"2024-07-27 00:00:00\",\"Aplikasi\",\"Error Code\",\"179901000394508.0\",\"LCC-CCTCALL\",\"\",\"\",\"ruth1cory5lusianna77@gmail.com\",\"Catur Vira Sagita\",\"081318900727\",\"\",\"\",\"1.0\",\"JKT00268\",\"CCT-MRS-JKT\",\"Catur Vira Sagita\",\"Ch komplain tidak bisa menggunakan BRImo', 'User id          : Ruthcory1577', 'Sejak             : 27/07/2024', 'Jenis trx         : TOPUP PASTRAPAY BRIVA', 'Error code yang diterima  : MT30', 'Sebanyak              : 5 (Berapa kali)', 'Status jaringan     : Normal', 'Merk Hp yang digunakan : REALME 8', 'Versi aplikasi BRImo yang digunakan : -', 'Versi Android/iOS yang digunakan : -', 'Nomor Ponsel yang terdaftar Brimo :  081318900727', 'Email yang terdaftar Brimo               : ruth1cory5lusianna77@gmail.com', 'Keterangan               : GAGAL MT30', 'Sudah informasikan :..', '==========================================', 'Status: Enabled', 'Date Registered: 2019-11-29', 'Nama: RUTH CORY LUSIANNA', 'User ID: Ruthcory1577', 'No Kartu: 522184******7777', 'No. Handphone: 0813****0727 (Active)', 'Smartphone Status: (Not Registered)', 'E-mail: ruth******************mail.com', 'Tangal Lahir: 1977-05-01', 'Alamat: KOMP GPA BLOK A7 NO.', 'Nama Gadis Ibu Kandung: HOTMAIDA\",\"Mobile Banking\",\"\",\"\",\"2024-08-05 01:16:46\",\"\",\"Servicing\",\"In Progress\",\"\",\"2024-07-27 16:27:22\",\"\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Desak Putu W\",\"\",\"\",\"\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"00229 -- Kas Kanpus\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14601.0\",\"\",\"\",\"\",\"2024-07-31 06:04:53\",\"\",\"\",\"Notes\",\"\",\"Call\",\"4.0\",\"Case Migration\",\"TTB0054221979\",\"INVALID_FIELD:Foreign key external ID: 179901000394508 not found for field SCC_cifno__c in entity Account\"', '4365020237633705,\"TTB000054222843\",\"4214\",\"Perubahan No HP/TLP pada Kartu Kredit Utama\",\"2024-07-27 17:14:34\",\"Phone\",\"Request\",\"REZKY FITRIANNOOR\",\"\",\"0.0\",\"Escalated\",\"\",\"\",\"\",\"Perubahan data\",\"4365020237633705.0\",\"KKD-CRI-DATAM\",\"90099638.0\",\"\",\"\",\"Eko Ardianto\",\"085249310709\",\"\",\"\",\"3.0\",\"SMG63135\",\"CCT-CDS-SMG\",\"Eko Ardianto\",\"Nasabah menghubungi dengan nomor 081351460335', 'Nasabah mengajukan penggantian nomor handphone', 'Alasan: nomer hp tidak aktif', 'Nomr handphone lama : 085249310709', 'Nomor handphone baru : 081351460335', 'Alamat email untuk kirim dokumen : REZKYDEWWI02@GMAIL.COM', 'Agent  sarankan mengirim dokumen surat pernyataan ditanda tangani, foto copy kartu kredit tampak depan, foto copy KTP', 'Tolong bantuannya untuk tindak lanjut.', 'Terima kasih.', '**** 24 MONTH PROFILE ***', '( 01-06 ) ,1,1,1,1,1,1', '( 07-12 ) ,1,1,1,1,1,1', '( 13-18 ) ,1,1,1,1,1,1', '( 19-24 ) ,1,1,1,1,1,1', 'PRIM BLOCK, ,ALT BLOCK,', 'U1,01,U2,  ,U3,  ,CYC,15', 'PF5=PCPO     PF6=PCIC\",\"Kartu Kredit\",\"\",\"\",\"2024-08-05 01:16:49\",\"\",\"Loans\",\"In Progress\",\"\",\"2024-07-29 12:31:25\",\"2024-07-29 06:55:12\",\"\",\"CDS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"00229 -- Kas Kanpus\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14503.0\",\"\",\"\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"15.0\",\"Case Migration\",\"TTB0054222843\",\"INVALID_FIELD:Foreign key external ID: 4365020237633705 not found for field SCC_cifno__c in entity Account\"', '1895121000239077,\"TTB000054229178\",\"1017\",\"CERIA - Penutupan & Pembatalan Akun CERIA\",\"2024-07-28 09:01:54\",\"Sabrina\",\"Request\",\"Shavira dayu mayori\",\"\",\"0.0\",\"Closed\",\"2024-08-02 09:44:38\",\"\",\"\",\"Penutupan Kartu\",\"1895121000239076.8\",\"KKD-CRI-DATAM\",\"90099638.0\",\"\",\"viradayum@gmail.com\",\"Mega\",\"081216502859\",\"\",\"\",\"10.0\",\"19660955\",\"CCT-SMS-SMG\",\"Mega\",\"ch infokan ingin melakukan penutupan Ceria', '=============', 'id ceria: 1895121000239077', 'nomor rekening: 058001021703500', 'atas nama: shavira dayu mayori', 'nomor ponsel yang terdaftar: 081216502859', 'alasan penutupan akun: sudah tidak digunakan lagi', 'alamat email: viradayum@gmail.com\",\"KTA - Digital\",\"\",\"\",\"2024-08-03 14:28:10\",\"\",\"Loans\",\"Met\",\"\",\"2024-08-03 14:28:10\",\"2024-07-29 06:47:04\",\"\",\"SMS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"RPA CRS\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"LCC-CCTCALL\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14606.0\",\"\",\"\",\"\",\"2024-08-02 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"15.0\",\"Case Migration\",\"TTB0054229178\",\"INVALID_FIELD:Foreign key external ID: 1895121000239077 not found for field SCC_cifno__c in entity Account\"', '4359720217431305,\"TTB000054233609\",\"4214\",\"Perubahan No HP/TLP pada Kartu Kredit Utama\",\"2024-07-28 14:55:40\",\"Phone\",\"Request\",\"MUHAMMAD RIDHO\",\"\",\"0.0\",\"Escalated\",\"\",\"\",\"\",\"Perubahan data\",\"4359720217431305.0\",\"KKD-CRI-DATAM\",\"90099638.0\",\"\",\"MUHRIDHORAMDHAN15@GMAIL.COM\",\"Marissa Yulenda\",\"082117757616\",\"\",\"\",\"3.0\",\"90162422\",\"CCT-MRS-JKT\",\"Marissa Yulenda\",\"#no avaya 085216995468', 'Nasabah mengajukan penggantian nomor handphone', 'Alasan: no lama tidak aktif', 'Nomr handphone lama :085216995468', 'Nomor handphone baru :082117757616', 'Alamat email untuk kirim dokumen : MUHRIDHORAMDHAN15@GMAIL.COM', 'Agent  sarankan mengirim dokumen surat pernyataan ditanda tangani, foto copy kartu kredit tampak depan, foto copy KTP', 'Tolong bantuannya untuk tindak lanjut.', 'Terima kasih.', 'Salin status kartu kredit (month profile, screen PCHI)', '**** 24 MONTH PROFILE ***', '( 01-06 ) ,B,B,B,B,B,0', '( 07-12 ) ,B,0, , , ,', '( 13-18 ) , , , , , ,', '( 19-24 ) , , , , , ,', 'PRIM BLOCK, ,ALT BLOCK,', 'U1,01,U2,  ,U3,  ,CYC,15', 'PF5=PCPO     PF6=PCIC\",\"Kartu Kredit\",\"\",\"\",\"2024-08-05 01:17:12\",\"\",\"Loans\",\"In Progress\",\"\",\"2024-07-30 14:55:51\",\"2024-07-31 16:14:26\",\"\",\"MBS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"LCC-CCTCALL\",\"\",\"\",\"KAS KANPUS\",\"Kas Kanpus\",\"14601.0\",\"\",\"2024-08-01 11:39:36\",\"\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"15.0\",\"Case Migration\",\"TTB0054233609\",\"INVALID_FIELD:Foreign key external ID: 4359720217431305 not found for field SCC_cifno__c in entity Account\"', '064501009106532,\"TTB000054236535\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-28 20:10:04\",\"Phone\",\"\",\"Muh. Alfian\",\"5221849904032432.0\",\"100000.0\",\"Closed\",\"2024-07-31 11:16:24\",\"2024-07-28 00:00:00\",\"\",\"\",\"64501009106532.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Muhammad Arif Gunawan\",\"085398742193\",\"\",\"\",\"5.0\",\"JKT00060\",\"CCT-MRS-JKT\",\"Muhammad Arif Gunawan\",\"Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500428291281', 'Sebesar : Rp.100.000', 'Sebanyak :  1 kali', 'Tanggal: 28/07/2024', 'Pukul: +- 20:00 WITA', 'Keterangan / Respon : Sukses', 'Keterangan tambahan : Saldo terpotong, belum masuk ke BRIZZI', '2024-07-2819:20:5564501009106532TOPD6013500428291281NBMB52218499040324322100.0000NA8888236\",\"\",\"\",\"\",\"2024-07-31 11:16:24\",\"\",\"\",\"Met\",\"\",\"2024-07-29 16:52:43\",\"2024-07-31 06:44:13\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"BRICare Administrator\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"00229 -- Kas Kanpus\",\"\",\"Gagal\",\"KAS KANPUS\",\"Kas Kanpus\",\"14601.0\",\"\",\"\",\"2024-07-31 11:16:24\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"2301.0\",\"Case Migration\",\"TTB0054236535\",\"INVALID_FIELD:Foreign key external ID: 064501009106532 not found for field SCC_cifno__c in entity Account\"', 'RGGK619,\"TTB000054240220\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-29 06:28:18\",\"Phone\",\"\",\"RESMOYO\",\"596601017854534.0\",\"100000.0\",\"Closed\",\"2024-08-02 14:12:13\",\"2024-07-28 00:00:00\",\"\",\"\",\"5221842190946100.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"Masna Khasbiya\",\"085200641399\",\"\",\"\",\"5.0\",\"SMG61122\",\"CCT-MRS-SMG\",\"Masna Khasbiya\",\"#TopUpDeposit', 'Nasabah gagal top up BRIZZI', 'Nomor kartu BRIZZI : 6013500804321371', 'Sebesar   : Rp. 100.000', 'Sebanyak         : 4 kali', 'Gagal di trx yang ke : 1 dan 2', 'Tanggal   : 2024-07-28', 'Pukul   : 10:43 dan 10:45 wib', 'Keterangan / Respon : saldo rekening terpotong namun saldo brizzi belum masuk', 'Keterangan tambahan : sudah aktivasi deposit namun saldo brizzi belum masuk', '( lampirkan 10 mutasi terakhir kartu BRIZZI )', '======================', '2024-07-2810:43:38596601017854534TOPD6013500804321371NBMB5221842190946100 ESB:NBMB:0200600T:7146181363352100.0000NA8888033', '2024-07-2810:45:24596601017854534TOPD6013500804321371NBMB5221842190946100 ESB:NBMB:0200600T:7146186636142100.0000NA8888575', '2024-07-2810:57:10596601017854534TOPU6013500804321371NBMB5221842190946100 ESB:NBMB:0200400T:7146222282332100.0000NA8888094', '2024-07-2817:38:00596601017854534TOPU6013500804321371NBMB5221842190946100 ESB:NBMB:0200400T:7147431300932150.0000NA8888032\",\"\",\"\",\"\",\"2024-08-02 14:12:13\",\"\",\"\",\"Met\",\"\",\"2024-07-30 09:34:42\",\"2024-08-02 07:23:23\",\"\",\"MRS\",\"\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"BRICare Administrator\",\"\",\"False\",\"\",\"\",\"SUMIYATI\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"LCC-CCTCALL\",\"\",\"Gagal\",\"KAS KANPUS\",\"Kas Kanpus\",\"14502.0\",\"\",\"\",\"2024-08-02 13:56:38\",\"2024-08-01 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Call\",\"2301.0\",\"Case Migration\",\"TTB0054240220\",\"INVALID_FIELD:Foreign key external ID: rggk619 not found for field SCC_cifno__c in entity Account\"', 'IFL4217,\"TTB000054240841\",\"7703\",\"Komplain Transaksi Kartu Kredit tidak di akui merchant Bank Lain\",\"2024-07-29 07:39:49\",\"SMS\",\"\",\"IRMAN ALI\",\"25301000344568.0\",\"5306000.0\",\"Escalated\",\"\",\"2024-07-27 23:00:00\",\"\",\"Sanggahan\",\"5326595011171050.0\",\"LCC-FBI\",\"\",\"\",\"irmanali1000@gmail.com\",\"andi asriyanti\",\"085255754358\",\"\",\"\",\"7.0\",\"00328746\",\"04890 -- UNIT BATANGMATA BENTENG SELAYA\",\"andi asriyanti\",\"Nasabah melaporkan Sanggahan Transaksi/Tidak Merasa Bertransaksi Dengan Kartu Debit.', 'Mengetahui transaksi melalui : SMS NOTIFIKASI DAN MUTASI REKENING DI BRIMO', 'Saat terjadi transaksi Nasabah berada di : DI KANTOR MADRASAH MTSN KEPULAUAN SELAYAR', 'Nominal Transaksi yang disanggah Rp 5.306.000', 'Sebanyak :  3 (kali )', 'Tanggal    :  28 JULI 2024', 'Transaksi yang disanggah :', '', '2DSMCEAPPLECOM/BILL I2129570002', '5326595011171050#420952377334#001070', 'ESB:INMC:000MG00F:420952377334', 'Kartu dipegang nasabah : YA', 'Kantor BRI yang dapat dikunjungi : BRI KC SELAYAR & KC BULUKUMBA', 'Alamat domisili nasabah : JL MELINJO SELAYAR', 'Sudah jelaskan jenis transaksi yang disanggah', '(Copy transaksi yang dilaporkan dari CHS Settle Complain)\",\"\",\"\",\"\",\"2024-08-05 01:17:42\",\"\",\"\",\"In Progress\",\"\",\"\",\"2024-07-29 12:58:55\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Ade Mulki Erlangga .\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"04890 -- UNIT BATANGMATA BENTENG SELAYA\",\"\",\"Gagal\",\"Makassar\",\"Benteng Selayar\",\"6366.0\",\"\",\"\",\"2024-07-29 12:58:55\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0054240841\",\"INVALID_FIELD:Foreign key external ID: ifl4217 not found for field SCC_cifno__c in entity Account\"', 'AJK1919,\"TTB000054240928\",\"8419\",\"sanggahan transaksi/tidak merasa bertransaksi ( tanpa kartu )\",\"2024-07-29 07:53:01\",\"SMS\",\"Fraud / Suspicious Transactions\",\"ASNIWATY YUSUF\",\"202301004828505.0\",\"5639000.0\",\"Escalated\",\"\",\"2024-03-12 23:00:00\",\"Internet Banking / BRImo\",\"Transaksi Normal\",\"6013011006589815.0\",\"LCC-FBI\",\"\",\"\",\"\",\"Sri Wahyuni Julianti Musa\",\"085256838688\",\"\",\"\",\"10.0\",\"00332065\",\"00027 -- Gorontalo\",\"Sri Wahyuni Julianti Musa\",\"Nasabah melaporkan Sanggahan Transaksi/Tidak Merasa Bertransaksi Dengan Kartu Debit.', 'Mengetahui transaksi melalui : mutasi rekening', 'Saat terjadi transaksi Nasabah berada di kantor', 'Nominal Transaksi yang disanggah Rp 5,639,000', 'Sebanyak : 1 (kali )', 'Tanggal    :  13 maret 2024', 'Transaksi yang disanggah :', 'Nama pemilik rekening tujuan  :  … (jika transaksi yang disanggah adalah transfer)', 'No rekening tujuan   :  …', 'Kartu dipegang nasabah : Ya', 'Kantor BRI yang dapat dikunjungi : bri kcp agusalim dan bri cabang gorontalo', 'Alamat domisili nasabah : dusun iv desa tenggela kec tilango kabupaten gorontalo prov gorontalo', 'Sudah jelaskan jenis transaksi yang disanggah', 'Nasabah melaporkan ada transaksi yg tidak di ketahui di tgl 13 maret 2024 dengan uraian transaksi : ATMSTRPRM 08888 000922942 0805451074 dan terdebet sebesar Rp 5,639,000. ]', 'Kartu debit sudah exp 07/23', 'Mohon dibantu jenis transaksi dan lokasi transaksinya.', 'Atas perhatian dan bantuannya di ucapkan terimakasih.\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 01:17:42\",\"\",\"Servicing\",\"In Progress\",\"\",\"2024-07-29 10:33:28\",\"2024-07-29 12:59:01\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"False\",\"\",\"\",\"\",\"\",\"False\",\"Ade Mulki Erlangga .\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"False\",\"\",\"\",\"Gagal\",\"\",\"\",\"2527.0\",\"\",\"\",\"2024-07-29 12:59:02\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"5.0\",\"Case Migration\",\"TTB0054240928\",\"INVALID_FIELD:Foreign key external ID: ajk1919 not found for field SCC_cifno__c in entity Account\"', 'SDB6572,\"TTB000054241150\",\"8411\",\"Salah Transfer antar BRI\",\"2024-07-29 08:12:24\",\"SMS\",\"Request\",\"SAMUEL UPA\",\"348801014802538.0\",\"4000000.0\",\"Escalated\",\"\",\"2024-07-23 23:00:00\",\"ATM BRI\",\"Transfer\",\"6013013204625540.0\",\"LCC-CRC\",\"90136590.0\",\"\",\"\",\"Richard Jeremiah Suradi\",\"081239901006\",\"\",\"\",\"10.0\",\"00349264\",\"03488 -- UNIT MERDEKA KUPANG\",\"Richard Jeremiah Suradi\",\"Nasabah melakukan transaksi transfer namun salah memasukkan nomor rekening tujuan :', 'Ke Bank : BRI', 'Nomor rekening yang salah : 348801052446532', 'Pemilik rekening : VINSENTY JULIO VALLEN', 'Sebesar : Rp.4.000.000', 'Sebanyak : 1  kali', 'Yang gagal trx ke    :1', 'Tanggal : 24/07/2024', 'Pukul : 12.47', 'Nomor rekening yang benar : 348801052426536', 'Pemilik rekening : NEKHA HELSA PAULINA UPA', 'Keterangan tambahan : Ybs melakukan transfer melalui agen bank lain (bank NTT), lalu petugas menginput nomor rekening yang salah sehingga terjadi kesalahan transfer ke nomor rekening yang berbeda. mohon bantuannya untuk ditindak lanjuti. Terima Kasih.', 'nb : rekening koran terlampir.\",\"Transaction Solution\",\"\",\"\",\"2024-08-05 01:17:43\",\"\",\"Transaction Banking\",\"In Progress\",\"\",\"2024-07-31 08:12:36\",\"2024-07-31 08:15:37\",\"\",\"\",\"True\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"Danny\\xa0Surya\\xa0N\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"\",\"\",\"\",\"\",\"\",\"False\",\"03488 -- UNIT MERDEKA KUPANG\",\"\",\"\",\"Denpasar\",\"Kupang\",\"4970.0\",\"\",\"\",\"2024-07-31 09:31:49\",\"2024-08-05 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"3.0\",\"Case Migration\",\"TTB0054241150\",\"INVALID_FIELD:Foreign key external ID: sdb6572 not found for field SCC_cifno__c in entity Account\"', 'SHNVS97,\"TTB000054243831\",\"8433\",\"Pembayaran Tagihan Gagal, Saldo Berkurang\",\"2024-07-29 09:41:14\",\"Walk-In\",\"Complaint - Transaction\",\"SUDIONO\",\"652001032152534.0\",\"601000.0\",\"Closed\",\"2024-07-30 11:32:43\",\"2024-07-26 00:00:00\",\"ATM BRI\",\"Pembayaran\",\"\",\"LCC-ISSUER\",\"90133775.0\",\"\",\"dava56220@gmail.com\",\"Ricky Maulana Mahgribi\",\"082139665556\",\"\",\"\",\"3.0\",\"00230032\",\"06519 -- UNIT PLAZA PROBOLINGGO\",\"Ricky Maulana Mahgribi\",\"Nasabah gagal top up saldo Link Aja melalui aplikasi DigiPos', 'Jumlah : Rp 600.000 dg biaya Rp 1.000, total Rp 601.000', 'Via : BRIMO', 'Tanggal : 26 Juli 2024 pukul 06:44 WIB.', 'No. BRIVA : 911999903304738', 'Saldo rekening telah terdebet, namun status transaksi Diproses, dan saldo tujuan tidak bertambah. Mohon bantuannya Tks\",\"Transaction Solution\",\"1970-01-03 22:53:52\",\"\",\"2024-07-30 11:32:43\",\"\",\"Transaction Banking\",\"Met\",\"\",\"2024-07-29 09:44:22\",\"2024-07-29 13:24:52\",\"\",\"\",\"False\",\"BRI\",\"\",\"\",\"\",\"True\",\"\",\"\",\"Ricky Maulana Mahgribi .\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"0 - No Kartu / No HP minimal 10 digit dan numeric\",\"\",\"\",\"\",\"\",\"True\",\"06519 -- UNIT PLAZA PROBOLINGGO\",\"\",\"\",\"KANWIL MALANG\",\"Probolinggo\",\"7990.0\",\"\",\"\",\"\",\"2024-07-30 00:00:00\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"1.0\",\"Case Migration\",\"TTB0054243831\",\"INVALID_FIELD:Foreign key external ID: shnvs97 not found for field SCC_cifno__c in entity Account\"', 'NIP3888,\"TTB000054256033\",\"8713\",\"Gagal Top Up dan Gagal Transaksi BRIZZI tapi Saldo Berkurang\",\"2024-07-29 15:30:32\",\"Walk-In\",\"Complaint - Transaction\",\"nuke julianti\",\"50801007460534.0\",\"100000.0\",\"Closed\",\"2024-08-01 15:43:41\",\"2024-07-29 00:00:00\",\"\",\"\",\"6013500152769593.0\",\"ADMIN QSC\",\"\",\"\",\"\",\"M Iqbal Juan Saputra\",\"085646236801\",\"\",\"\",\"5.0\",\"00343815\",\"\",\"M Iqbal Juan Saputra\",\"Kartu Brizzi tidak bisa terbaca dan masih ada saldo Rp 100.600 dan perlu pemindahan saldo ke kartu brizi yang baru 6013502001133674\",\"Electronic Money\",\"\",\"\",\"2024-08-01 15:43:41\",\"\",\"Servicing\",\"Met\",\"\",\"2024-07-31 19:30:13\",\"2024-08-01 06:54:35\",\"\",\"\",\"True\",\"\",\"\",\"\",\"\",\"False\",\"\",\"\",\"BRICare Administrator\",\"\",\"False\",\"\",\"\",\"\",\"False\",\"\",\"Bank BRI\",\"Insert Ticket Berhasil\",\"\",\"\",\"ADMIN_DRONE\",\"\",\"True\",\"06266 -- UNIT PESANTREN KEDIRI\",\"\",\"Gagal\",\"KANWIL MALANG\",\"Kediri\",\"\",\"\",\"\",\"2024-08-01 15:32:51\",\"2024-07-31 19:30:13\",\"\",\"\",\"Notes\",\"\",\"Non Call\",\"2301.0\",\"Case Migration\",\"TTB0054256033\",\"INVALID_FIELD:Foreign key external ID: nip3888 not found for field SCC_cifno__c in entity Account\"']\n",
      "Column 'cifno' not found in the data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['cifno', 'Nama_Nasabah', 'no_telepon', 'email'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerson Account\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Filter the columns and rows based on the specified conditions\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJenis_Nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Jenis_Nasabah is empty\u001b[39;49;00m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtipe_nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndividu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# tipe_nasabah is Individu\u001b[39;49;00m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecord_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPerson Account\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# record_type is Person Account\u001b[39;49;00m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNama_Nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_telepon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memail\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJenis_Nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtipe_nasabah\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecord_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Selected columns\u001b[39;49;00m\n\u001b[0;32m     52\u001b[0m \u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Ensure full numbers are retained in the CSV output\u001b[39;00m\n\u001b[0;32m     55\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mfloat_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['cifno', 'Nama_Nasabah', 'no_telepon', 'email'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = r\"D:\\Python2\\case_2024\\missing_ttb_from attachment\\error080724030058935.csv\"\n",
    "\n",
    "# Initialize a list to store rows\n",
    "rows = []\n",
    "\n",
    "# Open and read the CSV file, handling BOM\n",
    "with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "    reader = csv.reader(file, delimiter=';')\n",
    "    current_row = []\n",
    "    for line in reader:\n",
    "        if len(line) > 1:\n",
    "            if current_row:\n",
    "                rows.append(current_row)\n",
    "            current_row = line\n",
    "        else:\n",
    "            current_row.extend(line)\n",
    "\n",
    "    # Don't forget to add the last collected row\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "\n",
    "# Convert list of rows to DataFrame\n",
    "data = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "\n",
    "# Strip any leading/trailing whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Print out the column names to debug\n",
    "print(\"Column names in the DataFrame:\", data.columns.tolist())\n",
    "\n",
    "# Ensure 'cifno' is read as a string if it exists\n",
    "if 'cifno' in data.columns:\n",
    "    data['cifno'] = data['cifno'].astype(str)\n",
    "else:\n",
    "    print(\"Column 'cifno' not found in the data\")\n",
    "\n",
    "# Add the columns 'Jenis_Nasabah', 'tipe_nasabah', and 'record_type' with default values\n",
    "data['Jenis_Nasabah'] = None  # Assuming empty means None\n",
    "data['tipe_nasabah'] = 'Individu'\n",
    "data['record_type'] = 'Person Account'\n",
    "\n",
    "# Filter the columns and rows based on the specified conditions\n",
    "filtered_data = data.loc[\n",
    "    (data['Jenis_Nasabah'].isna()) &  # Jenis_Nasabah is empty\n",
    "    (data['tipe_nasabah'] == 'Individu') &  # tipe_nasabah is Individu\n",
    "    (data['record_type'] == 'Person Account'),  # record_type is Person Account\n",
    "    ['cifno', 'Nama_Nasabah', 'no_telepon', 'email', 'Jenis_Nasabah', 'tipe_nasabah', 'record_type']  # Selected columns\n",
    "]\n",
    "\n",
    "# Ensure full numbers are retained in the CSV output\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "output_path = r\"D:\\Python2\\case_2024\\missing_ttb_from attachment\\error080724030058935_done.csv\"\n",
    "filtered_data.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the filtered data to the user\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To close Taspen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\error081024020347257.csv\"\n",
    "path2=r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\error081024020347257_done.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['Jenis_Nasabah'] = None  \n",
    "df['tipe_nasabah'] = 'Individu'\n",
    "df['record_type1'] = 'Person Account'\n",
    "df\n",
    "df.to_csv(path2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticket Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ID        Ticket_ID     STATUS\n",
      "153839   500Mg00000F6DvoIAF  TTB000049945668     Closed\n",
      "153857   500Mg00000F6Dw6IAF  TTB000049945717     Closed\n",
      "153985   500Mg00000F6DxSIAV  TTB000049945832     Closed\n",
      "154127   500Mg00000EoHklIAF  TTB000049946039     Closed\n",
      "154131   500Mg00000EoHkpIAF  TTB000049946053     Closed\n",
      "...                     ...              ...        ...\n",
      "3729004  500Mg00000GZSdRIAX  TTB000054264303  Escalated\n",
      "3729006  500Mg00000GZSdSIAX  TTB000054264305  Escalated\n",
      "3729007  500Mg00000GZSdTIAX  TTB000054264306  Escalated\n",
      "3729008  500Mg00000GZSdUIAX  TTB000054264307  Escalated\n",
      "3729009  500Mg00000GZSdVIAX  TTB000054264308  Escalated\n",
      "\n",
      "[55387 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the files\n",
    "file_1_path = r\"D:\\Python2\\case_2024\\30 july- 4 aug\\extract_perubahan_ticket - Copy.csv\"\n",
    "file_2_path = r\"D:\\Python2\\case_2024\\30 july- 4 aug\\TICKET_ID_bricare_20240111_20240729_mod_less_4agustus_0_79_closed.csv\"\n",
    "filtered_file_1_path = r\"D:\\Python2\\case_2024\\30 july- 4 aug\\filtered_perubahan_tiket.csv\"\n",
    "\n",
    "# Load file 1\n",
    "df_file_1 = pd.read_csv(file_1_path, delimiter=';')\n",
    "\n",
    "# Load the modified file 2\n",
    "df_file_2_modified = pd.read_csv(file_2_path)\n",
    "\n",
    "# Extract the Ticket_ID column from the modified file 2\n",
    "ticket_ids_file_2_modified = df_file_2_modified['Ticket_ID'].dropna().unique()\n",
    "\n",
    "# Filter the rows in file 1 based on Ticket_ID values in the modified file 2\n",
    "filtered_df_file_1 = df_file_1[df_file_1['Ticket_ID'].isin(ticket_ids_file_2_modified)]\n",
    "\n",
    "# Display the filtered dataframe\n",
    "print(filtered_df_file_1)\n",
    "\n",
    "# Save the filtered dataframe to a new file\n",
    "filtered_df_file_1.to_csv(filtered_file_1_path, index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to generate the dummy data to change case owner in UAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>record_type</th>\n",
       "      <th>cif</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Details</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>...</th>\n",
       "      <th>Tanggal_Settlement</th>\n",
       "      <th>Tgl_Foward</th>\n",
       "      <th>Tgl_In_Progress</th>\n",
       "      <th>Tgl_Returned</th>\n",
       "      <th>Ticket_Referensi</th>\n",
       "      <th>Tiket_Urgency</th>\n",
       "      <th>Tipe_Remark</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>users</th>\n",
       "      <th>Usergroup_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET001339022</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TICKET01234567891</td>\n",
       "      <td>0</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>1/1/2023 7:07</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET001339022</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TICKET01234567892</td>\n",
       "      <td>999</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>1/1/2023 7:07</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET001339022</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TICKET01234567893</td>\n",
       "      <td>1000</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>1/1/2023 7:07</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET001339022</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TICKET01234567894</td>\n",
       "      <td>1001</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>1/1/2023 7:07</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET001339022</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TICKET01234567895</td>\n",
       "      <td>1002</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>1/1/2023 7:07</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case_number     record_type  cif          Ticket_ID  Call_Type_ID  \\\n",
       "0  TICKET001339022  Case Migration  NaN  TICKET01234567891             0   \n",
       "0  TICKET001339022  Case Migration  NaN  TICKET01234567892           999   \n",
       "0  TICKET001339022  Case Migration  NaN  TICKET01234567893          1000   \n",
       "0  TICKET001339022  Case Migration  NaN  TICKET01234567894          1001   \n",
       "0  TICKET001339022  Case Migration  NaN  TICKET01234567895          1002   \n",
       "\n",
       "                              Call_Type  \\\n",
       "0  Blokir Kartu ATM karena kartu hilang   \n",
       "0  Blokir Kartu ATM karena kartu hilang   \n",
       "0  Blokir Kartu ATM karena kartu hilang   \n",
       "0  Blokir Kartu ATM karena kartu hilang   \n",
       "0  Blokir Kartu ATM karena kartu hilang   \n",
       "\n",
       "                                             Details    Create_Date gateway  \\\n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  1/1/2023 7:07   Phone   \n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  1/1/2023 7:07   Phone   \n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  1/1/2023 7:07   Phone   \n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  1/1/2023 7:07   Phone   \n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  1/1/2023 7:07   Phone   \n",
       "\n",
       "  Jenis_Laporan  ... Tanggal_Settlement Tgl_Foward  Tgl_In_Progress  \\\n",
       "0       Request  ...                NaN        NaN              NaN   \n",
       "0       Request  ...                NaN        NaN              NaN   \n",
       "0       Request  ...                NaN        NaN              NaN   \n",
       "0       Request  ...                NaN        NaN              NaN   \n",
       "0       Request  ...                NaN        NaN              NaN   \n",
       "\n",
       "  Tgl_Returned  Ticket_Referensi  Tiket_Urgency  Tipe_Remark UniqueID users  \\\n",
       "0          NaN               NaN            NaN        Notes      NaN  Call   \n",
       "0          NaN               NaN            NaN        Notes      NaN  Call   \n",
       "0          NaN               NaN            NaN        Notes      NaN  Call   \n",
       "0          NaN               NaN            NaN        Notes      NaN  Call   \n",
       "0          NaN               NaN            NaN        Notes      NaN  Call   \n",
       "\n",
       "  Usergroup_ID  \n",
       "0            4  \n",
       "0            4  \n",
       "0            4  \n",
       "0            4  \n",
       "0            4  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original data\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\UAT\\dummy_data_case_uat - 79_new - Copy (2) - Copy.csv\"\n",
    "data = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Load the new data containing Call_Type_ID values\n",
    "file_path_new = r\"D:\\dataquality2\\Cleaned4_Call_Type - Copy.csv\"\n",
    "call_type_data = pd.read_csv(file_path_new, delimiter=';', lineterminator='\\n', encoding='ISO-8859-1')\n",
    "\n",
    "# Load the branch unit data containing user_group values\n",
    "file_path_branch_unit = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx - branch_unit.csv\"\n",
    "branch_unit_data = pd.read_csv(file_path_branch_unit)\n",
    "\n",
    "# Ensure Call_Type_ID is an integer by filling NaN values with -1 first\n",
    "call_type_data['Call_Type_ID'] = call_type_data['Call_Type_ID'].fillna(-1).astype(int)\n",
    "\n",
    "# Extract user_group values from the branch unit data\n",
    "user_groups = branch_unit_data['Name'].unique()\n",
    "\n",
    "# Function to generate multiple rows\n",
    "def generate_rows(data, call_type_data, user_groups, num_rows, start_ticket_id):\n",
    "    # Convert start_ticket_id to an integer for manipulation\n",
    "    base_id = int(start_ticket_id[7:])\n",
    "    prefix = start_ticket_id[:7]\n",
    "    \n",
    "    # Extract Call_Type_ID values from the new dataset\n",
    "    call_type_ids = call_type_data['Call_Type_ID'].unique()\n",
    "    \n",
    "    # Create a list to hold the new rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Generate the specified number of rows\n",
    "    for i in range(num_rows):\n",
    "        new_row = data.iloc[0].copy()\n",
    "        new_row['Ticket_ID'] = f\"{prefix}{base_id + i:010d}\"\n",
    "        new_row['Call_Type_ID'] = call_type_ids[i % len(call_type_ids)]\n",
    "        new_row['user_group'] = user_groups[i % len(user_groups)]\n",
    "        \n",
    "        # Ensure No_Rekening and Nomor_Kartu are strings\n",
    "        if 'No_Rekening' in new_row and pd.notna(new_row['No_Rekening']):\n",
    "            new_row['No_Rekening'] = '{:.0f}'.format(float(new_row['No_Rekening'].replace(',', '.')))\n",
    "        if 'Nomor_Kartu' in new_row and pd.notna(new_row['Nomor_Kartu']):\n",
    "            new_row['Nomor_Kartu'] = '{:.0f}'.format(float(new_row['Nomor_Kartu'].replace(',', '.')))\n",
    "        \n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    # Convert the list to a DataFrame\n",
    "    new_data = pd.DataFrame(new_rows)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Specify the number of rows to generate and the starting Ticket_ID\n",
    "num_rows = 5\n",
    "start_ticket_id = 'TICKET001234567891'\n",
    "\n",
    "# Generate the rows\n",
    "new_data = generate_rows(data, call_type_data, user_groups, num_rows, start_ticket_id)\n",
    "\n",
    "# Save the new data to a CSV file\n",
    "output_file_path = 'generated_data.csv'\n",
    "new_data.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "# Display the new data\n",
    "new_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To prepare the file to change owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>SCC_CALL_TYPE__C</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>SCC_USER_GROUP__C</th>\n",
       "      <th>OWNERID</th>\n",
       "      <th>CASE_OWNER_X_USER__C</th>\n",
       "      <th>SCC_CASE_OWNER__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000050358103</td>\n",
       "      <td>a1IMg000000DQUbMAO</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GMg0000000sFJMAY</td>\n",
       "      <td>False</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000050749322</td>\n",
       "      <td>a1IMg000000DQXYMA4</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GMg0000000sFNMAY</td>\n",
       "      <td>False</td>\n",
       "      <td>SCC - Non Fraud Transaction - ACQUIRER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000050956050</td>\n",
       "      <td>a1IMg000000DQXNMA4</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GMg0000000sFrMAI</td>\n",
       "      <td>False</td>\n",
       "      <td>ORD - Fraud Management &amp; Recovery Desk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000050357999</td>\n",
       "      <td>a1IMg000000DQUbMAO</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GMg0000000sFJMAY</td>\n",
       "      <td>False</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000050357993</td>\n",
       "      <td>a1IMg000000DQUbMAO</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GMg0000000sFJMAY</td>\n",
       "      <td>False</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47099</th>\n",
       "      <td>TTB000078030896</td>\n",
       "      <td>a1IMg000000DQWJMA4</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GMg0000000sFMMAY</td>\n",
       "      <td>False</td>\n",
       "      <td>SCC - Non Fraud Transaction - ISSUER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47100</th>\n",
       "      <td>TTB000054238436</td>\n",
       "      <td>a1IMg000000DQXPMA4</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>ADMIN QSC</td>\n",
       "      <td>005Mg000001PtAQIA0</td>\n",
       "      <td>True</td>\n",
       "      <td>Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47101</th>\n",
       "      <td>TTB000054252514</td>\n",
       "      <td>a1IMg000000DQXPMA4</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>ADMIN QSC</td>\n",
       "      <td>005Mg000001PtAQIA0</td>\n",
       "      <td>True</td>\n",
       "      <td>Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47102</th>\n",
       "      <td>TTB000054118155</td>\n",
       "      <td>a1IMg000000DQXPMA4</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>ADMIN QSC</td>\n",
       "      <td>005Mg000001PtAQIA0</td>\n",
       "      <td>True</td>\n",
       "      <td>Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47103</th>\n",
       "      <td>TTB000078198317</td>\n",
       "      <td>a1IMg000000DQhRMAW</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GMg0000000sFmMAI</td>\n",
       "      <td>False</td>\n",
       "      <td>INV - Business Operation DPLK Team</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47104 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SCC_LEGACY_TICKET_ID__C    SCC_CALL_TYPE__C     STATUS  \\\n",
       "0             TTB000050358103  a1IMg000000DQUbMAO  Escalated   \n",
       "1             TTB000050749322  a1IMg000000DQXYMA4  Escalated   \n",
       "2             TTB000050956050  a1IMg000000DQXNMA4  Escalated   \n",
       "3             TTB000050357999  a1IMg000000DQUbMAO  Escalated   \n",
       "4             TTB000050357993  a1IMg000000DQUbMAO  Escalated   \n",
       "...                       ...                 ...        ...   \n",
       "47099         TTB000078030896  a1IMg000000DQWJMA4  Escalated   \n",
       "47100         TTB000054238436  a1IMg000000DQXPMA4  Escalated   \n",
       "47101         TTB000054252514  a1IMg000000DQXPMA4  Escalated   \n",
       "47102         TTB000054118155  a1IMg000000DQXPMA4  Escalated   \n",
       "47103         TTB000078198317  a1IMg000000DQhRMAW  Escalated   \n",
       "\n",
       "      SCC_USER_GROUP__C             OWNERID  CASE_OWNER_X_USER__C  \\\n",
       "0                   NaN  00GMg0000000sFJMAY                 False   \n",
       "1                   NaN  00GMg0000000sFNMAY                 False   \n",
       "2                   NaN  00GMg0000000sFrMAI                 False   \n",
       "3                   NaN  00GMg0000000sFJMAY                 False   \n",
       "4                   NaN  00GMg0000000sFJMAY                 False   \n",
       "...                 ...                 ...                   ...   \n",
       "47099               NaN  00GMg0000000sFMMAY                 False   \n",
       "47100         ADMIN QSC  005Mg000001PtAQIA0                  True   \n",
       "47101         ADMIN QSC  005Mg000001PtAQIA0                  True   \n",
       "47102         ADMIN QSC  005Mg000001PtAQIA0                  True   \n",
       "47103               NaN  00GMg0000000sFmMAI                 False   \n",
       "\n",
       "                            SCC_CASE_OWNER__C  \n",
       "0                        SCC - Contact Center  \n",
       "1      SCC - Non Fraud Transaction - ACQUIRER  \n",
       "2      ORD - Fraud Management & Recovery Desk  \n",
       "3                        SCC - Contact Center  \n",
       "4                        SCC - Contact Center  \n",
       "...                                       ...  \n",
       "47099    SCC - Non Fraud Transaction - ISSUER  \n",
       "47100                               Migration  \n",
       "47101                               Migration  \n",
       "47102                               Migration  \n",
       "47103      INV - Business Operation DPLK Team  \n",
       "\n",
       "[47104 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\case_2024\\change_owner\\extract_legacy_calltype_usergroup_caseowner.csv\"\n",
    "\n",
    "df=pd.read_csv(path, delimiter=';')\n",
    "df.drop(columns=['Column2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1035</td>\n",
       "      <td>a1IMg000000DQSvMAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7999</td>\n",
       "      <td>a1IMg000000DQSwMAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000</td>\n",
       "      <td>a1IMg000000DQSxMAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8638</td>\n",
       "      <td>a1IMg000000DQSyMAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1037</td>\n",
       "      <td>a1IMg000000DQSzMAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>8407</td>\n",
       "      <td>a1IMg000000Ec2EMAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>1111</td>\n",
       "      <td>a1IMg000000Ekz3MAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>8971</td>\n",
       "      <td>a1IMg000000F2vxMAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>8972</td>\n",
       "      <td>a1IMg000000F341MAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>8973</td>\n",
       "      <td>a1IMg000000F37FMAS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NAME                  ID\n",
       "0    1035  a1IMg000000DQSvMAO\n",
       "1    7999  a1IMg000000DQSwMAO\n",
       "2    9000  a1IMg000000DQSxMAO\n",
       "3    8638  a1IMg000000DQSyMAO\n",
       "4    1037  a1IMg000000DQSzMAO\n",
       "..    ...                 ...\n",
       "521  8407  a1IMg000000Ec2EMAS\n",
       "522  1111  a1IMg000000Ekz3MAC\n",
       "523  8971  a1IMg000000F2vxMAC\n",
       "524  8972  a1IMg000000F341MAC\n",
       "525  8973  a1IMg000000F37FMAS\n",
       "\n",
       "[526 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\case_2024\\change_owner\\extract_case_type.csv\"\n",
    "\n",
    "df=pd.read_csv(path, delimiter=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_43472\\1811329917.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df.rename(columns={'NAME': 'CALL_TYPE_NAME'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>SCC_USER_GROUP__C</th>\n",
       "      <th>SCC_CASE_OWNER__C</th>\n",
       "      <th>CALL_TYPE_NAME</th>\n",
       "      <th>Priority</th>\n",
       "      <th>record_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000050358103</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "      <td>2202</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000050749322</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCC - Non Fraud Transaction - ACQUIRER</td>\n",
       "      <td>8899</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000050956050</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ORD - Fraud Management &amp; Recovery Desk</td>\n",
       "      <td>8711</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000050357999</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "      <td>2202</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000050357993</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "      <td>2202</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47099</th>\n",
       "      <td>TTB000078030896</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCC - Non Fraud Transaction - ISSUER</td>\n",
       "      <td>8227</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47100</th>\n",
       "      <td>TTB000054238436</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>ADMIN QSC</td>\n",
       "      <td>Migration</td>\n",
       "      <td>8713</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47101</th>\n",
       "      <td>TTB000054252514</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>ADMIN QSC</td>\n",
       "      <td>Migration</td>\n",
       "      <td>8713</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47102</th>\n",
       "      <td>TTB000054118155</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>ADMIN QSC</td>\n",
       "      <td>Migration</td>\n",
       "      <td>8713</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47103</th>\n",
       "      <td>TTB000078198317</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INV - Business Operation DPLK Team</td>\n",
       "      <td>8106</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47104 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SCC_LEGACY_TICKET_ID__C     STATUS SCC_USER_GROUP__C  \\\n",
       "0             TTB000050358103  Escalated               NaN   \n",
       "1             TTB000050749322  Escalated               NaN   \n",
       "2             TTB000050956050  Escalated               NaN   \n",
       "3             TTB000050357999  Escalated               NaN   \n",
       "4             TTB000050357993  Escalated               NaN   \n",
       "...                       ...        ...               ...   \n",
       "47099         TTB000078030896  Escalated               NaN   \n",
       "47100         TTB000054238436  Escalated         ADMIN QSC   \n",
       "47101         TTB000054252514  Escalated         ADMIN QSC   \n",
       "47102         TTB000054118155  Escalated         ADMIN QSC   \n",
       "47103         TTB000078198317  Escalated               NaN   \n",
       "\n",
       "                            SCC_CASE_OWNER__C CALL_TYPE_NAME Priority  \\\n",
       "0                        SCC - Contact Center           2202      Low   \n",
       "1      SCC - Non Fraud Transaction - ACQUIRER           8899      Low   \n",
       "2      ORD - Fraud Management & Recovery Desk           8711      Low   \n",
       "3                        SCC - Contact Center           2202      Low   \n",
       "4                        SCC - Contact Center           2202      Low   \n",
       "...                                       ...            ...      ...   \n",
       "47099    SCC - Non Fraud Transaction - ISSUER           8227      Low   \n",
       "47100                               Migration           8713      Low   \n",
       "47101                               Migration           8713      Low   \n",
       "47102                               Migration           8713      Low   \n",
       "47103      INV - Business Operation DPLK Team           8106      Low   \n",
       "\n",
       "          record_type  \n",
       "0      Case Migration  \n",
       "1      Case Migration  \n",
       "2      Case Migration  \n",
       "3      Case Migration  \n",
       "4      Case Migration  \n",
       "...               ...  \n",
       "47099  Case Migration  \n",
       "47100  Case Migration  \n",
       "47101  Case Migration  \n",
       "47102  Case Migration  \n",
       "47103  Case Migration  \n",
       "\n",
       "[47104 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "legacy_df = pd.read_csv(r\"D:\\Python2\\case_2024\\change_owner\\extract_legacy_calltype_usergroup_caseowner.csv\", delimiter=';')\n",
    "case_type_df = pd.read_csv(r\"D:\\Python2\\case_2024\\change_owner\\extract_case_type.csv\", delimiter=';')\n",
    "\n",
    "# Merge the dataframes based on the matching columns\n",
    "merged_df = legacy_df.merge(case_type_df, left_on='SCC_CALL_TYPE__C', right_on='ID', how='left')\n",
    "\n",
    "# Select relevant columns and rename 'NAME' to 'CALL_TYPE_NAME' for clarity\n",
    "final_df = merged_df[['SCC_LEGACY_TICKET_ID__C', 'SCC_CALL_TYPE__C', 'STATUS', 'SCC_USER_GROUP__C', 'OWNERID', 'CASE_OWNER_X_USER__C', 'SCC_CASE_OWNER__C', 'NAME']]\n",
    "final_df.rename(columns={'NAME': 'CALL_TYPE_NAME'}, inplace=True)\n",
    "\n",
    "final_df=final_df=final_df.drop(['SCC_CALL_TYPE__C', 'CASE_OWNER_X_USER__C','OWNERID'], axis=1)\n",
    "final_df['Priority']='Low'\n",
    "final_df['record_type']='Case Migration'\n",
    "final_df\n",
    "# len(final_df.SCC_USER_GROUP__C.unique())\n",
    "# final_df.to_csv(r\"D:\\Python2\\case_2024\\change_owner\\test_case_ownerUAT.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>SCC_USER_GROUP__C</th>\n",
       "      <th>SCC_CASE_OWNER__C</th>\n",
       "      <th>CALL_TYPE_NAME</th>\n",
       "      <th>Priority</th>\n",
       "      <th>record_type</th>\n",
       "      <th>case_owner</th>\n",
       "      <th>uker</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000051463308</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>CSM-1</td>\n",
       "      <td>CDD - Business Operation Team - ITEM</td>\n",
       "      <td>7200</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>00GMR0000000fqq2AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000051463308</td>\n",
       "      <td>Escalated</td>\n",
       "      <td>CSM-1</td>\n",
       "      <td>CDD - Business Operation Team - ITEM</td>\n",
       "      <td>7200</td>\n",
       "      <td>Low</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>SCC - Contact Center</td>\n",
       "      <td>#VALUE!</td>\n",
       "      <td>00GMR0000001ZOj2AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SCC_LEGACY_TICKET_ID__C     STATUS SCC_USER_GROUP__C  \\\n",
       "0         TTB000051463308  Escalated             CSM-1   \n",
       "1         TTB000051463308  Escalated             CSM-1   \n",
       "\n",
       "                      SCC_CASE_OWNER__C  CALL_TYPE_NAME Priority  \\\n",
       "0  CDD - Business Operation Team - ITEM            7200      Low   \n",
       "1  CDD - Business Operation Team - ITEM            7200      Low   \n",
       "\n",
       "      record_type            case_owner     uker                  ID  \n",
       "0  Case Migration  SCC - Contact Center  #VALUE!  00GMR0000000fqq2AA  \n",
       "1  Case Migration  SCC - Contact Center  #VALUE!  00GMR0000001ZOj2AM  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "extract_group_uat_path = r\"D:\\Python2\\case_2024\\change_owner\\extract_group_UAT.csv\"\n",
    "test_case_owner_uat_path = r\"D:\\Python2\\case_2024\\change_owner\\test_case_ownerUAT - 1 line.csv\"\n",
    "\n",
    "# Load the CSV files with the correct delimiter\n",
    "extract_group_uat = pd.read_csv(extract_group_uat_path, delimiter=';')\n",
    "test_case_owner_uat = pd.read_csv(test_case_owner_uat_path, delimiter=';')\n",
    "\n",
    "# Merging the dataframes on the 'case_owner' column\n",
    "merged_df = pd.merge(test_case_owner_uat, extract_group_uat, how='left', on='case_owner')\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'error_bad_lines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPython2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mremark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbricare_all_batch2_troublelog\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbricare_all_batch2_troublelog\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mremark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbricare_20240101_20240804_0_troublelog_closed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Function to correct Ticket_ID format\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_ticket_id\u001b[39m(ticket_id):\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'error_bad_lines'"
     ]
    }
   ],
   "source": [
    "\"C:\\Users\\maste\\Downloads\\Flexible_CSV_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading D:\\Python2\\remark\\clean: read_csv() got an unexpected keyword argument 'error_bad_lines'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "file_path = r\"D:\\Python2\\remark\\clean\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Read the CSV file into a DataFrame with error handling\n",
    "    df = pd.read_csv(file_path, error_bad_lines=False, warn_bad_lines=True)\n",
    "\n",
    "    # Count the number of rows in the DataFrame\n",
    "    row_count = len(df)\n",
    "\n",
    "    # Print the row count for the file\n",
    "    print(f'{file_path}: {row_count} rows')\n",
    "\n",
    "    # Print the total row count (since there's only one file here)\n",
    "    print(f'Total rows in the file: {row_count}')\n",
    "except Exception as e:\n",
    "    print(f'Error reading {file_path}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count all TTB in all files in a folder for REMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 'TTB' entries in all files within the folder: 3936352\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_ttb_in_file(file_path):\n",
    "    ttb_count = 0\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            ttb_count += line.count('TTB')\n",
    "    return ttb_count\n",
    "\n",
    "def count_ttb_in_folder(folder_path):\n",
    "    total_ttb_count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            total_ttb_count += count_ttb_in_file(file_path)\n",
    "    return total_ttb_count\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = r\"D:\\Python2\\remark\\bricare_all_batch2_troublelog\\bricare_all_batch2_troublelog\"\n",
    "\n",
    "# Count the number of 'TTB' entries in all files within the folder\n",
    "total_ttb_count = count_ttb_in_folder(folder_path)\n",
    "print(f\"Total number of 'TTB' entries in all files within the folder: {total_ttb_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique 'TTB' entries in all files within the folder: 16840\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def find_unique_ttb_in_file(file_path):\n",
    "    unique_ttb_set = set()\n",
    "    with open(file_path, 'r', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            ttb_matches = re.findall(r'TTB\\d{12}', line)\n",
    "            unique_ttb_set.update(ttb_matches)\n",
    "    return unique_ttb_set\n",
    "\n",
    "def find_unique_ttb_in_folder(folder_path):\n",
    "    total_unique_ttb_set = set()\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            unique_ttb_set = find_unique_ttb_in_file(file_path)\n",
    "            total_unique_ttb_set.update(unique_ttb_set)\n",
    "    return total_unique_ttb_set\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = r\"D:\\Python2\\remark\\clean\"\n",
    "\n",
    "# Find unique 'TTB' entries in all files within the folder\n",
    "total_unique_ttb_set = find_unique_ttb_in_folder(folder_path)\n",
    "print(f\"Total number of unique 'TTB' entries in all files within the folder: {len(total_unique_ttb_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCOUNT TO CHECK THE DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# file_path = r\"D:\\Python2\\case_2024\\5-6 aug\\data_akun_5_6\\error080724121347897.csv\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPython2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcase_2024\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m5-6 aug\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata_akun_5_6\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43merror080724122100281.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the dataframe to understand its structure\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "# file_path = r\"D:\\Python2\\case_2024\\5-6 aug\\data_akun_5_6\\error080724121347897.csv\"\n",
    "file_path - r\"D:\\Python2\\case_2024\\5-6 aug\\data_akun_5_6\\error080724122100281.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "df.head()\n",
    "\n",
    "# Filter rows where the 'ERROR' column does not contain the word 'duplicate'\n",
    "filtered_df = df[~df['ERROR'].str.contains('duplicate', case=False)]\n",
    "\n",
    "\n",
    "# Display the first few rows of the filtered dataframe\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_21000\\2974680954.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['status'].fillna('Escalated', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifno</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>Nominal</th>\n",
       "      <th>...</th>\n",
       "      <th>Tgl_In_Progress</th>\n",
       "      <th>Tgl_Returned</th>\n",
       "      <th>Ticket_Referensi</th>\n",
       "      <th>Tiket_Urgency</th>\n",
       "      <th>Tipe_Remark</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>users</th>\n",
       "      <th>Usergroup_ID</th>\n",
       "      <th>record_type</th>\n",
       "      <th>Legacy_Ticket_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RPY5441</td>\n",
       "      <td>TTB000049954663</td>\n",
       "      <td>8907</td>\n",
       "      <td>Taspen</td>\n",
       "      <td>2024-01-11 20:32:35</td>\n",
       "      <td>MMS</td>\n",
       "      <td>Request</td>\n",
       "      <td>ROSNITA</td>\n",
       "      <td>7.387010e+14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>8856.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0049954663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YGZ0987</td>\n",
       "      <td>TTB000052590234</td>\n",
       "      <td>8602</td>\n",
       "      <td>BRIMO</td>\n",
       "      <td>2024-05-09 16:47:07</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Platform &amp; Device Support</td>\n",
       "      <td>YUNDHASTRI WULANDARI</td>\n",
       "      <td>1.146010e+14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052590234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3565101280220705</td>\n",
       "      <td>TTB000052655911</td>\n",
       "      <td>1101</td>\n",
       "      <td>Sanggahan Pengajuan Kartu Kredit</td>\n",
       "      <td>2024-05-13 14:36:17</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Fraud / Suspicious Transactions</td>\n",
       "      <td>RISTRI ARIKA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052655911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBC5930</td>\n",
       "      <td>TTB000052760439</td>\n",
       "      <td>8907</td>\n",
       "      <td>Taspen</td>\n",
       "      <td>2024-05-17 13:37:08</td>\n",
       "      <td>SMS</td>\n",
       "      <td>Request</td>\n",
       "      <td>BESTY SUYANTO</td>\n",
       "      <td>4.200102e+13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>2908.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052760439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P045610</td>\n",
       "      <td>TTB000052765512</td>\n",
       "      <td>8907</td>\n",
       "      <td>Taspen</td>\n",
       "      <td>2024-05-17 14:30:58</td>\n",
       "      <td>SMS</td>\n",
       "      <td>Request</td>\n",
       "      <td>PAINTEN</td>\n",
       "      <td>4.010275e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>5490.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0052765512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>FA91164</td>\n",
       "      <td>TTB000054244476</td>\n",
       "      <td>1038</td>\n",
       "      <td>CERIA - Komplain Transaksi Tidak Diakui</td>\n",
       "      <td>2024-07-29 09:56:15</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Fraud / Suspicious Transactions</td>\n",
       "      <td>FATMA ALI HAMID LARIMA</td>\n",
       "      <td>5.660101e+13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-31 11:43:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>10102.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054244476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>ANEY666</td>\n",
       "      <td>TTB000054249329</td>\n",
       "      <td>8411</td>\n",
       "      <td>Salah Transfer antar BRI</td>\n",
       "      <td>2024-07-29 12:04:27</td>\n",
       "      <td>Phone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AHMAD HABIBI</td>\n",
       "      <td>3.554010e+14</td>\n",
       "      <td>2700000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-07-31 14:57:54</td>\n",
       "      <td>2024-07-31 10:12:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>5035.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054249329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>4687400201234600</td>\n",
       "      <td>TTB000054255107</td>\n",
       "      <td>2700</td>\n",
       "      <td>Informasi dan Pendaftaran Autodebet  Kartu Kredit</td>\n",
       "      <td>2024-07-29 14:57:11</td>\n",
       "      <td>Surat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JACUB SUTISNA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-07-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054255107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>NIP3888</td>\n",
       "      <td>TTB000054256033</td>\n",
       "      <td>8713</td>\n",
       "      <td>Gagal Top Up dan Gagal Transaksi BRIZZI tapi S...</td>\n",
       "      <td>2024-07-29 15:30:32</td>\n",
       "      <td>Walk-In</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>nuke julianti</td>\n",
       "      <td>5.080101e+13</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-01 15:32:51</td>\n",
       "      <td>2024-07-31 19:30:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>2301.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054256033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>B246201</td>\n",
       "      <td>TTB000054257005</td>\n",
       "      <td>8713</td>\n",
       "      <td>Gagal Top Up dan Gagal Transaksi BRIZZI tapi S...</td>\n",
       "      <td>2024-07-29 16:00:47</td>\n",
       "      <td>SMS</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>BOBBY RAHMADI</td>\n",
       "      <td>4.543010e+14</td>\n",
       "      <td>390000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-01 10:58:42</td>\n",
       "      <td>2024-07-31 14:49:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non Call</td>\n",
       "      <td>2301.0</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0054257005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                cifno        Ticket_ID  Call_Type_ID  \\\n",
       "0             RPY5441  TTB000049954663          8907   \n",
       "1             YGZ0987  TTB000052590234          8602   \n",
       "2    3565101280220705  TTB000052655911          1101   \n",
       "3             NBC5930  TTB000052760439          8907   \n",
       "4             P045610  TTB000052765512          8907   \n",
       "..                ...              ...           ...   \n",
       "247           FA91164  TTB000054244476          1038   \n",
       "248           ANEY666  TTB000054249329          8411   \n",
       "249  4687400201234600  TTB000054255107          2700   \n",
       "250           NIP3888  TTB000054256033          8713   \n",
       "251           B246201  TTB000054257005          8713   \n",
       "\n",
       "                                             Call_Type          Create_Date  \\\n",
       "0                                               Taspen  2024-01-11 20:32:35   \n",
       "1                                                BRIMO  2024-05-09 16:47:07   \n",
       "2                     Sanggahan Pengajuan Kartu Kredit  2024-05-13 14:36:17   \n",
       "3                                               Taspen  2024-05-17 13:37:08   \n",
       "4                                               Taspen  2024-05-17 14:30:58   \n",
       "..                                                 ...                  ...   \n",
       "247            CERIA - Komplain Transaksi Tidak Diakui  2024-07-29 09:56:15   \n",
       "248                           Salah Transfer antar BRI  2024-07-29 12:04:27   \n",
       "249  Informasi dan Pendaftaran Autodebet  Kartu Kredit  2024-07-29 14:57:11   \n",
       "250  Gagal Top Up dan Gagal Transaksi BRIZZI tapi S...  2024-07-29 15:30:32   \n",
       "251  Gagal Top Up dan Gagal Transaksi BRIZZI tapi S...  2024-07-29 16:00:47   \n",
       "\n",
       "     gateway                    Jenis_Laporan            Nama_Nasabah  \\\n",
       "0        MMS                          Request                 ROSNITA   \n",
       "1      Phone        Platform & Device Support    YUNDHASTRI WULANDARI   \n",
       "2      Phone  Fraud / Suspicious Transactions            RISTRI ARIKA   \n",
       "3        SMS                          Request           BESTY SUYANTO   \n",
       "4        SMS                          Request                 PAINTEN   \n",
       "..       ...                              ...                     ...   \n",
       "247    Phone  Fraud / Suspicious Transactions  FATMA ALI HAMID LARIMA   \n",
       "248    Phone                              NaN            AHMAD HABIBI   \n",
       "249    Surat                              NaN           JACUB SUTISNA   \n",
       "250  Walk-In          Complaint - Transaction           nuke julianti   \n",
       "251      SMS          Complaint - Transaction           BOBBY RAHMADI   \n",
       "\n",
       "      No_Rekening    Nominal  ...      Tgl_In_Progress         Tgl_Returned  \\\n",
       "0    7.387010e+14        0.0  ...                  NaN  2024-08-05 00:00:00   \n",
       "1    1.146010e+14        0.0  ...                  NaN  2024-08-05 00:00:00   \n",
       "2             NaN        0.0  ...                  NaN  2024-08-05 00:00:00   \n",
       "3    4.200102e+13        0.0  ...                  NaN  2024-08-05 00:00:00   \n",
       "4    4.010275e+11        0.0  ...                  NaN  2024-08-05 00:00:00   \n",
       "..            ...        ...  ...                  ...                  ...   \n",
       "247  5.660101e+13        0.0  ...                  NaN  2024-07-31 11:43:36   \n",
       "248  3.554010e+14  2700000.0  ...  2024-07-31 14:57:54  2024-07-31 10:12:28   \n",
       "249           NaN        0.0  ...                  NaN  2024-07-31 00:00:00   \n",
       "250  5.080101e+13   100000.0  ...  2024-08-01 15:32:51  2024-07-31 19:30:13   \n",
       "251  4.543010e+14   390000.0  ...  2024-08-01 10:58:42  2024-07-31 14:49:46   \n",
       "\n",
       "    Ticket_Referensi Tiket_Urgency Tipe_Remark  UniqueID     users  \\\n",
       "0                NaN           NaN       Notes       NaN  Non Call   \n",
       "1                NaN           NaN       Notes       NaN      Call   \n",
       "2                NaN           NaN       Notes       NaN      Call   \n",
       "3                NaN           NaN       Notes       NaN  Non Call   \n",
       "4                NaN           NaN       Notes       NaN  Non Call   \n",
       "..               ...           ...         ...       ...       ...   \n",
       "247              NaN           NaN       Notes       NaN      Call   \n",
       "248              NaN           NaN       Notes       NaN      Call   \n",
       "249              NaN           NaN       Notes       NaN  Non Call   \n",
       "250              NaN           NaN       Notes       NaN  Non Call   \n",
       "251              NaN           NaN       Notes       NaN  Non Call   \n",
       "\n",
       "     Usergroup_ID     record_type Legacy_Ticket_ID  \n",
       "0          8856.0  Case Migration    TTB0049954663  \n",
       "1             4.0  Case Migration    TTB0052590234  \n",
       "2             5.0  Case Migration    TTB0052655911  \n",
       "3          2908.0  Case Migration    TTB0052760439  \n",
       "4          5490.0  Case Migration    TTB0052765512  \n",
       "..            ...             ...              ...  \n",
       "247       10102.0  Case Migration    TTB0054244476  \n",
       "248        5035.0  Case Migration    TTB0054249329  \n",
       "249          15.0  Case Migration    TTB0054255107  \n",
       "250        2301.0  Case Migration    TTB0054256033  \n",
       "251        2301.0  Case Migration    TTB0054257005  \n",
       "\n",
       "[252 rows x 82 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\bricare_20240111_20240729_0_case_ttb_missing_79_clean.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df['status'].fillna('Escalated', inplace=True)\n",
    "empty_values_exist = df['status'].isnull().sum()\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\bricare_20240111_20240729_0_case_ttb_missing_79_clean_new.csv\", index=False)\n",
    "# empty_values_exist\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# path=r\"D:\\Python2\\case_2024\\7 aug\\update to closed\\bricare_all_update_status_clean\\error080824043430366.csv\"\n",
    "\n",
    "path=r\"D:\\Python2\\error081624121056959.csv\"\n",
    "\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df['Jenis_Nasabah'] = None  \n",
    "df['tipe_nasabah'] = 'Individu'\n",
    "df['record_type_1'] = 'Person Account'\n",
    "df.to_csv(r\"D:\\Python2\\error081624121056959_done.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [TroubleTicketId, Notes, Status]\n",
      "Index: []\n",
      "       TroubleTicketId Notes Status\n",
      "count                0     0      0\n",
      "unique               0     0      0\n",
      "top                NaN   NaN    NaN\n",
      "freq               NaN   NaN    NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to read and process the CSV file with a multi-character delimiter\n",
    "def read_and_clean_csv(file_path, delimiter):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    parsed_data = []\n",
    "    for line in lines:\n",
    "        if delimiter in line:\n",
    "            split_line = line.split(delimiter)\n",
    "            # Ensuring the split line has exactly 3 elements (TroubleTicketId, Notes, Status)\n",
    "            if len(split_line) == 3:\n",
    "                parsed_data.append(split_line)\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "# Define the file path and multi-character delimiter\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Drone_20240808.csv\"\n",
    "delimiter = '|-|-|'\n",
    "\n",
    "# Read and clean the new file using the custom delimiter\n",
    "new_cleaned_data = read_and_clean_csv(file_path, delimiter)\n",
    "\n",
    "# Convert the list of rows into a DataFrame\n",
    "cleaned_df = pd.DataFrame(new_cleaned_data, columns=['TroubleTicketId', 'Notes', 'Status'])\n",
    "\n",
    "# Filter rows where TroubleTicketId follows the TICKET format\n",
    "cleaned_df = cleaned_df[cleaned_df['TroubleTicketId'].apply(lambda x: re.match(r'^TICKET\\d+$', x) is not None)]\n",
    "\n",
    "# Further clean and format the DataFrame to match the expected output\n",
    "cleaned_df['Notes'] = cleaned_df['Notes'].str.replace('\\n', ' ').str.strip()\n",
    "cleaned_df['TroubleTicketId'] = cleaned_df['TroubleTicketId'].str.strip()\n",
    "cleaned_df['Status'] = cleaned_df['Status'].str.strip()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "cleaned_df.to_csv('Cleaned_Drone_20240808.csv', index=False)\n",
    "\n",
    "# Display the head and summary of the cleaned DataFrame\n",
    "print(cleaned_df.head())\n",
    "print(cleaned_df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\Transformed_Ticket_IDs.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\Transformed_Ticket_IDs_1line.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Nomer kartu dan Nomer rekening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with the correct delimiter\n",
    "file_path = '/mnt/data/ttbcc.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Define the transformation function\n",
    "def transform_ticket_id(ticket_id):\n",
    "    if ticket_id.startswith('TTB'):\n",
    "        return 'TTB' + ticket_id[3:].zfill(12)\n",
    "    return ticket_id\n",
    "\n",
    "# Apply the transformation to the scc_legacy_ticket_id__c column\n",
    "df['scc_legacy_ticket_id__c'] = df['scc_legacy_ticket_id__c'].apply(transform_ticket_id)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_file_path = '/mnt/data/transformed_ttbcc.csv'\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to parse the file based on the clarified structure with correct delimiter\n",
    "def parse_file_preserve_format(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = []\n",
    "        ticket_data = {}\n",
    "        current_ticket_id = None\n",
    "        current_notes = []\n",
    "        \n",
    "        for line in file:\n",
    "            if '|-|-|' in line:\n",
    "                parts = line.strip().split('|-|-|')\n",
    "                \n",
    "                ticket_id = parts[0].strip('\"')\n",
    "                if ticket_id.startswith(\"TICKET\") or ticket_id.startswith(\"TTB\"):\n",
    "                    if current_ticket_id is not None:\n",
    "                        ticket_data['Notes'] = '\\n'.join(current_notes).strip().strip('\"')\n",
    "                        data.append(ticket_data)\n",
    "                    \n",
    "                    ticket_data = {'TroubleTicketId': ticket_id, 'Notes': '', 'Status': ''}\n",
    "                    current_ticket_id = ticket_id\n",
    "                    current_notes = []\n",
    "                    \n",
    "                    if len(parts) > 1:\n",
    "                        current_notes.append(parts[1].strip())\n",
    "                    \n",
    "                    if len(parts) > 2:\n",
    "                        ticket_data['Status'] = parts[2].strip('\"')\n",
    "                else:\n",
    "                    if current_ticket_id is not None:\n",
    "                        current_notes.append(line.rstrip())\n",
    "            else:\n",
    "                if current_ticket_id is not None:\n",
    "                    current_notes.append(line.rstrip())\n",
    "        \n",
    "        if current_ticket_id is not None:\n",
    "            ticket_data['Notes'] = '\\n'.join(current_notes).strip().strip('\"')\n",
    "            data.append(ticket_data)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Parse the file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Drone\\drone20240808.csv\"\n",
    "df_parsed_correctly = parse_file_preserve_format(file_path)\n",
    "\n",
    "# Function to correctly extract Status from Notes\n",
    "def extract_status(notes):\n",
    "    if '|-|-|' in notes:\n",
    "        return notes.split('|-|-|')[-1].strip('\"')\n",
    "    return ''\n",
    "\n",
    "df_parsed_correctly['Status'] = df_parsed_correctly['Notes'].apply(extract_status)\n",
    "\n",
    "# Clean the Notes column to remove Status part\n",
    "df_parsed_correctly['Notes'] = df_parsed_correctly['Notes'].apply(lambda x: x.split('|-|-|')[0].strip())\n",
    "\n",
    "# Remove surrounding double quotes from Notes\n",
    "df_parsed_correctly['Notes'] = df_parsed_correctly['Notes'].apply(lambda x: x.strip('\"'))\n",
    "\n",
    "df_parsed_correctly.to_csv('cleaned_drone.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TroubleTicketId</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000053698249</td>\n",
       "      <td>{\\n  \\&amp;#34;NAMA MERCHANT\\&amp;#34;: \\&amp;#34;NASI KEB...</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000053698530</td>\n",
       "      <td>{\\n  \\&amp;#34;NAMA MERCHANT\\&amp;#34;: \\&amp;#34;CAMBRIGE...</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000053698541</td>\n",
       "      <td>{\\n  \\&amp;#34;NAMA MERCHANT\\&amp;#34;: \\&amp;#34;KAMADIKA...</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000053699338</td>\n",
       "      <td>{\\n  \\&amp;#34;NAMA MERCHANT\\&amp;#34;: \\&amp;#34;MOORI BE...</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000053699344</td>\n",
       "      <td>{\\n  \\&amp;#34;NAMA MERCHANT\\&amp;#34;: \\&amp;#34;BIANCA R...</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TroubleTicketId                                              Notes  Status\n",
       "0  TTB000053698249  {\\n  \\&#34;NAMA MERCHANT\\&#34;: \\&#34;NASI KEB...  Closed\n",
       "1  TTB000053698530  {\\n  \\&#34;NAMA MERCHANT\\&#34;: \\&#34;CAMBRIGE...  Closed\n",
       "2  TTB000053698541  {\\n  \\&#34;NAMA MERCHANT\\&#34;: \\&#34;KAMADIKA...  Closed\n",
       "3  TTB000053699338  {\\n  \\&#34;NAMA MERCHANT\\&#34;: \\&#34;MOORI BE...  Closed\n",
       "4  TTB000053699344  {\\n  \\&#34;NAMA MERCHANT\\&#34;: \\&#34;BIANCA R...  Closed"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to parse the file based on the clarified structure with correct delimiter\n",
    "def parse_file_preserve_format(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = []\n",
    "        ticket_data = {}\n",
    "        current_ticket_id = None\n",
    "        current_notes = []\n",
    "        \n",
    "        for line in file:\n",
    "            if '|-|-|' in line:\n",
    "                parts = line.strip().split('|-|-|')\n",
    "                \n",
    "                ticket_id = parts[0].strip('\"')\n",
    "                if ticket_id.startswith(\"TICKET\") or ticket_id.startswith(\"TTB\"):\n",
    "                    if current_ticket_id is not None:\n",
    "                        ticket_data['Notes'] = '\\n'.join(current_notes).strip().strip('\"')\n",
    "                        data.append(ticket_data)\n",
    "                    \n",
    "                    ticket_data = {'TroubleTicketId': ticket_id, 'Notes': '', 'Status': ''}\n",
    "                    current_ticket_id = ticket_id\n",
    "                    current_notes = []\n",
    "                    \n",
    "                    if len(parts) > 1:\n",
    "                        current_notes.append(parts[1].strip())\n",
    "                    \n",
    "                    if len(parts) > 2:\n",
    "                        ticket_data['Status'] = parts[2].strip('\"')\n",
    "                else:\n",
    "                    if current_ticket_id is not None:\n",
    "                        current_notes.append(line.rstrip())\n",
    "            else:\n",
    "                if current_ticket_id is not None:\n",
    "                    current_notes.append(line.rstrip())\n",
    "        \n",
    "        if current_ticket_id is not None:\n",
    "            ticket_data['Notes'] = '\\n'.join(current_notes).strip().strip('\"')\n",
    "            data.append(ticket_data)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to clean up Notes column\n",
    "def clean_notes(notes):\n",
    "    notes = notes.replace('\\\\n\\\\n', '\\n').replace('\\\\n', '\\n').replace('\\n\\\\', '\\n').strip()\n",
    "    return notes\n",
    "\n",
    "# Parse the file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Drone\\drone20240808.csv\"\n",
    "df_parsed_correctly = parse_file_preserve_format(file_path)\n",
    "\n",
    "# Function to correctly extract Status from Notes\n",
    "def extract_status(notes):\n",
    "    if '|-|-|' in notes:\n",
    "        return notes.split('|-|-|')[-1].strip('\"')\n",
    "    return ''\n",
    "\n",
    "df_parsed_correctly['Status'] = df_parsed_correctly['Notes'].apply(extract_status)\n",
    "\n",
    "# Clean the Notes column to remove Status part and clean newlines\n",
    "df_parsed_correctly['Notes'] = df_parsed_correctly['Notes'].apply(lambda x: clean_notes(x.split('|-|-|')[0].strip()))\n",
    "\n",
    "# Remove surrounding double quotes from Notes\n",
    "df_parsed_correctly['Notes'] = df_parsed_correctly['Notes'].apply(lambda x: x.strip('\"'))\n",
    "\n",
    "df_parsed_correctly.to_csv('cleaned_drone2.csv', index=False)\n",
    "df_parsed_correctly.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To close DRONE Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find max and min TTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TTB000078498061', 'TTB000078498646')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# path=r\"D:\\Python2\\datadroneclosed.csv\"\n",
    "path=r\"D:\\Python2\\Close5\\testing SABRINA.csv\"\n",
    "\n",
    "df=pd.read_csv(path, delimiter=';')\n",
    "# df=df.iloc[:1]\n",
    "# df.to_csv(r\"D:\\Python2\\extract_droneStatus_open_1lines.csv\", index=False)\n",
    "min_value = df['SCC_LEGACY_TICKET_ID__C'].min()\n",
    "max_value = df['SCC_LEGACY_TICKET_ID__C'].max()\n",
    "df.to_csv(path, index=False)\n",
    "min_value, max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000054447632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000054442717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000054446048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000054446181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000054419622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TTB000054412180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TTB000054412649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>TTB000054389346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>TTB000054441582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>TTB000054367079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>TTB000054439545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>TTB000054441282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>TTB000054413628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>TTB000054396168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>TTB000054390575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SCC_LEGACY_TICKET_ID__C\n",
       "0           TTB000054447632\n",
       "1           TTB000054442717\n",
       "2           TTB000054446048\n",
       "3           TTB000054446181\n",
       "4           TTB000054419622\n",
       "5           TTB000054412180\n",
       "6           TTB000054412649\n",
       "197         TTB000054389346\n",
       "198         TTB000054441582\n",
       "214         TTB000054367079\n",
       "221         TTB000054439545\n",
       "225         TTB000054441282\n",
       "226         TTB000054413628\n",
       "228         TTB000054396168\n",
       "230         TTB000054390575"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv_file_path_1 = r\"D:\\Python2\\Close4\\cobaclosed1.csv\"\n",
    "csv_file_path_2 = r\"D:\\Python2\\Close4\\save_merged_file.csv\"\n",
    "\n",
    "# Reading the CSV files\n",
    "csv_df_1 = pd.read_csv(csv_file_path_1)\n",
    "csv_df_2 = pd.read_csv(csv_file_path_2)\n",
    "\n",
    "# Extracting the SCC_LEGACY_TICKET_ID__C column from both dataframes\n",
    "ids_1 = csv_df_1['SCC_LEGACY_TICKET_ID__C']\n",
    "ids_2 = csv_df_2['SCC_LEGACY_TICKET_ID__C']\n",
    "\n",
    "# Identifying missing values\n",
    "missing_in_1 = ids_2[~ids_2.isin(ids_1)]\n",
    "missing_in_2 = ids_1[~ids_1.isin(ids_2)]\n",
    "\n",
    "# Creating DataFrames for missing values\n",
    "missing_in_1_df = pd.DataFrame(missing_in_1, columns=['SCC_LEGACY_TICKET_ID__C'])\n",
    "missing_in_2_df = pd.DataFrame(missing_in_2, columns=['SCC_LEGACY_TICKET_ID__C'])\n",
    "\n",
    "# Saving the results to CSV files\n",
    "# missing_in_1_df.to_csv('missing_in_cobaclosed1.csv', index=False)\n",
    "missing_in_2_df.to_csv('missing_in_save_merged_file.csv', index=False)\n",
    "missing_in_2_df\n",
    "# print(\"Missing values in cobaclosed1.csv saved to missing_in_cobaclosed1.csv\")\n",
    "# print(\"Missing values in save_merged_file.csv saved to missing_in_save_merged_file.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "extract_file_path = r\"D:\\Python2\\Close7\\extract_23_done.csv\"\n",
    "datadroneclosed_file_path = r\"D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy.csv\"\n",
    "\n",
    "extract_df = pd.read_csv(extract_file_path)\n",
    "datadroneclosed_df = pd.read_csv(datadroneclosed_file_path, delimiter=';')\n",
    "\n",
    "# Identify the columns in extract that are not in datadroneclosed\n",
    "remaining_columns = [col for col in extract_df.columns if col not in datadroneclosed_df.columns]\n",
    "\n",
    "# Merge the dataframes: take all columns from datadroneclosed and the remaining columns from extract\n",
    "merged_df = pd.merge(datadroneclosed_df, extract_df[remaining_columns + ['SCC_LEGACY_TICKET_ID__C']], on='SCC_LEGACY_TICKET_ID__C', how='inner')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "output_file_path = r\"D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\" \n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\Close7\\extract_117_dataloader.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df[df['STATUS']==\"Escalated\"]\n",
    "df.to_csv(\"D:\\Python2\\Close7\\extract_117_dataloader_open.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Drone ADI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17640\\2193017327.py:8: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  extract_df = pd.read_csv(extract_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to D:\\Python2\\filtered_extract_drone_not_migration2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "extract_file_path = r\"D:\\extract_drone_.csv\"\n",
    "datadroneclosed_file_path = r\"D:\\Python2\\drone_open.csv\"  \n",
    "\n",
    "# Load the dataframes\n",
    "extract_df = pd.read_csv(extract_file_path)\n",
    "datadroneclosed_df = pd.read_csv(datadroneclosed_file_path, delimiter=';')\n",
    "\n",
    "# Get the list of SCC_LEGACY_TICKET_ID__C from datadroneclosed_df\n",
    "filter_ids = datadroneclosed_df['CASENUMBER'].unique()\n",
    "\n",
    "# Filter the extract_df based on the SCC_LEGACY_TICKET_ID__C values in datadroneclosed_df\n",
    "filtered_df = extract_df[extract_df['CASENUMBER'].isin(filter_ids)]\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file\n",
    "filtered_output_file_path = r'D:\\Python2\\filtered_extract_drone_not_migration2.csv'  # Update this with your desired output file path\n",
    "filtered_df.to_csv(filtered_output_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {filtered_output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix no telp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to D:\\Python2\\Close5\\save_merged_file_sabrina_done3.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Load the merged CSV file\n",
    "# merged_file_path = r\"D:\\Python2\\Close5\\save_merged_file_sabrina_done2.csv\"\n",
    "\n",
    "# # Function to clean phone numbers\n",
    "# def clean_phone_number(phone):\n",
    "#     if pd.isna(phone):\n",
    "#         return phone\n",
    "#     # Remove non-numeric characters\n",
    "#     phone = re.sub(r'\\D', '', str(phone))\n",
    "#     # Ensure the phone number is between 9 and 14 digits\n",
    "#     if 9 <= len(phone) <= 14:\n",
    "#         return phone\n",
    "#     return ''  # Return empty if it doesn't meet the criteria\n",
    "\n",
    "# # Apply the function to the Cust_Current_Phone__c column\n",
    "# merged_df['CUST_CURRENT_PHONE__C'] = merged_df['CUST_CURRENT_PHONE__C'].apply(clean_phone_number)\n",
    "\n",
    "# # Save the cleaned file\n",
    "# cleaned_file_path = r\"D:\\Python2\\Close5\\save_merged_file_sabrina_done3.csv\"\n",
    "# merged_df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# print(f\"Cleaned data saved to {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_4360\\1078473945.py:5: DtypeWarning: Columns (11,33,45,60,64,65,96,112,120,124,127,130,134,136,142,150,167,173,174,190,209,210,220,221,273,275,276,284) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    8.123045e+13\n",
       "1             NaN\n",
       "2    8.536053e+13\n",
       "3    8.564362e+13\n",
       "4    8.515682e+13\n",
       "Name: CUST_CURRENT_PHONE__C, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\save_merged_file_7k.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df['CUST_CURRENT_PHONE__C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python2\\\\Close4\\\\save_merged_file2.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the uploaded CSV file\n",
    "# input_file_path = r\"D:\\Python2\\Close4\\save_merged_file2.csv\"\n",
    "# df_latest = pd.read_csv(input_file_path)\n",
    "\n",
    "# # Reprocess the dates by forcing them to be timezone-naive\n",
    "# def correct_dates_force_timezone_naive(row):\n",
    "#     try:\n",
    "#         # Convert to datetime and force remove any timezones\n",
    "#         row['CREATEDDATE'] = pd.to_datetime(row['CREATEDDATE'], errors='coerce').tz_localize(None)\n",
    "#         row['CLOSEDDATE'] = pd.to_datetime(row['CLOSEDDATE'], errors='coerce').tz_localize(None)\n",
    "\n",
    "#         # Apply the rule: if CLOSEDDATE < CREATEDDATE, set CREATEDDATE = CLOSEDDATE\n",
    "#         if pd.notnull(row['CLOSEDDATE']) and pd.notnull(row['CREATEDDATE']):\n",
    "#             if row['CLOSEDDATE'] < row['CREATEDDATE']:\n",
    "#                 row['CREATEDDATE'] = row['CLOSEDDATE']\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing row: {e}\")  # Print the error for debugging, remove if not needed\n",
    "\n",
    "#     return row\n",
    "\n",
    "# # Apply the date correction function to the dataframe\n",
    "# df_final_corrected = df_latest.apply(correct_dates_force_timezone_naive, axis=1)\n",
    "\n",
    "# # Save the corrected dataframe to a new CSV file\n",
    "# output_file_path_final_corrected = r\"D:\\Python2\\Close4\\save_merged_file2.csv\"\n",
    "# df_final_corrected.to_csv(output_file_path_final_corrected, index=False)\n",
    "\n",
    "# output_file_path_final_corrected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Convert the 'CLOSEDDATE' column using the custom parser\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLOSEDDATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCLOSEDDATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_date\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Save the updated dataframe back to a CSV file\u001b[39;00m\n\u001b[0;32m     26\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPython2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mClose7\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNew Bricare Issuer 2 Over SLA(3) - Copy.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mparse_date\u001b[1;34m(date_str)\u001b[0m\n\u001b[0;32m      6\u001b[0m months \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJanuari\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJanuary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFebruari\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFebruary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaret\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarch\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApril\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApril\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMei\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJuni\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJune\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJuli\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJuly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgustus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAugust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeptember\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeptember\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOktober\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOctober\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNovember\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNovember\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDesember\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecember\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, eng \u001b[38;5;129;01min\u001b[39;00m months\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 14\u001b[0m     date_str \u001b[38;5;241m=\u001b[39m \u001b[43mdate_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(ind, eng)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mstrptime(date_str, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom date parser function to handle Indonesian month names\n",
    "def parse_date(date_str):\n",
    "    months = {\n",
    "        'Januari': 'January', 'Februari': 'February', 'Maret': 'March',\n",
    "        'April': 'April', 'Mei': 'May', 'Juni': 'June',\n",
    "        'Juli': 'July', 'Agustus': 'August', 'September': 'September',\n",
    "        'Oktober': 'October', 'November': 'November', 'Desember': 'December'\n",
    "    }\n",
    "    \n",
    "    for ind, eng in months.items():\n",
    "        date_str = date_str.replace(ind, eng)\n",
    "    \n",
    "    return datetime.strptime(date_str, '%d %B %Y')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Close7\\New Bricare Issuer 2 Over SLA(3) - Copy.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Convert the 'CLOSEDDATE' column using the custom parser\n",
    "df['CLOSEDDATE'] = df['CLOSEDDATE'].apply(parse_date).dt.strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Save the updated dataframe back to a CSV file\n",
    "output_file_path = r\"D:\\Python2\\Close7\\New Bricare Issuer 2 Over SLA(3) - Copy.csv\"\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File has been updated and saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been updated and saved as D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom date parser function to handle Indonesian month names\n",
    "def parse_date_safe(date_str):\n",
    "    # Check if the input is a string to avoid issues with NaN or non-string values\n",
    "    if isinstance(date_str, str):\n",
    "        months = {\n",
    "            'Januari': 'January', 'Februari': 'February', 'Maret': 'March',\n",
    "            'April': 'April', 'Mei': 'May', 'Juni': 'June',\n",
    "            'Juli': 'July', 'Agustus': 'August', 'September': 'September',\n",
    "            'Oktober': 'October', 'November': 'November', 'Desember': 'December'\n",
    "        }\n",
    "        \n",
    "        # Replace Indonesian month names with English equivalents\n",
    "        for ind, eng in months.items():\n",
    "            date_str = date_str.replace(ind, eng)\n",
    "        \n",
    "        # Parse the date string into a datetime object\n",
    "        return datetime.strptime(date_str, '%d %B %Y')\n",
    "    return date_str  # Return as-is if not a string\n",
    "\n",
    "# Load the CSV file with the correct delimiter\n",
    "file_path = r\"D:\\Python2\\Close7\\New Bricare Issuer 2 Over SLA(3) - Copy.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Apply the date parsing function and handle NaT values properly\n",
    "df['CLOSEDDATE'] = df['CLOSEDDATE'].apply(parse_date_safe).apply(\n",
    "    lambda x: x.strftime('%d/%m/%Y %H:%M:%S') if isinstance(x, pd.Timestamp) else ''\n",
    ")\n",
    "\n",
    "# Save the updated dataframe back to a CSV file\n",
    "output_file_path = r\"D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy.csv\"\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"File has been updated and saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved to: D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the CLOSEDDATE column to the desired format 'yyyy-mm-dd hh:mm'\n",
    "df['CLOSEDDATE'] = pd.to_datetime(df['CLOSEDDATE'], format='%d/%m/%Y %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "updated_file_path = r\"D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\"\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Updated file saved to: {updated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status and to fix no telp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\Close7\\extract_117_dataloader_open_cleaned.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "# df['CLOSEDDATE']=df['CREATEDDATE']\n",
    "\n",
    "# df['CREATEDDATE'] = pd.to_datetime(df['CREATEDDATE'])\n",
    "# df['CLOSEDDATE'] = df['CREATEDDATE'] + timedelta(hours=1)\n",
    "\n",
    "\n",
    "# df['SCC_CUST_PHONE1__C'] = '0' + df['SCC_CUST_PHONE1__C'].astype(str)\n",
    "# df['CUST_CURRENT_PHONE__C'] = '0' + df['CUST_CURRENT_PHONE__C'].astype(str)\n",
    "df['SUPPLIEDPHONE'] = '0' + df['SUPPLIEDPHONE'].astype(str)\n",
    "\n",
    "# df['STATUS']='Closed'\n",
    "\n",
    "df.to_csv(r\"D:\\Python2\\Close7\\extract_117_dataloader_open_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Drone for ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_26632\\1762696848.py:6: DtypeWarning: Columns (17,44,61,62,93,97,104,109,110,111,120,127,133,135,136,139,141,145,148,151,165,172,175,189,208,209,219,273,283) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.read_csv(r\"D:\\Python2\\Close2\\extract_Prod.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1281, 300), 'D:\\\\Python2\\\\save_merged_file_1281.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged CSV file\n",
    "# merged_df = pd.read_csv(r\"D:\\Python2\\save_merged_file.csv\")\n",
    "\n",
    "merged_df = pd.read_csv(r\"D:\\Python2\\Close2\\extract_Prod.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "# error_df = pd.read_excel(r\"D:\\Python2\\error_221.xlsx\")\n",
    "\n",
    "error_df = pd.read_excel(r\"D:\\Python2\\Close2\\1281.xlsx\")\n",
    "\n",
    "# Filter the merged dataframe using the ticket IDs from the error dataframe\n",
    "filtered_df = merged_df[merged_df['SCC_LEGACY_TICKET_ID__C'].isin(error_df['SCC_LEGACY_TICKET_ID__C'])]\n",
    "\n",
    "# filtered_df['CLOSEDDATE'] = pd.to_datetime(df['CLOSEDDATE'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file\n",
    "filtered_file_path = r\"D:\\Python2\\save_merged_file_1281.csv\"\n",
    "\n",
    "filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "filtered_df.shape, filtered_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add additional sentences in column SCC_DRONE_REMARK_UPDATE__C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been updated and saved as D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add the sentences from 'REMARK' to the end of 'SCC_DRONE_REMARK_UPDATE__C' on a new line\n",
    "df['SCC_DRONE_REMARK_UPDATE__C'] = df['SCC_DRONE_REMARK_UPDATE__C'].fillna('') + '\\n' + df['REMARK'].fillna('')\n",
    "\n",
    "# Save the updated dataframe back to a CSV file\n",
    "output_file_path = r\"D:\\Python2\\Close7\\Updated_New_Bricare_Issuer_2_Over_SLA_Copy_DONE.csv\" \n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File has been updated and saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV file\n",
    "# file_path = r\"D:\\Python2\\Close2\\extract_1line_UAT.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Define the additional sentence to be added\n",
    "# additional_sentence = \"This is the new sentence.\"\n",
    "\n",
    "# # Function to add the sentence in a new line\n",
    "# def add_sentence_in_new_line(remark):\n",
    "#     if pd.isna(remark):  # Check if the cell is empty or NaN\n",
    "#         return additional_sentence\n",
    "#     else:\n",
    "#         return f\"{remark}\\n{additional_sentence}\"\n",
    "\n",
    "# # Apply the function to the SCC_DRONE_REMARK_UPDATE__C column\n",
    "# df['SCC_DRONE_REMARK_UPDATE__C'] = df['SCC_DRONE_REMARK_UPDATE__C'].apply(add_sentence_in_new_line)\n",
    "\n",
    "# # Save the modified DataFrame back to the CSV file\n",
    "# df.to_csv(r\"D:\\Python2\\Close2\\extract_1line_UAT_remarkupdated.csv\", index=False)\n",
    "\n",
    "# print(\"The additional sentence has been added and the file is saved as 'extract_1line_UAT_updated.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "# file_path = r\"D:\\Python2\\Close2\\save_merged_file_1281.csv\"\n",
    "file_path=r\"D:\\Python2\\Close6\\extract_17_done.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the additional sentence to be added\n",
    "additional_sentence = \"Mohon lengkapi dengan emergency receipt/ struk transaksi.\"\n",
    "\n",
    "# Function to add the sentence in a new line\n",
    "def add_sentence_in_new_line(remark):\n",
    "    if pd.isna(remark): \n",
    "        return additional_sentence\n",
    "    else:\n",
    "        return f\"{remark}\\n{additional_sentence}\"\n",
    "\n",
    "# Apply the function to the SCC_DRONE_REMARK_UPDATE__C column\n",
    "df['SCC_DRONE_REMARK_UPDATE__C'] = df['SCC_DRONE_REMARK_UPDATE__C'].apply(add_sentence_in_new_line)\n",
    "\n",
    "# # Fill the STATUS column with \"Closed\"\n",
    "# df['STATUS'] = 'Closed'\n",
    "\n",
    "# # Fill the CLOSEDDATE column with the same value as CREATEDDATE\n",
    "# df['CLOSEDDATE'] = df['CREATEDDATE']\n",
    "\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(r\"D:\\Python2\\Close6\\extract_17_done.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to remove .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in 'SCC_CARD_NUMBER__C' have '.0' removed.\n",
      "All values in 'SCC_ACCOUNT_NUMBER__C' have '.0' removed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"D:\\Python2\\Close7\\extract_117_dataloader_open.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove \".0\" at the end of values in 'SCC_CARD_NUMBER__C' and 'SCC_ACCOUNT_NUMBER__C'\n",
    "df['SCC_CARD_NUMBER__C'] = df['SCC_CARD_NUMBER__C'].astype(str).str.replace('.0', '', regex=False)\n",
    "df['SCC_ACCOUNT_NUMBER__C'] = df['SCC_ACCOUNT_NUMBER__C'].astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "# Convert 'CREATEDDATE' to datetime and add 3 business days\n",
    "df['CREATEDDATE'] = pd.to_datetime(df['CREATEDDATE'], errors='coerce')\n",
    "df['CLOSEDDATE'] = df['CREATEDDATE'] + BDay(3)\n",
    "\n",
    "# # Save the modified DataFrame to a new CSV file\n",
    "# output_file_path = r\"D:\\Python2\\Close7\\extract_117_dataloader_open_cleaned.csv\"\n",
    "# df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# print(\"Modifications completed and saved to:\", output_file_path)\n",
    "# Check if '.0' is still present in SCC_CARD_NUMBER__C and SCC_ACCOUNT_NUMBER__C\n",
    "\n",
    "# Check SCC_CARD_NUMBER__C\n",
    "if df['SCC_CARD_NUMBER__C'].str.endswith('.0').any():\n",
    "    print(\"Some values in 'SCC_CARD_NUMBER__C' still end with '.0'.\")\n",
    "else:\n",
    "    print(\"All values in 'SCC_CARD_NUMBER__C' have '.0' removed.\")\n",
    "\n",
    "# Check SCC_ACCOUNT_NUMBER__C\n",
    "if df['SCC_ACCOUNT_NUMBER__C'].str.endswith('.0').any():\n",
    "    print(\"Some values in 'SCC_ACCOUNT_NUMBER__C' still end with '.0'.\")\n",
    "else:\n",
    "    print(\"All values in 'SCC_ACCOUNT_NUMBER__C' have '.0' removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\Close7\\extract_117_dataloader_open_cleaned.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df[df['ACCOUNTID']=='001Mg0000089X1VIAU']\n",
    "df=df[df['CUST_CURRENT_PHONE__C']==0]\n",
    "df['CUST_CURRENT_PHONE__C']=''\n",
    "df.to_csv(r\"D:\\Python2\\Close7\\extract_117_dataloader_open_cleaned_1lineerror.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_4360\\1763359392.py:4: DtypeWarning: Columns (11,33,45,60,64,65,96,112,120,124,127,130,134,136,142,150,167,173,174,190,209,210,220,221,273,275,276,284) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  filtered_df = pd.read_csv(r\"D:\\Python2\\save_merged_file_7k.csv\")\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_4360\\1763359392.py:8: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  filtered_df['CLOSEDDATE'] = pd.to_datetime(filtered_df['CLOSEDDATE'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7395, 300), 'D:\\\\Python2\\\\save_merged_file_7k.csv')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "filtered_df = pd.read_csv(r\"D:\\Python2\\save_merged_file_7k.csv\")\n",
    "\n",
    "# Convert the 'CREATEDDATE' and 'CLOSEDDATE' columns to datetime format\n",
    "filtered_df['CREATEDDATE'] = pd.to_datetime(filtered_df['CREATEDDATE'])\n",
    "filtered_df['CLOSEDDATE'] = pd.to_datetime(filtered_df['CLOSEDDATE'])\n",
    "\n",
    "# Specify the file path to save the DataFrame\n",
    "filtered_file_path = r\"D:\\Python2\\save_merged_file_7k.csv\"\n",
    "\n",
    "# Save the DataFrame back to a CSV file\n",
    "filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "# Output the shape of the DataFrame and the file path\n",
    "filtered_df.shape, filtered_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waktu Transaksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\error080824083548216_clean.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"D:\\error080824083548216.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Generate a list of random times within the same day\n",
    "base_time = datetime.strptime('2024-08-07T07:00:00.000+0000', '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "time_deltas = [timedelta(hours=i) for i in range(24)]  # List of hourly intervals within a day\n",
    "\n",
    "# Randomly assign these times to the column\n",
    "random_times = [base_time + random.choice(time_deltas) for _ in range(len(df))]\n",
    "df['SCC_WAKTU_TRANSAKSI__C'] = [time.strftime('%Y-%m-%dT%H:%M:%S.%f%z') for time in random_times]\n",
    "\n",
    "# Save the modified dataframe\n",
    "output_file_path = r\"D:\\error080824083548216_clean.csv\"\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file_path = r\"D:\\error080824083548216.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid phone numbers saved to: valid_phones.csv\n",
      "Invalid phone numbers saved to: invalid_phones.csv\n",
      "Combined file saved to: combined_phones.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"D:\\error080824083548216.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Generate a list of random times within the same day\n",
    "base_time = datetime.strptime('2024-08-07T07:00:00.000+0000', '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "time_deltas = [timedelta(hours=i) for i in range(24)]  # List of hourly intervals within a day\n",
    "\n",
    "# Randomly assign these times to the column\n",
    "random_times = [base_time + random.choice(time_deltas) for _ in range(len(df))]\n",
    "df['SCC_WAKTU_TRANSAKSI__C'] = [time.strftime('%Y-%m-%dT%H:%M:%S.%f%z') for time in random_times]\n",
    "\n",
    "# Function to check if phone number is valid\n",
    "def is_valid_phone(phone):\n",
    "    # Define a simple regex for a valid phone number (adjust as needed)\n",
    "    phone_pattern = re.compile(r'^\\+?\\d[\\d\\s-]{7,}\\d$')\n",
    "    return bool(phone_pattern.match(phone))\n",
    "\n",
    "# Convert phone numbers to string and then validate\n",
    "df['CUST_CURRENT_PHONE__C'] = df['CUST_CURRENT_PHONE__C'].astype(str)\n",
    "\n",
    "# Check the phone numbers\n",
    "df['is_valid_phone'] = df['CUST_CURRENT_PHONE__C'].apply(is_valid_phone)\n",
    "\n",
    "# Split into valid and invalid phone numbers\n",
    "valid_phones_df = df[df['is_valid_phone']].copy()\n",
    "invalid_phones_df = df[~df['is_valid_phone']].copy()\n",
    "\n",
    "# Add zero in front of each valid phone number\n",
    "valid_phones_df['CUST_CURRENT_PHONE__C'] = '0' + valid_phones_df['CUST_CURRENT_PHONE__C']\n",
    "\n",
    "# Set the CUST_CURRENT_PHONE__C column to empty for invalid phone numbers\n",
    "invalid_phones_df['CUST_CURRENT_PHONE__C'] = ''\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "valid_output_file_path = 'valid_phones.csv'\n",
    "invalid_output_file_path = 'invalid_phones.csv'\n",
    "combined_output_file_path = 'combined_phones.csv'\n",
    "\n",
    "valid_phones_df.to_csv(valid_output_file_path, index=False)\n",
    "invalid_phones_df.to_csv(invalid_output_file_path, index=False)\n",
    "\n",
    "# Combine valid and invalid phone dataframes for the combined file\n",
    "combined_df = pd.concat([valid_phones_df, invalid_phones_df])\n",
    "combined_df.to_csv(combined_output_file_path, index=False)\n",
    "\n",
    "print(f\"Valid phone numbers saved to: {valid_output_file_path}\")\n",
    "print(f\"Invalid phone numbers saved to: {invalid_output_file_path}\")\n",
    "print(f\"Combined file saved to: {combined_output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all Tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_0_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_1000001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_1200001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_1400001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_1600001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_1800001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_200001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_400001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_600001_79_closed.csv\n",
      "'Ticket_ID' column not found in file: ._bricare_20240101_20240807_800001_79_closed.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 1618, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Read each CSV file with a fallback encoding and specify the delimiter\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# or 'latin1'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 1618, saw 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory containing your CSV files\n",
    "folder_path = r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\Bricare Case Closed PT 1\"\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through all the CSV files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Read each CSV file with a fallback encoding and specify the delimiter\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', delimiter=';')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1', delimiter=';')  # or 'latin1'\n",
    "        \n",
    "        # Check if 'Ticket_ID' column exists\n",
    "        if 'Ticket_ID' in df.columns:\n",
    "            # Extract the Ticket_ID column and store it in the list\n",
    "            dfs.append(df[['Ticket_ID']])\n",
    "        else:\n",
    "            print(f\"'Ticket_ID' column not found in file: {file_name}\")\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "if dfs:  # Proceed if there are any DataFrames to concatenate\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save the combined Ticket_IDs to a new CSV file\n",
    "    combined_df.to_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\combines_ticketID2.csv\", index=False)\n",
    "else:\n",
    "    print(\"No valid 'Ticket_ID' columns found in any of the files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find all common values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000049844098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000051015925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000051591293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000051845825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000051965050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TTB000050134135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TTB000050160303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TTB000052956480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TTB000053067022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TTB000053327530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TTB000053503626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TTB000050357988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TTB000050358041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TTB000050357993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TTB000050358103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TTB000050358033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TTB000050251544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TTB000050358042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TTB000050357992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TTB000050357998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TTB000050358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TTB000050357999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TTB000050544921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TTB000050749322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TTB000050956050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ticket_ID\n",
       "0   TTB000049844098\n",
       "1   TTB000051015925\n",
       "2   TTB000051591293\n",
       "3   TTB000051845825\n",
       "4   TTB000051965050\n",
       "5   TTB000050134135\n",
       "6   TTB000050160303\n",
       "7   TTB000052956480\n",
       "8   TTB000053067022\n",
       "9   TTB000053327530\n",
       "10  TTB000053503626\n",
       "11  TTB000050357988\n",
       "12  TTB000050358041\n",
       "13  TTB000050357993\n",
       "14  TTB000050358103\n",
       "15  TTB000050358033\n",
       "16  TTB000050251544\n",
       "17  TTB000050358042\n",
       "18  TTB000050357992\n",
       "19  TTB000050357998\n",
       "20  TTB000050358000\n",
       "21  TTB000050357999\n",
       "22  TTB000050544921\n",
       "23  TTB000050749322\n",
       "24  TTB000050956050"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files into pandas DataFrames\n",
    "# file1_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\New folder\\combines_ticketID.csv\")  # Replace 'file1.csv' with your actual file path\n",
    "# file2_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\extract_check_closed24.csv\", delimiter=';')  # Replace 'file2.csv' with your actual file path\n",
    "\n",
    "file1_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\combines_ticketID - 26 no status.csv\")  # Replace 'file1.csv' with your actual file path\n",
    "file2_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\extract_check_closed24.csv\", delimiter=';')  # Replace 'file2.csv' with your actual file path\n",
    "\n",
    "# Filter the DataFrame from File 1 to include only those rows where Ticket_ID is also in File 2\n",
    "filtered_df = file1_df[file1_df['Ticket_ID'].isin(file2_df['Ticket_ID'])]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\sf_open_bricare_closed2.csv\", index=False)  # Replace 'filtered_file1.csv' with your desired file name\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To find common values 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_27896\\1898664671.py:16: DtypeWarning: Columns (15,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file2_df = pd.read_csv(file2_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Ticket_ID]\n",
       "Index: []"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the paths for the files\n",
    "file1_path = r\"D:\\extract_new_status_done_onlyTTB.csv\"\n",
    "folder_path = r\"D:\\Python2\\case_2024\\7 aug\\update to closed\\bricare_all_update_status_clean\" # Replace with the actual folder containing multiple CSV files\n",
    "\n",
    "# Load the first file into a pandas DataFrame\n",
    "file1_df = pd.read_csv(file1_path)\n",
    "file1_df\n",
    "\n",
    "# Iterate over each file in the folder (for this example, just the uploaded file)\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.startswith(\"bricare\") and filename.endswith(\".csv\"):  # Process only bricare CSV files\n",
    "        file2_path = os.path.join(folder_path, filename)\n",
    "        file2_df = pd.read_csv(file2_path)\n",
    "\n",
    "        # Filter the DataFrame from File 1 to include only those rows where Ticket_ID is also in File 2\n",
    "        filtered_df = file1_df[file1_df['Ticket_ID'].isin(file2_df['Ticket_ID'])]\n",
    "\n",
    "        # Save the filtered DataFrame to a new CSV file, named after the original File 2\n",
    "        output_filename = f\"sf_open_bricare_closed2_{filename}\"\n",
    "        output_path = os.path.join(folder_path, output_filename)\n",
    "        # filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the last processed DataFrame (if you want to check the result)\n",
    "filtered_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To serach all relevant information based on Ticket_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 43, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbricare_*_closed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Load the current bricare file\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         bricare_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust encoding if needed\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Filter the DataFrame to include only rows where Ticket_ID exists in sf_open_df\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         filtered_df \u001b[38;5;241m=\u001b[39m bricare_df[bricare_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicket_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(sf_open_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicket_ID\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 43, saw 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load the sf_open_bricare_closed file\n",
    "# sf_open_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\extract_nomer_kartu_ttb.csv\", encoding='ISO-8859-1')  # Adjust encoding if needed\n",
    "sf_open_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\combines_ticketID - 26 no status - 2.csv\")  # Adjust encoding if needed\n",
    "\n",
    "# Specify the folder containing the bricare files\n",
    "folder_path = r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\bricare_all_closed_batch2_2024\"  # Replace with the folder containing your bricare files\n",
    "# output_file = r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\no_kartu.csv\"  # Replace with your desired output file path\n",
    "output_file = r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\to_close2.csv\"  # Replace with your desired output file path\n",
    "\n",
    "# Initialize an empty list to store the filtered DataFrames\n",
    "filtered_dfs = []\n",
    "\n",
    "# Iterate over all files matching the pattern 'bricare_*_closed.csv'\n",
    "for file_name in glob.glob(os.path.join(folder_path, 'bricare_*_closed.csv')):\n",
    "    try:\n",
    "        # Load the current bricare file\n",
    "        bricare_df = pd.read_csv(file_name, encoding='ISO-8859-1')  # Adjust encoding if needed\n",
    "        \n",
    "        # Filter the DataFrame to include only rows where Ticket_ID exists in sf_open_df\n",
    "        filtered_df = bricare_df[bricare_df['Ticket_ID'].isin(sf_open_df['Ticket_ID'])]\n",
    "        \n",
    "        # Append the filtered DataFrame to the list\n",
    "        filtered_dfs.append(filtered_df)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Could not read file {file_name} due to encoding error. Skipping this file.\")\n",
    "\n",
    "# Concatenate all filtered DataFrames into a single DataFrame\n",
    "combined_filtered_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined filtered DataFrame to a new CSV file\n",
    "combined_filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Optional: Display the combined data to the user\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Combined Filtered Bricare Data\", dataframe=combined_filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To add column ID on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_34960\\4143017330.py:4: DtypeWarning: Columns (17,23,36,53,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  to_close_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\to_close.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the to_close and extract_check_closed24 files into pandas DataFrames\n",
    "to_close_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\to_close.csv\")\n",
    "extract_df = pd.read_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\extract_check_closed24.csv\", delimiter=';')\n",
    "\n",
    "# Merge the DataFrames on the Ticket_ID column to add the ID column from extract_df to to_close_df\n",
    "merged_df = pd.merge(to_close_df, extract_df[['Ticket_ID', 'ID']], on='Ticket_ID', how='left')\n",
    "\n",
    "# Filter the merged DataFrame to keep only rows where the status column is 'Closed'\n",
    "filtered_df = merged_df[merged_df['status'] == 'Closed']\n",
    "\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "output_file = r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\merged_to_close_with_ID2.csv\"\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To fix no kartu and no rekening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with the correct delimiter\n",
    "file_path = r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\extract_nomer_kartu.csv\"\n",
    "data = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Convert the card numbers to text format, ensuring no scientific notation\n",
    "data['SCC_CARD_NUMBER__C'] = data['SCC_CARD_NUMBER__C'].apply(lambda x: '{:.0f}'.format(x) if pd.notnull(x) else '')\n",
    "\n",
    "# Convert the account numbers to text format, ensuring no scientific notation\n",
    "data['SCC_ACCOUNT_NUMBER__C'] = data['SCC_ACCOUNT_NUMBER__C'].apply(lambda x: '{:.0f}'.format(x) if pd.notnull(x) else '')\n",
    "\n",
    "# Display the first few rows of the corrected dataset\n",
    "data.to_csv(r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\extract_nomer_kartu_fixed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To fix no telp in account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cifno       No_telp\n",
      "0  TFI1251  085230254781\n",
      "1  SJNC508  081390357064\n",
      "2  MADX789  082363303463\n",
      "3  IMF8791  082114456934\n",
      "4  AKDX781  085322820709\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with the correct delimiter ';'\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\bricare_case_account_missing.csv\"\n",
    "data = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Function to clean the phone numbers\n",
    "def clean_phone_number(phone_number):\n",
    "    if pd.isna(phone_number):\n",
    "        return phone_number  # Return NaN as is\n",
    "    \n",
    "    # If the number is in scientific notation, convert it to an integer\n",
    "    if isinstance(phone_number, float):\n",
    "        phone_number = str(int(phone_number))\n",
    "    else:\n",
    "        phone_number = str(phone_number)\n",
    "    \n",
    "    # Remove any non-numeric characters\n",
    "    phone_number = ''.join(filter(str.isdigit, phone_number))\n",
    "    \n",
    "    # Add a leading zero if the phone number doesn't start with '0'\n",
    "    if not phone_number.startswith('0'):\n",
    "        phone_number = '0' + phone_number\n",
    "    \n",
    "    return phone_number\n",
    "\n",
    "# Apply the function to the 'No_telp' column\n",
    "data['No_telp'] = data['No_telp'].apply(clean_phone_number)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(data.head())\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\bricare_case_account_missing_clean.csv\"\n",
    "data.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full code to clean and adjust the dataset\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset with the correct delimiter\n",
    "new_data_cleaned = pd.read_csv(new_file_path, delimiter=';', error_bad_lines=False)\n",
    "\n",
    "# Drop columns that are entirely empty\n",
    "new_data_cleaned = new_data_cleaned.dropna(axis=1, how='all')\n",
    "\n",
    "# Function to adjust Ticket_ID to 12-digit format\n",
    "def adjust_ticket_id(ticket_id):\n",
    "    if len(ticket_id) == 13:  # TTB followed by 10 digits\n",
    "        return f\"TTB0000{ticket_id[3:]}\"  # Pad with zeros to make 12 digits after TTB\n",
    "    return ticket_id\n",
    "\n",
    "# Apply the adjustment function to the Ticket_ID column\n",
    "new_data_cleaned['Ticket_ID'] = new_data_cleaned['Ticket_ID'].apply(adjust_ticket_id)\n",
    "\n",
    "# Change the status to \"Escalated\"\n",
    "new_data_cleaned['status'] = 'Escalated'\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "date_columns = ['Create_Date', 'tanggalTransaksi', 'Modified_Date', 'TanggalClosed']  # Adjust based on available columns\n",
    "for col in date_columns:\n",
    "    if col in new_data_cleaned.columns:\n",
    "        new_data_cleaned[col] = pd.to_datetime(new_data_cleaned[col], errors='coerce')\n",
    "\n",
    "# Fill missing values for specific columns (if required)\n",
    "new_data_cleaned = new_data_cleaned.fillna({'Jenis_Laporan': 'Unknown', 'email': 'no_email@example.com'})\n",
    "\n",
    "# Save the cleaned and updated data to a new CSV file\n",
    "cleaned_file_path = '/mnt/data/cleaned_bricks_data_final.csv'\n",
    "new_data_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "cleaned_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"D:\\extract_new_status.csv\"\n",
    "\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df[df['STATUS']=='New']\n",
    "df = df[(df['SCC_CASE_OWNER__C'] == 'Migration') | (df['SCC_CASE_OWNER__C'] == 'migration')]\n",
    "df.to_csv(r\"D:\\extract_new_status_done.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\extract_new_status_done.csv\"\n",
    "path2=r\"D:\\extract_new_status_done_onlyTTB.csv\"\n",
    "df=pd.read_csv(path)\n",
    "# df=df.drop(columns=['Unnamed: 0.1'])\n",
    "df.to_csv(r\"D:\\extract_new_status_done.csv\", index=False)\n",
    "df=df['SCC_LEGACY_TICKET_ID__C']\n",
    "\n",
    "# df.to_csv(path2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for a Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,36,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,17,30,33,36,47,49,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,31,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,31,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,31,33,47,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\618758926.py:19: DtypeWarning: Columns (15,47,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket_ID TTB000054253533 found in file: bricare_20240101_20240807_3600001_79_closed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = r\"C:\\Users\\maste\\Downloads\\bricare_2024_data_clean\"\n",
    "\n",
    "# Define the Ticket_ID to search for\n",
    "search_id = 'TTB000054253533'\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the Ticket_ID is present in the 'Ticket_ID' column\n",
    "        if search_id in df['Ticket_ID'].values:\n",
    "            print(f\"Ticket_ID {search_id} found in file: {filename}\")\n",
    "            break\n",
    "else:\n",
    "    print(f\"Ticket_ID {search_id} not found in any files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,36,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,17,30,33,36,47,49,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,31,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,31,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,31,33,47,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,47,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (15,36,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\239589948.py:35: DtypeWarning: Columns (9,15,31,33,45,46,47,48,50,51,53,59,62,63,75,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to found_ticket_ids_2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = r\"C:\\Users\\maste\\Downloads\\bricare_2024_data_clean\"\n",
    "\n",
    "# Define the path to the file containing the list of Ticket_IDs\n",
    "# ticket_id_file = r\"D:\\extract_new_status_done_onlyTTB.csv\"\n",
    "\n",
    "\n",
    "ticket_id_file = r\"D:\\escalated_havingcloseddate.txt\"\n",
    "\n",
    "# Define the output CSV file\n",
    "output_file = 'found_ticket_ids_2.csv'\n",
    "\n",
    "# Determine the file type and read the Ticket_IDs\n",
    "if ticket_id_file.endswith('.txt'):\n",
    "    with open(ticket_id_file, 'r') as file:\n",
    "        ticket_ids = [line.strip() for line in file.readlines()]\n",
    "elif ticket_id_file.endswith('.csv'):\n",
    "    df_ticket_ids = pd.read_csv(ticket_id_file)\n",
    "    ticket_ids = df_ticket_ids['Ticket_ID'].tolist()\n",
    "else:\n",
    "    raise ValueError(\"Unsupported file type. The file must be either a .txt or .csv.\")\n",
    "\n",
    "# List to collect the results\n",
    "results = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if any of the Ticket_IDs are present in the 'Ticket_ID' column\n",
    "        found_ids = df[df['Ticket_ID'].isin(ticket_ids)]\n",
    "        \n",
    "        if not found_ids.empty:\n",
    "            for ticket_id in found_ids['Ticket_ID']:\n",
    "                results.append({'Ticket_ID': ticket_id, 'File': filename})\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df_results.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,36,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,17,30,33,36,47,49,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  found_ids['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,31,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,31,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,31,33,47,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,47,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  found_ids['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (15,36,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2796145597.py:32: DtypeWarning: Columns (9,15,31,33,45,46,47,48,50,51,53,59,62,63,75,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All found data saved to found_ticket_ids_with_details.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = r\"C:\\Users\\maste\\Downloads\\bricare_2024_data_clean\"\n",
    "\n",
    "# Define the path to the file containing the list of Ticket_IDs\n",
    "ticket_id_file = r\"D:\\extract_new_status_done_onlyTTB.csv\"\n",
    "\n",
    "# Define the output CSV file\n",
    "output_file = 'found_ticket_ids_with_details.csv'\n",
    "\n",
    "# Determine the file type and read the Ticket_IDs\n",
    "if ticket_id_file.endswith('.txt'):\n",
    "    with open(ticket_id_file, 'r') as file:\n",
    "        ticket_ids = [line.strip() for line in file.readlines()]\n",
    "elif ticket_id_file.endswith('.csv'):\n",
    "    df_ticket_ids = pd.read_csv(ticket_id_file)\n",
    "    ticket_ids = df_ticket_ids['Ticket_ID'].tolist()\n",
    "else:\n",
    "    raise ValueError(\"Unsupported file type. The file must be either a .txt or .csv.\")\n",
    "\n",
    "# List to collect all rows with found Ticket_IDs\n",
    "all_found_data = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if any of the Ticket_IDs are present in the 'Ticket_ID' column\n",
    "        found_ids = df[df['Ticket_ID'].isin(ticket_ids)]\n",
    "        \n",
    "        if not found_ids.empty:\n",
    "            found_ids['Source_File'] = filename  # Add a column indicating the source file\n",
    "            all_found_data.append(found_ids)\n",
    "\n",
    "# Concatenate all found data into a single DataFrame\n",
    "df_all_found = pd.concat(all_found_data, ignore_index=True)\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "df_all_found.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All found data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,36,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,17,30,33,36,47,49,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,31,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,31,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,31,33,47,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,47,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (15,36,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:26: DtypeWarning: Columns (9,15,31,33,45,46,47,48,50,51,53,59,62,63,75,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2617288852.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining tickets saved to remaining_tickets.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = r\"C:\\Users\\maste\\Downloads\\bricare_2024_data_clean\"\n",
    "\n",
    "# Define the path to the exclusion file\n",
    "exclusion_file = r\"D:\\extract_ttb24.csv\"\n",
    "\n",
    "# Define the output CSV file\n",
    "output_file = 'remaining_tickets.csv'\n",
    "\n",
    "# Load the exclusion list from the exclusion file\n",
    "df_exclusion = pd.read_csv(exclusion_file)\n",
    "excluded_tickets = df_exclusion['Ticket_ID'].tolist()\n",
    "\n",
    "# List to collect all rows with Ticket_IDs not in the exclusion list\n",
    "all_remaining_data = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv') and filename != os.path.basename(exclusion_file):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Filter rows where Ticket_ID starts with 'TTB' and is not in the exclusion list\n",
    "        remaining_tickets = df[df['Ticket_ID'].str.startswith('TTB') & ~df['Ticket_ID'].isin(excluded_tickets)]\n",
    "        \n",
    "        if not remaining_tickets.empty:\n",
    "            remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
    "            all_remaining_data.append(remaining_tickets)\n",
    "\n",
    "# Concatenate all remaining data into a single DataFrame\n",
    "df_all_remaining = pd.concat(all_remaining_data, ignore_index=True)\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "df_all_remaining.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Remaining tickets saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTB createddate up to midnight 6 aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,36,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,17,30,33,36,47,49,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,31,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,31,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,31,33,47,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,47,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (15,36,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:29: DtypeWarning: Columns (9,15,31,33,45,46,47,48,50,51,53,59,62,63,75,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\849014698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining tickets saved to remaining_tickets_upto_6aug.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = r\"C:\\Users\\maste\\Downloads\\bricare_2024_data_clean\"\n",
    "\n",
    "# Define the path to the exclusion file\n",
    "exclusion_file = r\"D:\\extract_ttb24.csv\"\n",
    "\n",
    "# Define the output CSV file\n",
    "output_file = 'remaining_tickets_upto_6aug.csv'\n",
    "\n",
    "# Load the exclusion list from the exclusion file\n",
    "df_exclusion = pd.read_csv(exclusion_file)\n",
    "excluded_tickets = df_exclusion['Ticket_ID'].tolist()\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = pd.to_datetime('2024-08-06 23:59:59')\n",
    "\n",
    "# List to collect all rows with Ticket_IDs not in the exclusion list\n",
    "all_remaining_data = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv') and filename != os.path.basename(exclusion_file):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the Create_Date column to datetime\n",
    "        df['Create_Date'] = pd.to_datetime(df['Create_Date'])\n",
    "        \n",
    "        # Filter rows where Ticket_ID starts with 'TTB', is not in the exclusion list, and is within the cutoff date\n",
    "        remaining_tickets = df[\n",
    "            df['Ticket_ID'].str.startswith('TTB') &\n",
    "            ~df['Ticket_ID'].isin(excluded_tickets) &\n",
    "            (df['Create_Date'] <= cutoff_date)\n",
    "        ]\n",
    "        \n",
    "        if not remaining_tickets.empty:\n",
    "            remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
    "            all_remaining_data.append(remaining_tickets)\n",
    "\n",
    "# Concatenate all remaining data into a single DataFrame\n",
    "df_all_remaining = pd.concat(all_remaining_data, ignore_index=True)\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "df_all_remaining.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Remaining tickets saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTB createddate up to 6 aug and closeddate 6 aug 24 midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,36,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,17,30,33,36,47,49,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,31,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,31,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,31,33,47,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,47,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (15,36,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:29: DtypeWarning: Columns (9,15,31,33,45,46,47,48,50,51,53,59,62,63,75,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_8912\\2776318400.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining tickets saved to remaining_tickets_CC_6aug.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = r\"C:\\Users\\maste\\Downloads\\bricare_2024_data_clean\"\n",
    "\n",
    "# Define the path to the exclusion file\n",
    "exclusion_file = r\"D:\\extract_ttb24.csv\"\n",
    "\n",
    "# Define the output CSV file\n",
    "output_file = 'remaining_tickets_CC_6aug.csv'\n",
    "\n",
    "# Load the exclusion list from the exclusion file\n",
    "df_exclusion = pd.read_csv(exclusion_file)\n",
    "excluded_tickets = df_exclusion['Ticket_ID'].tolist()\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = pd.to_datetime('2024-08-06 23:59:59')\n",
    "\n",
    "# List to collect all rows with Ticket_IDs not in the exclusion list\n",
    "all_remaining_data = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv') and filename != os.path.basename(exclusion_file):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the Create_Date and TanggalClosed columns to datetime\n",
    "        df['Create_Date'] = pd.to_datetime(df['Create_Date'])\n",
    "        df['TanggalClosed'] = pd.to_datetime(df['TanggalClosed'])\n",
    "        \n",
    "        # Filter rows where Ticket_ID starts with 'TTB', is not in the exclusion list,\n",
    "        # and both Create_Date and TanggalClosed are within the cutoff date\n",
    "        remaining_tickets = df[\n",
    "            df['Ticket_ID'].str.startswith('TTB') &\n",
    "            ~df['Ticket_ID'].isin(excluded_tickets) &\n",
    "            (df['Create_Date'] <= cutoff_date) &\n",
    "            (df['TanggalClosed'] <= cutoff_date)\n",
    "        ]\n",
    "        \n",
    "        if not remaining_tickets.empty:\n",
    "            remaining_tickets['Source_File'] = filename  # Add a column indicating the source file\n",
    "            all_remaining_data.append(remaining_tickets)\n",
    "\n",
    "# Concatenate all remaining data into a single DataFrame\n",
    "df_all_remaining = pd.concat(all_remaining_data, ignore_index=True)\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "df_all_remaining.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Remaining tickets saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\remaining_tickets_CC_6aug.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df.iloc[1178:1179]\n",
    "df.to_csv(r\"D:\\dataquality2\\remaining_tickets_CC_6aug_1line.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate copies of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows with updated 'APPROVAL_STATUS__C' and sequential 'SCC_LEGACY_TICKET_ID__C' saved to C:\\Users\\maste\\Downloads\\extract-case_done.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "input_file_path = r\"C:\\Users\\maste\\Downloads\\extract-case.csv\"\n",
    "output_file_path = r\"C:\\Users\\maste\\Downloads\\extract-case_done.csv\"\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "df['APPROVAL_STATUS__C'] = 'Waiting for Checker'\n",
    "df['OWNERID'] = '00GMR0000000fqe2AA'\n",
    "copies_per_row = 1  \n",
    "duplicated_df = df.loc[df.index.repeat(copies_per_row)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "start_ticket_id = 1642667  \n",
    "prefix = \"TICKET00\"\n",
    "\n",
    "# Create a sequence of ticket IDs\n",
    "total_rows = len(duplicated_df)\n",
    "duplicated_df['SCC_LEGACY_TICKET_ID__C'] = [f\"{prefix}{start_ticket_id + i}\" for i in range(total_rows)]\n",
    "\n",
    "\n",
    "duplicated_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Duplicated rows with updated 'APPROVAL_STATUS__C' and sequential 'SCC_LEGACY_TICKET_ID__C' saved to {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_SIT.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df['EXTERNAL_ID__C']=df['NAME']\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_SIT_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded CSV files\n",
    "data1 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\data_bricarelamavsbricarebaru\\data1_newbricare.csv\")\n",
    "data2 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\data_bricarelamavsbricarebaru\\data2_bricarelama.csv\")\n",
    "\n",
    "# Compare the 'Ticket_ID' column in both files to find values in data2 that are not in data1\n",
    "# Assuming 'Ticket_ID' is the column name in both files\n",
    "missing_in_data1 = data2[~data2['Ticket_ID'].isin(data1['Ticket_ID'])]\n",
    "\n",
    "missing_in_data1.to_csv(\"missing_TTB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_3884\\1456068170.py:9: DtypeWarning: Columns (15,17,36,46,47,51,53,56,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  big_file = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\bricare_20240101_20240807_0_79_clean (1)\\bricare_20240101_20240807_0_79.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "send_to_drone = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\sendtodrone.csv\")\n",
    "\n",
    "\n",
    "exclude_ids = set(send_to_drone['Call_Type_ID'])\n",
    "\n",
    "big_file = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\bricare_20240101_20240807_0_79_clean (1)\\bricare_20240101_20240807_0_79.csv\")\n",
    "filtered_big_file = big_file[~big_file['Call_Type_ID'].isin(exclude_ids)]\n",
    "# filtered_big_file.to_csv('filtered_bigfile.csv', index=False)\n",
    "\n",
    "big_file = big_file[big_file['status'] != 'Closed']\n",
    "# big_file\n",
    "\n",
    "\n",
    "# print(\"Filtering complete. The filtered data is saved in 'filtered_bigfile.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case type take out only two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Type: Case Type Number</th>\n",
       "      <th>Deskripsi Case</th>\n",
       "      <th>Total SLA</th>\n",
       "      <th>Level 1 Escalation Team</th>\n",
       "      <th>Level 1 OLA</th>\n",
       "      <th>Level 2 Escalation Team</th>\n",
       "      <th>Level 2 OLA</th>\n",
       "      <th>Level 3 Escalation Team</th>\n",
       "      <th>Level 3 OLA</th>\n",
       "      <th>Level 4 Escalation Team</th>\n",
       "      <th>Level 4 OLA</th>\n",
       "      <th>List Dokumen yang dibutuhkan</th>\n",
       "      <th>Additional Details</th>\n",
       "      <th>Sub Case Reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8634</td>\n",
       "      <td>Merchant Mengajukan Pengembalian Dana (Refund)...</td>\n",
       "      <td>4</td>\n",
       "      <td>FST - Merchant Care &amp; Operation</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. Berita Acara, \\n2. Notif Transaksi Berhasil...</td>\n",
       "      <td>Store merchant dengan data sbb :          \\nID...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>Nasabah Menanyakan Informasi Pengajuan Terkait...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Information Cara pengajuan Ceria : \\nNasabah m...</td>\n",
       "      <td>1. Cara, Syarat &amp; Ketentuan\\n2. Status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>Nasabah Menyakan Terkait Promo Dan Program Ceria</td>\n",
       "      <td>4</td>\n",
       "      <td>CDD - Strgy&amp;Prod Credit Card&amp;Dgtl Lend</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xxx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>Nasabah Mengajukan Pelunasan Awal Cicilan Ceria</td>\n",
       "      <td>5</td>\n",
       "      <td>DDB - Helpdesk</td>\n",
       "      <td>3.0</td>\n",
       "      <td>DDB - Operational - CERIA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xxx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>Nasabah Mengajukan Pemblokiran Ceria</td>\n",
       "      <td>1</td>\n",
       "      <td>DDB - Helpdesk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nasabah mengajukan pemblokiran Sementara Akun ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>8102-2</td>\n",
       "      <td>Nasabah Komplain Terkait Ketidaksesuaian Angsu...</td>\n",
       "      <td>10</td>\n",
       "      <td>SCC - Customer Resolution</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Branch</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nasabah Komplain terkait Saldo Tertahan (Hold ...</td>\n",
       "      <td>1. Saldo Tertahan (Hold Dana)\\r\\n2. Ketidakses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>8102-3</td>\n",
       "      <td>Nasabah Komplain Terkait Data Nasabah Pinjaman</td>\n",
       "      <td>10</td>\n",
       "      <td>SCC - Customer Resolution</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Branch</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nasabah Komplain terkait Data SLIK\\r\\nNo KTP :...</td>\n",
       "      <td>1. SLIK - Status Kolektabilitas\\r\\n2. SIKP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>8102-4</td>\n",
       "      <td>Nasabah Komplain Terkait Sanggahan Kepemilikan...</td>\n",
       "      <td>10</td>\n",
       "      <td>SCC - Customer Resolution</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Branch</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nasabah Komplain terkait Sanggahan Kepemilikan...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>8102-5</td>\n",
       "      <td>Nasabah Komplain Terkait Permasalahan Asuransi...</td>\n",
       "      <td>10</td>\n",
       "      <td>SCC - Customer Resolution</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Branch</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nasabah Komplain terkait Permasalahan Asuransi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>8102-6</td>\n",
       "      <td>Nasabah Komplain Terkait Agunan Pinjaman</td>\n",
       "      <td>10</td>\n",
       "      <td>SCC - Customer Resolution</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Branch</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nasabah Komplain terkait Permasalahan Agunan P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case Type: Case Type Number  \\\n",
       "0                          8634   \n",
       "1                          1000   \n",
       "2                          1002   \n",
       "3                          1003   \n",
       "4                          1005   \n",
       "..                          ...   \n",
       "377                      8102-2   \n",
       "378                      8102-3   \n",
       "379                      8102-4   \n",
       "380                      8102-5   \n",
       "381                      8102-6   \n",
       "\n",
       "                                        Deskripsi Case  Total SLA  \\\n",
       "0    Merchant Mengajukan Pengembalian Dana (Refund)...          4   \n",
       "1    Nasabah Menanyakan Informasi Pengajuan Terkait...          0   \n",
       "2     Nasabah Menyakan Terkait Promo Dan Program Ceria          4   \n",
       "3      Nasabah Mengajukan Pelunasan Awal Cicilan Ceria          5   \n",
       "4                 Nasabah Mengajukan Pemblokiran Ceria          1   \n",
       "..                                                 ...        ...   \n",
       "377  Nasabah Komplain Terkait Ketidaksesuaian Angsu...         10   \n",
       "378     Nasabah Komplain Terkait Data Nasabah Pinjaman         10   \n",
       "379  Nasabah Komplain Terkait Sanggahan Kepemilikan...         10   \n",
       "380  Nasabah Komplain Terkait Permasalahan Asuransi...         10   \n",
       "381           Nasabah Komplain Terkait Agunan Pinjaman         10   \n",
       "\n",
       "                    Level 1 Escalation Team  Level 1 OLA  \\\n",
       "0           FST - Merchant Care & Operation          4.0   \n",
       "1                                       NaN          NaN   \n",
       "2    CDD - Strgy&Prod Credit Card&Dgtl Lend          4.0   \n",
       "3                            DDB - Helpdesk          3.0   \n",
       "4                            DDB - Helpdesk          1.0   \n",
       "..                                      ...          ...   \n",
       "377               SCC - Customer Resolution          1.0   \n",
       "378               SCC - Customer Resolution          1.0   \n",
       "379               SCC - Customer Resolution          1.0   \n",
       "380               SCC - Customer Resolution          1.0   \n",
       "381               SCC - Customer Resolution          1.0   \n",
       "\n",
       "       Level 2 Escalation Team  Level 2 OLA Level 3 Escalation Team  \\\n",
       "0                          NaN          NaN                     NaN   \n",
       "1                          NaN          NaN                     NaN   \n",
       "2                          NaN          NaN                     NaN   \n",
       "3    DDB - Operational - CERIA          2.0                     NaN   \n",
       "4                          NaN          NaN                     NaN   \n",
       "..                         ...          ...                     ...   \n",
       "377                     Branch          9.0                     NaN   \n",
       "378                     Branch          9.0                     NaN   \n",
       "379                     Branch          9.0                     NaN   \n",
       "380                     Branch          9.0                     NaN   \n",
       "381                     Branch          9.0                     NaN   \n",
       "\n",
       "     Level 3 OLA  Level 4 Escalation Team  Level 4 OLA  \\\n",
       "0            NaN                      NaN          NaN   \n",
       "1            NaN                      NaN          NaN   \n",
       "2            NaN                      NaN          NaN   \n",
       "3            NaN                      NaN          NaN   \n",
       "4            NaN                      NaN          NaN   \n",
       "..           ...                      ...          ...   \n",
       "377          NaN                      NaN          NaN   \n",
       "378          NaN                      NaN          NaN   \n",
       "379          NaN                      NaN          NaN   \n",
       "380          NaN                      NaN          NaN   \n",
       "381          NaN                      NaN          NaN   \n",
       "\n",
       "                          List Dokumen yang dibutuhkan  \\\n",
       "0    1. Berita Acara, \\n2. Notif Transaksi Berhasil...   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "377                                                NaN   \n",
       "378                                                NaN   \n",
       "379                                                NaN   \n",
       "380                                                NaN   \n",
       "381                                                NaN   \n",
       "\n",
       "                                    Additional Details  \\\n",
       "0    Store merchant dengan data sbb :          \\nID...   \n",
       "1    Information Cara pengajuan Ceria : \\nNasabah m...   \n",
       "2                                                  xxx   \n",
       "3                                                  xxx   \n",
       "4    Nasabah mengajukan pemblokiran Sementara Akun ...   \n",
       "..                                                 ...   \n",
       "377  Nasabah Komplain terkait Saldo Tertahan (Hold ...   \n",
       "378  Nasabah Komplain terkait Data SLIK\\r\\nNo KTP :...   \n",
       "379  Nasabah Komplain terkait Sanggahan Kepemilikan...   \n",
       "380  Nasabah Komplain terkait Permasalahan Asuransi...   \n",
       "381  Nasabah Komplain terkait Permasalahan Agunan P...   \n",
       "\n",
       "                                    Sub Case Reference  \n",
       "0                                                  NaN  \n",
       "1               1. Cara, Syarat & Ketentuan\\n2. Status  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "377  1. Saldo Tertahan (Hold Dana)\\r\\n2. Ketidakses...  \n",
       "378         1. SLIK - Status Kolektabilitas\\r\\n2. SIKP  \n",
       "379                                                NaN  \n",
       "380                                                NaN  \n",
       "381                                                NaN  \n",
       "\n",
       "[382 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df= pd.read_excel(r\"C:\\Users\\maste\\Downloads\\Master Case Type Aktif-2024-10-02-15-10-22.xlsx\")\n",
    "\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     085876692752\n",
       "1     085876692752\n",
       "2     085876692752\n",
       "3     085876692752\n",
       "4     085876692752\n",
       "5     085876692752\n",
       "6     085876692752\n",
       "7     085876692752\n",
       "8     085876692752\n",
       "9     085876692752\n",
       "10    085876692752\n",
       "11    085876692752\n",
       "12    085876692752\n",
       "13    085876692752\n",
       "14    085876692752\n",
       "15    085876692752\n",
       "16    085876692752\n",
       "17    085876692752\n",
       "18    085876692752\n",
       "19    085876692752\n",
       "20    085876692752\n",
       "21    085876692752\n",
       "22    085876692752\n",
       "23    085876692752\n",
       "24    085876692752\n",
       "25    085876692752\n",
       "26    085876692752\n",
       "27    085876692752\n",
       "28    085876692752\n",
       "29    085876692752\n",
       "30    085876692752\n",
       "31    085876692752\n",
       "32    085876692752\n",
       "33    085876692752\n",
       "34    085876692752\n",
       "35    085876692752\n",
       "36    085876692752\n",
       "37    085876692752\n",
       "38    085876692752\n",
       "39    085876692752\n",
       "40    085876692752\n",
       "41    085876692752\n",
       "42    085876692752\n",
       "43    085876692752\n",
       "44    085876692752\n",
       "45    085876692752\n",
       "46    085876692752\n",
       "47    085876692752\n",
       "48    085876692752\n",
       "49    085876692752\n",
       "Name: CUST_CURRENT_PHONE__C, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# UAT\n",
    "# file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_UAT.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "# df_duplicated = pd.concat([df] * 1000, ignore_index=True)\n",
    "# # df_duplicated['OWNERID']='005MR0000005T4bYAE'\n",
    "# df_duplicated['OWNERID']='005MR0000005FCvYAM'\n",
    "# df_duplicated=df_duplicated.iloc[:750]\n",
    "# output_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_UAT_test_1line.csv\"\n",
    "# df_duplicated.to_csv(output_file_path, index=False)\n",
    "# output_file_path\n",
    "\n",
    "\n",
    "\n",
    "# 8812\n",
    "# file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_traininguat8812.csv\"\n",
    "# df = pd.read_csv(file_path, dtype={'CUST_CURRENT_PHONE__C': str})\n",
    "# df_duplicated = pd.concat([df] * 50, ignore_index=True)\n",
    "# # df_duplicated['OWNERID']='005MR0000005T4bYAE'\n",
    "# # df_duplicated=df_duplicated.iloc[:750]\n",
    "# df['CUST_CURRENT_PHONE__C'] = df['CUST_CURRENT_PHONE__C'].apply(lambda x: f\"{int(x):0>10}\" if pd.notnull(x) else x)\n",
    "# output_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\UAT_training_8812.csv\"\n",
    "# df_duplicated.to_csv(output_file_path, index=False)\n",
    "# output_file_path\n",
    "# df_duplicated['CUST_CURRENT_PHONE__C']\n",
    "\n",
    "#8915\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_traininguat8915_100k.csv\"\n",
    "df = pd.read_csv(file_path, dtype={'CUST_CURRENT_PHONE__C': str})\n",
    "\n",
    "# df['SCC_AMOUNT__C']\n",
    "df_duplicated = pd.concat([df] * 50, ignore_index=True)\n",
    "# df_duplicated['OWNERID']='005MR0000005T4bYAE'\n",
    "df['CUST_CURRENT_PHONE__C'] = df['CUST_CURRENT_PHONE__C'].apply(lambda x: f\"{int(x):0>10}\" if pd.notnull(x) else x)\n",
    "# df_duplicated=df_duplicated.iloc[:750]\n",
    "output_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\UAT_training_8915_amount100k.csv\"\n",
    "df_duplicated.to_csv(output_file_path, index=False)\n",
    "# output_file_path\n",
    "df_duplicated['CUST_CURRENT_PHONE__C']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PDF with size of 1 KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "# Create instance of FPDF class\n",
    "pdf = FPDF()\n",
    "\n",
    "# Add a page\n",
    "pdf.add_page()\n",
    "\n",
    "# Set font\n",
    "pdf.set_font('Arial', 'B', 12)\n",
    "\n",
    "# Add a cell with minimal content to reduce the file size\n",
    "pdf.cell(0, 10, 'Test PDF for 1KB size')\n",
    "\n",
    "# Save the PDF to a file\n",
    "file_path = \"small_pdf.pdf\"\n",
    "pdf.output(file_path)\n",
    "\n",
    "# Check the file size\n",
    "import os\n",
    "file_size = os.path.getsize(file_path)\n",
    "\n",
    "# Display the file size\n",
    "file_size, file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Calculate the business day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business days difference: 63 days\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_business_days_detailed(start_date_str, end_date_str):\n",
    "    start_date = datetime.strptime(start_date_str, \"%m/%d/%Y, %I:%M %p\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%m/%d/%Y, %I:%M %p\")\n",
    "    business_days = np.busday_count(start_date.date(), end_date.date())\n",
    "    \n",
    "    if business_days == 0 and start_date.date() == end_date.date():\n",
    "        time_difference = (end_date - start_date).total_seconds() / 3600  \n",
    "        return f\"Same business day, time difference: {time_difference:.2f} hours\"\n",
    "    \n",
    "    return f\"Business days difference: {business_days} days\"\n",
    "start_date_detailed = \"7/13/2024, 4:21 PM\"\n",
    "end_date_detailed = \"10/10/2024, 10:17 AM\"\n",
    "business_days_detailed_diff = calculate_business_days_detailed(start_date_detailed, end_date_detailed)\n",
    "print(business_days_detailed_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 days, 17:56:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_date_detailed = \"7/13/2024, 4:21 PM\"\n",
    "end_date_detailed = \"10/10/2024, 10:17 AM\"\n",
    "\n",
    "\n",
    "date_format = \"%m/%d/%Y, %I:%M %p\"\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(start_date_detailed, date_format)\n",
    "end_date = datetime.strptime(end_date_detailed, date_format)\n",
    "\n",
    "\n",
    "date_difference = end_date - start_date\n",
    "\n",
    "print(date_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business days: 9, additional hours: 19.25\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the difference in business days with hours and minutes accounted for\n",
    "def calculate_business_days_detailed_with_time(start_date_str, end_date_str):\n",
    "    # Convert the input strings to datetime objects\n",
    "    start_date = datetime.strptime(start_date_str, \"%m/%d/%Y, %I:%M %p\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%m/%d/%Y, %I:%M %p\")\n",
    "    \n",
    "    # Use numpy's busday_count to calculate business days between the dates (without time)\n",
    "    business_days = np.busday_count(start_date.date(), end_date.date())\n",
    "    \n",
    "    # If the dates are on the same business day, we calculate time difference directly\n",
    "    if start_date.date() == end_date.date():\n",
    "        time_difference = (end_date - start_date).total_seconds() / 3600  # in hours\n",
    "        return f\"Same business day, time difference: {time_difference:.2f} hours\"\n",
    "    \n",
    "    # Calculate remaining hours on the start and end dates using Python's timedelta\n",
    "    start_of_next_day = datetime.combine(start_date.date(), datetime.min.time()) + timedelta(days=1)\n",
    "    end_of_previous_day = datetime.combine(end_date.date(), datetime.min.time())\n",
    "    \n",
    "    remaining_start_day_hours = (start_of_next_day - start_date).total_seconds() / 3600  # in hours\n",
    "    remaining_end_day_hours = (end_date - end_of_previous_day).total_seconds() / 3600  # in hours\n",
    "    \n",
    "    # Total business days with the remaining hours of the first and last day\n",
    "    return f\"Business days: {business_days}, additional hours: {remaining_start_day_hours + remaining_end_day_hours:.2f}\"\n",
    "\n",
    "# Example usage\n",
    "start_date_detailed = \"09/24/2024, 3:02 PM\"\n",
    "end_date_detailed = \"10/07/2024, 10:17 AM\"\n",
    "business_days_detailed_diff = calculate_business_days_detailed_with_time(start_date_detailed, end_date_detailed)\n",
    "print(business_days_detailed_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9999822924791344\n",
      "   Actual   Predicted\n",
      "1   102.0  102.999991\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your stock data (e.g., CSV file with historical prices)\n",
    "# For example purposes, let's assume you have a DataFrame `df`\n",
    "# where 'Date' is the index and columns include 'Open', 'High', 'Low', 'Close', 'Volume'\n",
    "\n",
    "# Example: Load a dataset (assuming you have 'stock_data.csv')\n",
    "# df = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# For simplicity, let's simulate some financial data\n",
    "data = {\n",
    "    'Open': [100, 102, 101, 103, 102],\n",
    "    'High': [102, 103, 102, 104, 103],\n",
    "    'Low': [99, 101, 100, 102, 101],\n",
    "    'Close': [101, 102, 102, 103, 103],\n",
    "    'Volume': [1000, 1500, 1200, 1300, 1400]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# The target variable is the 'Close' price of the next day\n",
    "df['Next_Close'] = df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row as it has NaN in 'Next_Close'\n",
    "df = df.dropna()\n",
    "\n",
    "# Define features (Open, High, Low, Volume) and target (Next_Close)\n",
    "X = df[['Open', 'High', 'Low', 'Volume']]\n",
    "y = df['Next_Close']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor model\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Display the predictions vs actual values\n",
    "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(r'c:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_overlevel2 di SIT.csv')\n",
    "df['CREATEDDATE']='2024-09-14T02:30:10.000Z'\n",
    "df['LASTMODIFIEDDATE']='2024-09-14T17:00:04.000Z'\n",
    "df.to_csv('test_level2.csv', index=False)\n",
    "\n",
    "# df=pd.read_csv(r\"D:\\dataquality2\\test_level2.csv\")\n",
    "# df['CREATEDDATE']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

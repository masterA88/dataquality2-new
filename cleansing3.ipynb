{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script for data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRICARE:\n",
    "\n",
    "BRICARE consists of 2 different types of files by year:\n",
    "\n",
    "a. File after 2022 (2023-2024) = 79 kolom\n",
    "\n",
    "\n",
    "b. File before 2022 (2019-2022) = 27 kolom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case Origin\n",
    "\n",
    "origin_mappings = {\n",
    "    \"Sms\": \"SMS\",\n",
    "    \"Telegram\": \"Telegram\",\n",
    "    \"WebChat\": \"WebChat\",\n",
    "    \"Sabrina\": \"Sabrina\",\n",
    "    \"Twitter\": \"Twitter\",\n",
    "    \"Facebook\": \"Facebook\",\n",
    "    \"Instagram\": \"Instagram\",\n",
    "    \"Pesan IB\": \"Pesan IB\",\n",
    "    \"Email\": \"Email\",\n",
    "    \"Video Banking\": \"Video Banking\",\n",
    "    \"Media Cetak\": \"Media Cetak\",\n",
    "    \"Media Online\": \"Media Online\",\n",
    "    \"OJK\": \"OJK\",\n",
    "    \"Lembaga Hukum Lainya\": \"Lembaga Hukum Lainya\",\n",
    "    \"Media Umum Lainya\": \"Media Umum Lainya\",\n",
    "    \"Phone\": \"Phone\",\n",
    "    \"Surat\": \"Surat\",\n",
    "    \"Walk In\": \"Walk In\",\n",
    "    \"Ã‹DC & Brilink\": \"EDC\",\n",
    "    \"Brilink\": \"BRILink\",\n",
    "    \"BRIMO\": \"BRIMO\",\n",
    "    \"PPID\": \"PPID\",\n",
    "    \"SP4N LAPOR\": \"SP4N LAPOR\",\n",
    "    \"Media Konsumen\": \"Media Konsumen\",\n",
    "    \"Bank Indonesia\": \"Bank Indonesia\",\n",
    "    \"Ombudsman\": \"Ombudsman\",\n",
    "    \"BPKN\": \"BPKN\",\n",
    "    \"MMS\": \"MMS\",\n",
    "    \"PKSS Oten\": \"PKSS Oten\",\n",
    "    \"BRIPENS\": \"BRIPENS\",\n",
    "    \"Ceria\": \"Ceria\"\n",
    "}\n",
    "len(origin_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type A\n",
    "\n",
    "\n",
    "Data Extraction for File Type A must be 2 Files:\n",
    "\n",
    "\n",
    "A.1 Columns (without \"Details\")\n",
    "\n",
    "\n",
    "A.2 Details only \n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['Create_Date','TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\"\n",
    "\n",
    "- Columns shoud be mapped based on their Call_Type_ID:\n",
    "\n",
    "['Produk','Jenis_Produk','Jenis_Laporan']\n",
    "\n",
    "- PLEASE ADD CIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_uat20230101_20230101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('NULL', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:52: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('None', np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_65008\\3996884930.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_file(file_path):\n",
    "\n",
    "    data = []\n",
    "    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "\n",
    "            date_index = next(i for i, part in enumerate(parts) if date_pattern.match(part))\n",
    "\n",
    "            ticket_id = parts[0] \n",
    "            call_type_id = parts[1]  \n",
    "            description = ';'.join(parts[2:date_index])  \n",
    "            create_date = parts[date_index]  \n",
    "\n",
    "      \n",
    "            data.append([ticket_id, call_type_id, description, create_date] + parts[date_index + 1:])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.txt\"\n",
    "\n",
    "df = parse_file(file_path)\n",
    "df.replace('NULL', np.nan, inplace=True)\n",
    "df.replace('None', np.nan, inplace=True)\n",
    "df.replace('N/A', np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "   \n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_uat{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.1 Columns (without \"Details\"). Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_51572\\2385745217.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bricare_20230101_20230101.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 78 Columns\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "def parse_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if len(parts) > 78:\n",
    "                description = ';'.join(parts[2:-75])\n",
    "                new_parts = parts[:2] + [description] + parts[-75:]\n",
    "                data.append(new_parts)\n",
    "            else:\n",
    "                data.append(parts)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    df = df.replace(['0', 0], '')\n",
    "\n",
    "    columns_to_convert = ['TanggalClosed', 'tanggalTransaksi','Modified_Date','tanggalAttachmentDone','Tgl_Assigned','Tgl_Eskalasi','Tanggal_Settlement','Tgl_Foward','Tgl_In_Progress','Tgl_Returned']\n",
    "    for column in columns_to_convert:\n",
    "        df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "    \n",
    "    df['Ticket_ID'] = df['Ticket_ID'].apply(lambda x: x.replace('\\ufeff', '').strip())\n",
    "\n",
    "    return df\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_1masking.csv\"\n",
    "df = parse_csv(file_path)\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Type di tes MMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Data Mapping - Case_Type.csv\"\n",
    "df=pd.read_csv(path)\n",
    "# case_types = ['8634', '8635', '8636', '8623', '8624', '8628', '8629', '8630', '8412', '8625', '8615', '8616', '8617', '8618', '8619', '8638', '8620']\n",
    "\n",
    "case_types = ['8412']\n",
    "# Filter the dataframe\n",
    "df = df[df['Case Types'].isin(case_types)]\n",
    "df['Segment'] = df['Segment'].replace('All', '')\n",
    "df\n",
    "df.to_csv(r'C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\Case_Type_MMS2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Type Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the fix document\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan (4).xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='FIX Upload - Update Here')\n",
    "\n",
    "#df = df.drop(columns=['Salesforce', 'Status.1', 'Revisi Detail', 'Notes', 'User Grouping_Old', 'T4.3 - Technical Team', 'T5.1 - Third Party', 'T5.2 - Third Party', 'T5.3 - Third Party', 'Jenis Produk dan atau Layanan LKPBU', 'Kategori Permasalahan LKPBU', 'Jenis Transaksi LKPBU', 'Penyebab Pengaduan LKPBU', 'Jenis Produk LBUT', 'Sandi Jenis  Produk LBUT', 'Sebab Pengaduan LBUT', 'Sandi Penyebab LBUT'])\n",
    "\n",
    "\n",
    "df=df.drop(columns=['Actual SLA\\n(days) Old','Target SLA \\n(days) Old', 'BRI Notes','Team Name', 'New SLA', 'Status', 'crosscek', 'GIC', 'Maker', 'Checker', 'Signer', 'Courtesy Awal', 'Kebutuhan Data'])\n",
    "\n",
    "columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "df[columns_to_replace] = df[columns_to_replace].replace('-', '')\n",
    "\n",
    "if 'Active' in df.columns:\n",
    "    df['Active'] = df['Active'].astype(str).str.upper()\n",
    "\n",
    "\n",
    "df['Detail BRICare'] = df['Detail BRICare'].fillna('-')\n",
    "df\n",
    "\n",
    "def clean_case_types(value):\n",
    "    if pd.isna(value) or value == '':\n",
    "        return '0'\n",
    "    match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "    if match:\n",
    "        year, month = match.groups()\n",
    "        return f'{year}-{int(month)}'\n",
    "    value = re.sub(r'\\.0$', '', str(value))\n",
    "    return value\n",
    "\n",
    "df['Case Types'] = df['Case Types'].apply(clean_case_types) \n",
    "df['external_id'] = df['Case Types']\n",
    "\n",
    "output_path = 'Cleaned4_Call_Type.xlsx'\n",
    "output_path2 = 'Cleaned4_Call_Type.csv'\n",
    "df.to_excel(output_path, index=False)\n",
    "df.to_csv(output_path2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Case Types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Case Types]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all call types are covered\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your CSV files\n",
    "file1_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\calltype_psuyanto.csv\"\n",
    "# file2_path = r\"D:\\dataquality2\\Combined3_Cleaned_Call_Type.csv\"\n",
    "\n",
    "file2_path = r\"D:\\dataquality2\\Cleaned4_Call_Type.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Extract the 'Case Types' column from each DataFrame\n",
    "case_types_1 = df1['calltypeidtxt'].astype(str)\n",
    "case_types_2 = df2['Case Types'].astype(str)\n",
    "\n",
    "# Convert to sets for comparison\n",
    "set_case_types_1 = set(case_types_1)\n",
    "set_case_types_2 = set(case_types_2)\n",
    "\n",
    "# Find values in file1 that are missing in file2\n",
    "missing_values = set_case_types_1 - set_case_types_2\n",
    "\n",
    "# Convert the missing values set to a DataFrame\n",
    "missing_values_df = pd.DataFrame(list(missing_values), columns=['Missing Case Types'])\n",
    "\n",
    "# Save the missing values to a new CSV file\n",
    "# missing_values_df.to_csv('Missing_Case_Types.csv', index=False)\n",
    "missing_values_df\n",
    "# print(f\"Missing values from file1 that are not in file2 have been saved to 'Missing_Case_Types.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique case types: 520\n",
      "Duplicate case types:\n",
      "Number of rows dropped: 0\n",
      "   No Case Types  Active               User Grouping_New        Segment  \\\n",
      "0   1       1000    True               Resolution Center  Individu Umum   \n",
      "1   2       1002    True               Resolution Center  Individu Umum   \n",
      "2   3       1003    True                 Digital Banking  Individu Umum   \n",
      "3   4       1005    True  Card, Digital Lending & Assset  Individu Umum   \n",
      "4   5       1006    True                Operational Risk  Individu Umum   \n",
      "\n",
      "  Product    Sub Product                 Case Category  \\\n",
      "0   Loans  KTA - Digital                       Inquiry   \n",
      "1   Loans  KTA - Digital  Products / Promotion Inquiry   \n",
      "2   Loans  KTA - Digital                       Request   \n",
      "3   Loans  KTA - Digital                       Request   \n",
      "4   Loans  KTA - Digital                       Request   \n",
      "\n",
      "                                Old Case Description  \\\n",
      "0  CERIA - Status Pengajuan, Aplikasi, Cara & Syarat   \n",
      "1                    CERIA - Promo dan Program CERIA   \n",
      "2          CERIA - Permintaan Pelunasan Awal Cicilan   \n",
      "3                          CERIA - Permintaan Blokir   \n",
      "4  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...   \n",
      "\n",
      "                                New Case Description  ...  \\\n",
      "0  Nasabah Menanyakan Informasi Pengajuan Terkait...  ...   \n",
      "1   Nasabah Menyakan Terkait Promo Dan Program Ceria  ...   \n",
      "2    Nasabah Mengajukan Pelunasan Awal Cicilan Ceria  ...   \n",
      "3               Nasabah Mengajukan Pemblokiran Ceria  ...   \n",
      "4  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...  ...   \n",
      "\n",
      "   Eskalasi Team 4 Full Agent CCT/Frontliner  OLA Eskalasi team 1  \\\n",
      "0                   NaN                  1.0                  NaN   \n",
      "1                   NaN                  1.0                  NaN   \n",
      "2                   NaN                  1.0                  2.0   \n",
      "3                   NaN                  1.0                  1.0   \n",
      "4                   NaN                  1.0                  8.0   \n",
      "\n",
      "  OLA Eskalasi team 2 OLA Eskalasi team 3 OLA Eskalasi team 4 Total SLA  \\\n",
      "0                 NaN                 NaN                 NaN         1   \n",
      "1                 NaN                 NaN                 NaN         1   \n",
      "2                 2.0                 NaN                 NaN         5   \n",
      "3                 NaN                 NaN                 NaN         2   \n",
      "4                 NaN                 NaN                 NaN         9   \n",
      "\n",
      "                                      Detail BRICare external_id  Unnamed: 40  \n",
      "0  Information Cara pengajuan Ceria : \\nNasabah m...        1000          1.0  \n",
      "1                                                xxx        1002          NaN  \n",
      "2                                                xxx        1003          NaN  \n",
      "3  Nasabah mengajukan pemblokiran Sementara Akun ...        1005          NaN  \n",
      "4  Nasabah Request Buka/Release Blokir Akun Ceria...        1006          NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Remove duplicates if any\n",
    "\n",
    "# remove any duplicates\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "# path = r\"D:\\dataquality2\\Combined3_Cleaned_Call_Type.csv\"\n",
    "\n",
    "path = r\"D:\\dataquality2\\Cleaned4_Call_Type.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Step 1: Check for duplicates in 'Case Types'\n",
    "unique_case_types = df['Case Types'].unique()\n",
    "num_unique_case_types = len(unique_case_types)\n",
    "\n",
    "print(f\"Number of unique case types: {num_unique_case_types}\")\n",
    "\n",
    "# Find and print duplicate values in the 'Case Types' column\n",
    "duplicate_case_types = df['Case Types'][df['Case Types'].duplicated()].unique()\n",
    "print(\"Duplicate case types:\")\n",
    "for case_type in duplicate_case_types:\n",
    "    print(case_type)\n",
    "\n",
    "# Step 2: Identify and remove rows where 'Case Types' is a duplicate and 'Active' is False\n",
    "\n",
    "# Identify duplicate 'Case Types' values\n",
    "duplicates = df[df.duplicated(subset=['Case Types'], keep=False)]\n",
    "\n",
    "# Find rows where 'Case Types' is a duplicate and 'Active' is False\n",
    "to_drop = duplicates[(duplicates['Active'] == False)]\n",
    "\n",
    "# Drop these rows from the original DataFrame\n",
    "df_cleaned = df.drop(to_drop.index)\n",
    "\n",
    "# Optionally, save the cleaned DataFrame to a new CSV file\n",
    "output_path = r\"D:\\dataquality2\\Combined_Cleaned_Call_Type.csv\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of rows dropped\n",
    "print(f\"Number of rows dropped: {len(to_drop)}\")\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # Define the path to your Excel file\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# # Display all columns in the DataFrame\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Load the Excel file\n",
    "# df = pd.read_excel(path, sheet_name='missing_calltype')\n",
    "\n",
    "# # Replace 0 with NaN and then fill NaN with empty string\n",
    "# df = df.replace(0, np.nan).fillna('')\n",
    "\n",
    "# # Define the columns to check\n",
    "# columns_to_check = [\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner', 'T3.2 - Product Owner', 'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team', 'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# # Add new columns for Level 1-4 OLA\n",
    "# df['Level 1 OLA'] = None\n",
    "# df['Level 2 OLA'] = None\n",
    "# df['Level 3 OLA'] = None\n",
    "# df['Level 4 OLA'] = None\n",
    "\n",
    "# # Function to fill in the OLA levels based on non-empty columns in order\n",
    "# def fill_ola_levels(row):\n",
    "#     values = []\n",
    "#     for col in columns_to_check:\n",
    "#         if pd.notna(row[col]):\n",
    "#             values.append(row[col])\n",
    "#         if len(values) == 4:\n",
    "#             break\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if i < len(values):\n",
    "#             row[f'Level {i+1} OLA'] = values[i]\n",
    "#         else:\n",
    "#             row[f'Level {i+1} OLA'] = None\n",
    "#     return row\n",
    "\n",
    "# # Apply the function to each row\n",
    "# df = df.apply(fill_ola_levels, axis=1)\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'Level 1 OLA',\n",
    "#     'Level 2 OLA',\n",
    "#     'Level 3 OLA',\n",
    "#     'Level 4 OLA',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "        \n",
    "# # Function to remove .0 from float values and handle empty strings\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     if value == '':\n",
    "#         return np.nan\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero).astype('Int64')\n",
    "\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # df = df.drop(columns=columns_to_check)\n",
    "# df.to_csv('Cleaned3_Call_Type_missing.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning missing call type\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # Define the path to your Excel file\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# # Display all columns in the DataFrame\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Load the Excel file\n",
    "# df = pd.read_excel(path, sheet_name='missing_calltype')\n",
    "\n",
    "\n",
    "\n",
    "# # Replace 0 with NaN and then fill NaN with empty string\n",
    "# df = df.replace(0, np.nan).fillna('')\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'No',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Ensure columns_to_format exist in df\n",
    "# existing_columns_to_format = [col for col in columns_to_format if col in df.columns]\n",
    "\n",
    "# # Function to format columns\n",
    "# def format_columns(col):\n",
    "#     try:\n",
    "#         return str(float(col)).rstrip('.0')\n",
    "#     except ValueError:\n",
    "#         return col\n",
    "\n",
    "# # Apply the formatting function to selected columns\n",
    "# df[existing_columns_to_format] = df[existing_columns_to_format].applymap(format_columns)\n",
    "\n",
    "# # Ensure the 'active' column exists before modifying\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # Convert relevant columns to numeric for addition if they exist\n",
    "# numeric_columns = [\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# existing_numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "\n",
    "# for col in existing_numeric_columns:\n",
    "#     df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# # Combine columns with addition\n",
    "# if 'T2.2 - Supporting Unit' in df.columns and 'T2.3 - Supporting Unit' in df.columns:\n",
    "#     df['T2.2 - Supporting Unit'] += df['T2.3 - Supporting Unit']\n",
    "#     df = df.drop(columns=['T2.3 - Supporting Unit'])\n",
    "\n",
    "# if 'T3.1 - Product Owner' in df.columns and 'T3.2 - Product Owner' in df.columns and 'T3.3 - Product Owner' in df.columns:\n",
    "#     df['T3.1 - Product Owner'] += df['T3.2 - Product Owner'] + df['T3.3 - Product Owner']\n",
    "#     df = df.drop(columns=['T3.2 - Product Owner', 'T3.3 - Product Owner'])\n",
    "\n",
    "# if 'T4.1 - Technical Team' in df.columns and 'T4.2 - Technical Team' in df.columns:\n",
    "#     df['T4.1 - Technical Team'] += df['T4.2 - Technical Team']\n",
    "#     df = df.drop(columns=['T4.2 - Technical Team'])\n",
    "\n",
    "# # Format the combined columns to remove .0\n",
    "# combined_columns = ['T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team']\n",
    "# for col in combined_columns:\n",
    "#     if col in df.columns:\n",
    "#         df[col] = df[col].apply(format_columns)\n",
    "\n",
    "# # Columns to replace '-' with empty string\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# # Ensure columns_to_replace exist in df\n",
    "# existing_columns_to_replace = [col for col in columns_to_replace if col in df.columns]\n",
    "\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# df[existing_columns_to_replace] = df[existing_columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# # Add the 'external_id' column\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero)\n",
    "    \n",
    "# # Save the cleaned data to a CSV file\n",
    "# df.to_csv('Cleaned2_Call_Type_missing.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Load the Excel file\n",
    "# file_path = r\"C:\\Users\\maste\\Downloads\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan (2).xlsx\"\n",
    "# df = pd.read_excel(file_path, sheet_name='FIX Upload - Update Here')\n",
    "# # df = df.drop(columns=['Salesforce', 'Status.1', 'Revisi Detail', 'Notes', 'User Grouping_Old', 'T4.3 - Technical Team', 'T5.1 - Third Party', 'T5.2 - Third Party', 'T5.3 - Third Party', 'Jenis Produk dan atau Layanan LKPBU', 'Kategori Permasalahan LKPBU', 'Jenis Transaksi LKPBU', 'Penyebab Pengaduan LKPBU', 'Jenis Produk LBUT', 'Sandi Jenis  Produk LBUT', 'Sebab Pengaduan LBUT', 'Sandi Penyebab LBUT'])\n",
    "\n",
    "# # Define the columns to check\n",
    "# columns_to_check = [\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner', 'T3.2 - Product Owner', 'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team', 'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# # Add new columns for Level 1-4 OLA\n",
    "# df['Level 1 OLA'] = None\n",
    "# df['Level 2 OLA'] = None\n",
    "# df['Level 3 OLA'] = None\n",
    "# df['Level 4 OLA'] = None\n",
    "\n",
    "# # Function to fill in the OLA levels based on non-empty columns in order\n",
    "# def fill_ola_levels(row):\n",
    "#     values = []\n",
    "#     for col in columns_to_check:\n",
    "#         if pd.notna(row[col]):\n",
    "#             values.append(row[col])\n",
    "#         if len(values) == 4:\n",
    "#             break\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if i < len(values):\n",
    "#             row[f'Level {i+1} OLA'] = values[i]\n",
    "#         else:\n",
    "#             row[f'Level {i+1} OLA'] = None\n",
    "#     return row\n",
    "\n",
    "# # Apply the function to each row\n",
    "# df = df.apply(fill_ola_levels, axis=1)\n",
    "\n",
    "# # df = df.drop(columns=columns_to_check)\n",
    "\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# df[columns_to_replace] = df[columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'Level 1 OLA',\n",
    "#     'Level 2 OLA',\n",
    "#     'Level 3 OLA',\n",
    "#     'Level 4 OLA',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "    \n",
    "# ]\n",
    "\t\t\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero).astype('Int64')\n",
    "\n",
    "\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# df['Detail BRICare'] = df['Detail BRICare'].fillna('-')\n",
    "# # Save the modified dataframe to a new Excel file\n",
    "# output_path = 'Cleaned3_Call_Type.xlsx'\n",
    "# output_path2 = 'Cleaned3_Call_Type.csv'\n",
    "# df.to_excel(output_path, index=False)\n",
    "# df.to_csv(output_path2, index=False)\n",
    "\n",
    "# # Output path of the updated file\n",
    "# print(output_path)\n",
    "# print(output_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_52768\\1505204889.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[columns_to_format] = df[columns_to_format].applymap(format_columns)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan.xlsx\"\n",
    "# # path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# pd.set_option('display.max_columns', None)  \n",
    "# # pd.set_option('display.max_rows', None) \n",
    "# df = pd.read_excel(path)\n",
    "# df = df.drop(columns=['Salesforce', 'Status.1', 'Revisi Detail', 'Notes', 'User Grouping_Old','T4.3 - Technical Team','T5.1 - Third Party','T5.2 - Third Party','T5.3 - Third Party','Jenis Produk dan atau Layanan LKPBU','Kategori Permasalahan LKPBU','Jenis Transaksi LKPBU','Penyebab Pengaduan LKPBU','Jenis Produk LBUT','Sandi Jenis  Produk LBUT','Sebab Pengaduan LBUT','Sandi Penyebab LBUT'])\n",
    "\n",
    "# df = df.replace(0, np.nan)\n",
    "# df = df.fillna('')\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'No',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to format columns\n",
    "# def format_columns(col):\n",
    "#     try:\n",
    "#         # Convert to float to handle empty strings and non-numeric values\n",
    "#         return str(float(col)).rstrip('.0')\n",
    "#     except ValueError:\n",
    "#         return col  # Return original value if conversion fails\n",
    "\n",
    "# # Apply the formatting function to selected columns\n",
    "# df[columns_to_format] = df[columns_to_format].applymap(format_columns)\n",
    "# df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # Define the columns to check\n",
    "# columns_to_check = [\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner', 'T3.2 - Product Owner', 'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team', 'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# # Add new columns for Level 1-4 OLA\n",
    "# df['Level 1 OLA'] = None\n",
    "# df['Level 2 OLA'] = None\n",
    "# df['Level 3 OLA'] = None\n",
    "# df['Level 4 OLA'] = None\n",
    "\n",
    "\n",
    "# # Function to fill in the OLA levels based on non-empty columns in order\n",
    "# def fill_ola_levels(row):\n",
    "#     values = []\n",
    "#     for col in columns_to_check:\n",
    "#         if pd.notna(row[col]):\n",
    "#             values.append(row[col])\n",
    "#         if len(values) == 4:\n",
    "#             break\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if i < len(values):\n",
    "#             row[f'Level {i+1} OLA'] = values[i]\n",
    "#         else:\n",
    "#             row[f'Level {i+1} OLA'] = None\n",
    "#     return row\n",
    "\n",
    "# # Apply the function to each row\n",
    "# df = df.apply(fill_ola_levels, axis=1)\n",
    "\n",
    "# # Apply the function to each row\n",
    "\n",
    "# df = df.drop(columns=columns_to_check)\n",
    "# # ====================================================================================================================Convert relevant columns to numeric for addition\n",
    "# # df['T2.2 - Supporting Unit'] = pd.to_numeric(df['T2.2 - Supporting Unit'], errors='coerce').fillna(0)\n",
    "# # df['T2.3 - Supporting Unit'] = pd.to_numeric(df['T2.3 - Supporting Unit'], errors='coerce').fillna(0)\n",
    "# # df['T3.1 - Product Owner'] = pd.to_numeric(df['T3.1 - Product Owner'], errors='coerce').fillna(0)\n",
    "# # df['T3.2 - Product Owner'] = pd.to_numeric(df['T3.2 - Product Owner'], errors='coerce').fillna(0)\n",
    "# # df['T3.3 - Product Owner'] = pd.to_numeric(df['T3.3 - Product Owner'], errors='coerce').fillna(0)\n",
    "# # df['T4.1 - Technical Team'] = pd.to_numeric(df['T4.1 - Technical Team'], errors='coerce').fillna(0)\n",
    "# # df['T4.2 - Technical Team'] = pd.to_numeric(df['T4.2 - Technical Team'], errors='coerce').fillna(0)\n",
    "\n",
    "# # # Combine columns with addition\n",
    "# # df['T2.2 - Supporting Unit'] = df['T2.2 - Supporting Unit'] + df['T2.3 - Supporting Unit']\n",
    "# # df = df.drop(columns=['T2.3 - Supporting Unit'])\n",
    "\n",
    "# # df['T3.1 - Product Owner'] = df['T3.1 - Product Owner'] + df['T3.2 - Product Owner'] + df['T3.3 - Product Owner']\n",
    "# # df = df.drop(columns=['T3.2 - Product Owner', 'T3.3 - Product Owner'])\n",
    "\n",
    "# # df['T4.1 - Technical Team'] = df['T4.1 - Technical Team'] + df['T4.2 - Technical Team']\n",
    "# # df = df.drop(columns=['T4.2 - Technical Team'])\n",
    "\n",
    "# # # Format the combined columns to remove .0\n",
    "# # df['T2.2 - Supporting Unit'] = df['T2.2 - Supporting Unit'].apply(format_columns)\n",
    "# # df['T3.1 - Product Owner'] = df['T3.1 - Product Owner'].apply(format_columns)\n",
    "# # df['T4.1 - Technical Team'] = df['T4.1 - Technical Team'].apply(format_columns)\n",
    "# # =============================================================================================================================================================\n",
    "# # Columns to replace '-' with empty string\n",
    "\n",
    "\n",
    "\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# df[columns_to_replace] = df[columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero)\n",
    "\n",
    "# df.to_csv('Cleaned3_Call_Type.csv', index=False)\n",
    "# # df['external_id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_62668\\3534049870.py:48: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[existing_columns_to_format] = df[existing_columns_to_format].applymap(format_columns)\n"
     ]
    }
   ],
   "source": [
    "# # Cleaning missing call type\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # Define the path to your Excel file\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\Data Migration Monitoring.xlsx\"\n",
    "\n",
    "# # Display all columns in the DataFrame\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Load the Excel file\n",
    "# df = pd.read_excel(path, sheet_name='missing_calltype')\n",
    "\n",
    "\n",
    "\n",
    "# # Replace 0 with NaN and then fill NaN with empty string\n",
    "# df = df.replace(0, np.nan).fillna('')\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'No',\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Ensure columns_to_format exist in df\n",
    "# existing_columns_to_format = [col for col in columns_to_format if col in df.columns]\n",
    "\n",
    "# # Function to format columns\n",
    "# def format_columns(col):\n",
    "#     try:\n",
    "#         return str(float(col)).rstrip('.0')\n",
    "#     except ValueError:\n",
    "#         return col\n",
    "\n",
    "# # Apply the formatting function to selected columns\n",
    "# df[existing_columns_to_format] = df[existing_columns_to_format].applymap(format_columns)\n",
    "\n",
    "# # Ensure the 'active' column exists before modifying\n",
    "# if 'active' in df.columns:\n",
    "#     df['active'] = df['active'].astype(str).str.upper()\n",
    "\n",
    "# # Convert relevant columns to numeric for addition if they exist\n",
    "# numeric_columns = [\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T2.3 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T3.2 - Product Owner',\n",
    "#     'T3.3 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'T4.2 - Technical Team'\n",
    "# ]\n",
    "\n",
    "# existing_numeric_columns = [col for col in numeric_columns if col in df.columns]\n",
    "\n",
    "# for col in existing_numeric_columns:\n",
    "#     df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# # Combine columns with addition\n",
    "# if 'T2.2 - Supporting Unit' in df.columns and 'T2.3 - Supporting Unit' in df.columns:\n",
    "#     df['T2.2 - Supporting Unit'] += df['T2.3 - Supporting Unit']\n",
    "#     df = df.drop(columns=['T2.3 - Supporting Unit'])\n",
    "\n",
    "# if 'T3.1 - Product Owner' in df.columns and 'T3.2 - Product Owner' in df.columns and 'T3.3 - Product Owner' in df.columns:\n",
    "#     df['T3.1 - Product Owner'] += df['T3.2 - Product Owner'] + df['T3.3 - Product Owner']\n",
    "#     df = df.drop(columns=['T3.2 - Product Owner', 'T3.3 - Product Owner'])\n",
    "\n",
    "# if 'T4.1 - Technical Team' in df.columns and 'T4.2 - Technical Team' in df.columns:\n",
    "#     df['T4.1 - Technical Team'] += df['T4.2 - Technical Team']\n",
    "#     df = df.drop(columns=['T4.2 - Technical Team'])\n",
    "\n",
    "# # Format the combined columns to remove .0\n",
    "# combined_columns = ['T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team']\n",
    "# for col in combined_columns:\n",
    "#     if col in df.columns:\n",
    "#         df[col] = df[col].apply(format_columns)\n",
    "\n",
    "# # Columns to replace '-' with empty string\n",
    "# columns_to_replace = ['Segment', 'Product', 'Sub Product', 'Case Category']\n",
    "# # Ensure columns_to_replace exist in df\n",
    "# existing_columns_to_replace = [col for col in columns_to_replace if col in df.columns]\n",
    "\n",
    "# # Replace '-' with empty string in selected columns\n",
    "# df[existing_columns_to_replace] = df[existing_columns_to_replace].replace('-', '')\n",
    "\n",
    "# # Ensure 'Case Types' is formatted correctly and handle empty values\n",
    "# def clean_case_types(value):\n",
    "#     if pd.isna(value) or value == '':\n",
    "#         return '0'\n",
    "#     match = re.match(r'(\\d{4})-(\\d{2})', str(value))\n",
    "#     if match:\n",
    "#         year, month = match.groups()\n",
    "#         return f'{year}-{int(month)}'\n",
    "#     # Remove .0 from the end if present\n",
    "#     value = re.sub(r'\\.0$', '', str(value))\n",
    "#     return value\n",
    "\n",
    "# df['Case Types'] = df['Case Types'].apply(clean_case_types)\n",
    "# # Add the 'external_id' column\n",
    "# df['external_id'] = df['Case Types']\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     df[column] = df[column].apply(remove_decimal_zero)\n",
    "    \n",
    "# # Save the cleaned data to a CSV file\n",
    "# df.to_csv('Cleaned2_Call_Type_missing.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates in File 1.\n",
      "Duplicates in File 2:\n",
      "443    1201\n",
      "Name: Case Types, dtype: object\n",
      "File 1 has all unique values: True\n",
      "File 2 has all unique values: False\n",
      "No overlapping values in 'Case Types' between the two files.\n"
     ]
    }
   ],
   "source": [
    "# check if the missing call type and Cleaned file not overlapping case types\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your two CSV files\n",
    "file1_path = r\"D:\\dataquality2\\Cleaned3_Call_Type_missing.csv\"\n",
    "file2_path = r\"D:\\dataquality2\\Cleaned3_Call_Type.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Extract the 'Case Types' column from each DataFrame\n",
    "case_types_1 = df1['Case Types'].astype(str)\n",
    "case_types_2 = df2['Case Types'].astype(str)\n",
    "\n",
    "# Check for duplicates within each file\n",
    "duplicates_1 = case_types_1[case_types_1.duplicated()]\n",
    "duplicates_2 = case_types_2[case_types_2.duplicated()]\n",
    "\n",
    "# Print duplicates within each file\n",
    "if not duplicates_1.empty:\n",
    "    print(\"Duplicates in File 1:\")\n",
    "    print(duplicates_1)\n",
    "else:\n",
    "    print(\"No duplicates in File 1.\")\n",
    "\n",
    "if not duplicates_2.empty:\n",
    "    print(\"Duplicates in File 2:\")\n",
    "    print(duplicates_2)\n",
    "else:\n",
    "    print(\"No duplicates in File 2.\")\n",
    "\n",
    "# Check if each file has unique values in the 'Case Types' column\n",
    "unique_case_types_1 = case_types_1.nunique() == len(case_types_1)\n",
    "unique_case_types_2 = case_types_2.nunique() == len(case_types_2)\n",
    "\n",
    "print(f\"File 1 has all unique values: {unique_case_types_1}\")\n",
    "print(f\"File 2 has all unique values: {unique_case_types_2}\")\n",
    "\n",
    "# Convert to sets for comparison\n",
    "set_case_types_1 = set(case_types_1)\n",
    "set_case_types_2 = set(case_types_2)\n",
    "\n",
    "# Find overlapping values\n",
    "overlapping_values = set_case_types_1.intersection(set_case_types_2)\n",
    "\n",
    "if overlapping_values:\n",
    "    print(\"There are overlapping values in 'Case Types' between the two files:\")\n",
    "    print(overlapping_values)\n",
    "else:\n",
    "    print(\"No overlapping values in 'Case Types' between the two files.\")\n",
    "\n",
    "# Optionally, save the results to CSV files if needed\n",
    "# pd.Series(list(overlapping_values)).to_csv('Overlapping_Values.csv', index=False)\n",
    "\n",
    "if not duplicates_1.empty:\n",
    "    duplicates_1.to_csv('Duplicates_in_File1.csv', index=False)\n",
    "if not duplicates_2.empty:\n",
    "    duplicates_2.to_csv('Duplicates_in_File2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_1100\\104105547.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([df1, df2], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Combine Cleaned Call Type with Missing Call Type:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV files\n",
    "file_1_path = r\"D:\\dataquality2\\Cleaned3_Call_Type_missing.csv\"\n",
    "file_2_path = r\"D:\\dataquality2\\Cleaned3_Call_Type.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_1_path)\n",
    "df2 = pd.read_csv(file_2_path)\n",
    "\n",
    "# Identify columns in File 2 that are not in File 1\n",
    "missing_in_file_1 = set(df2.columns) - set(df1.columns)\n",
    "\n",
    "# Ensure all columns from the larger dataframe are present in the smaller dataframe before combining\n",
    "# Add missing columns to df1 with NaN values\n",
    "for col in missing_in_file_1:\n",
    "    df1[col] = pd.NA\n",
    "\n",
    "# Reorder columns in df1 to match the order of columns in df2\n",
    "df1 = df1[df2.columns]\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# # Columns to format\n",
    "# columns_to_format = [\n",
    "#     'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit',\n",
    "#     'T2.2 - Supporting Unit',\n",
    "#     'T3.1 - Product Owner',\n",
    "#     'T4.1 - Technical Team',\n",
    "#     'Total SLA',\n",
    "#     'New SLA'\n",
    "# ]\n",
    "\n",
    "# # Function to remove .0 from float values\n",
    "# def remove_decimal_zero(value):\n",
    "#     if isinstance(value, float) and value.is_integer():\n",
    "#         return int(value)\n",
    "#     return value\n",
    "\n",
    "# # Apply function to specified columns\n",
    "# for column in columns_to_format:\n",
    "#     combined_df[column] = combined_df[column].apply(remove_decimal_zero)\n",
    "\n",
    "\n",
    "# Columns to format\n",
    "columns_to_format = [\n",
    "    'Level 1 OLA',\n",
    "    'Level 2 OLA',\n",
    "    'Level 3 OLA',\n",
    "    'Level 4 OLA',\n",
    "    'T1 - Agent CCT/Frontliner',\n",
    "    'Total SLA',\n",
    "    'New SLA'\n",
    "]\n",
    "        \n",
    "# Function to remove .0 from float values and handle empty strings\n",
    "def remove_decimal_zero(value):\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return int(value)\n",
    "    if value == '':\n",
    "        return np.nan\n",
    "    return value\n",
    "\n",
    "# Apply function to specified columns\n",
    "for column in columns_to_format:\n",
    "    combined_df[column] = combined_df[column].apply(remove_decimal_zero).astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_file_path = 'Combined3_Cleaned_Call_Type.csv'\n",
    "combined_file_path2 = 'Combined3_Cleaned_Call_Type.xlsx'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "combined_df.to_excel(combined_file_path2, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Call Type IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Call Type IDs]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Compare the call type Pak SUyanto/ bricare with master call type data\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# calltype_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\calltype_psuyanto.csv\")\n",
    "# # final_df = pd.read_excel(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan.xlsx\", sheet_name=None)\n",
    "# final_call_type_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\calltype_psuyanto2.csv\")\n",
    "\n",
    "# # final_call_type_df = final_df['Full Call Type Fix']\n",
    "\n",
    "\n",
    "# calltype_ids = calltype_df['calltypeidtxt'].astype(str)\n",
    "# case_types = final_call_type_df['Case Types'].astype(str)\n",
    "\n",
    "\n",
    "# missing_calltype_ids = calltype_ids[~calltype_ids.isin(case_types)]\n",
    "\n",
    "# missing_calltype_ids_df = missing_calltype_ids.to_frame(name='Missing Call Type IDs')\n",
    "\n",
    "\n",
    "# # missing_calltype_ids_df.to_csv(r\"C:\\Users\\maste\\Downloads\\missing_calltype_ids.csv\", index=False)\n",
    "\n",
    "# # print(\"Missing call type IDs have been saved to missing_calltype_ids.csv\")\n",
    "# missing_calltype_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add Escalation Team\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the provided CSV files\n",
    "# combined_cleaned_call_type = pd.read_csv(r\"D:\\dataquality2\\Combined_Cleaned_Call_Type.csv\")\n",
    "# sla_ola_new_user_grouping = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type\\(FINAL) SLA&OLA_NewUserGrouping_Ringkasan - Appendix Team Name.csv\")\n",
    "\n",
    "# # Create a mapping from the SLA & OLA dataframe\n",
    "# team_mapping = sla_ola_new_user_grouping.set_index('/xc//.vs')['Full Name / Queue Name'].to_dict()\n",
    "\n",
    "# # Adding more mappings based on the provided information\n",
    "# additional_mappings = {\n",
    "#     'RO BRILink': 'RO - Business Channel BRILink',\n",
    "#     'RO ITE': 'RO - Information, Technology & E-Channel',\n",
    "#     'BRANCH(Uker)': 'Branch - Operational Unit',\n",
    "#     'CDD(CLF-1DATAM)': 'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     'CDD(CLF-1 DATAM)': 'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     'CCTFrontliner': 'CCT - Contact Center Frontliner'\n",
    "# }\n",
    "\n",
    "# # Update the original mapping with additional mappings\n",
    "# team_mapping.update(additional_mappings)\n",
    "\n",
    "# # Function to clean team names\n",
    "# def clean_team_name_further(team_name):\n",
    "#     if pd.isna(team_name):\n",
    "#         return team_name\n",
    "#     # Remove specific patterns and clean up the string\n",
    "#     team_name = team_name.replace(\"Agent\", \"\").replace(\"Include\", \"\").strip()\n",
    "#     team_name = team_name.lstrip(\"-\").strip()\n",
    "#     # Remove spaces within names for better mapping\n",
    "#     team_name = team_name.replace(\" \", \"\")\n",
    "#     team_name = ' '.join(word.strip('-').strip() for word in team_name.split())\n",
    "#     return team_name\n",
    "\n",
    "# # Applying the cleaning function to the correct column 'Team Name'\n",
    "# combined_cleaned_call_type['Cleaned Team Name'] = combined_cleaned_call_type['Team Name'].apply(clean_team_name_further)\n",
    "\n",
    "# # Function to clean and translate team names with specific rules\n",
    "# def clean_and_translate_team_name_with_specific_rules(team_name, mapping):\n",
    "#     if pd.isna(team_name) or team_name == \"\" or isinstance(team_name, float):\n",
    "#         return [None, None, None, None]\n",
    "#     team_name_cleaned = ' '.join(word.strip('-').strip() for word in str(team_name).split())\n",
    "#     # Specific rule: if the team name contains \"T1-CCT/Frontliner\", return None for all levels\n",
    "#     if \"T1-CCT/Frontliner\" in team_name:\n",
    "#         return [None, None, None, None]\n",
    "#     # Remove \"Insurance Company\" and \"Vendor\"\n",
    "#     team_name_cleaned = team_name_cleaned.replace(\"InsuranceCompany\", \"\").replace(\"Vendor\", \"\").strip()\n",
    "#     # Translate using the mapping\n",
    "#     teams = [mapping.get(t.strip(), t.strip()) for t in team_name_cleaned.split(',')]\n",
    "#     return teams + [None] * (4 - len(teams))\n",
    "\n",
    "# # Reapplying the cleaning and translation process with the specific rules\n",
    "# levels = combined_cleaned_call_type['Cleaned Team Name'].apply(lambda x: clean_and_translate_team_name_with_specific_rules(x, team_mapping))\n",
    "\n",
    "# # Ensuring the result is a DataFrame with correct None values\n",
    "# level_columns = pd.DataFrame(levels.tolist(), columns=['Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team'], index=combined_cleaned_call_type.index)\n",
    "\n",
    "# # Creating placeholders for OLA levels\n",
    "# ola_columns = pd.DataFrame(levels.tolist(), columns=['Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA'], index=combined_cleaned_call_type.index)\n",
    "\n",
    "# # Assigning the translated names to the escalation levels and placeholders to OLA levels\n",
    "# combined_cleaned_call_type = pd.concat([combined_cleaned_call_type, level_columns, ola_columns], axis=1)\n",
    "\n",
    "# # Manually correcting specific known issues\n",
    "# index_to_correct = combined_cleaned_call_type['Cleaned Team Name'] == 'T1-CCT/Frontliner'\n",
    "# combined_cleaned_call_type.loc[index_to_correct, ['Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team', 'Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA']] = None\n",
    "\n",
    "# index_to_correct = combined_cleaned_call_type['Cleaned Team Name'].str.contains('CDD\\(ACUOTO\\),CDD\\(ACUCHARGEBACK\\),CDD\\(CLF-1DATAM\\)', na=False)\n",
    "# combined_cleaned_call_type.loc[index_to_correct, ['Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team', 'Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA']] = [\n",
    "#     'CDD - Auth,Chrgbck & UserCo - OTO',\n",
    "#     'CDD - Auth,Chrgbck & UserCo - CHARGEBACK',\n",
    "#     'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     None,\n",
    "#     'CDD - Auth,Chrgbck & UserCo - OTO',\n",
    "#     'CDD - Auth,Chrgbck & UserCo - CHARGEBACK',\n",
    "#     'CDD - Clf-1 Credit Card Init - DATAM',\n",
    "#     None\n",
    "# ]\n",
    "\n",
    "# # Keeping all specified columns\n",
    "# columns_to_keep = [\n",
    "#     'No', 'Case Types', 'active', 'User Grouping_New', 'Segment', 'Product', 'Sub Product', 'Case Category', 'Case Description',\n",
    "#     'New Case Description', 'Required Sub-Description', 'Sub', 'Doc Intake', 'List Kebutuhan Dokumen', 'Suggestion', 'Suggestion - Breakdown',\n",
    "#     'Level of Effort', 'Range of Days \\n(Actual SLA)', 'Actual SLA\\n(days)', 'Target SLA \\n(days)', 'BRI Notes', 'Team Name', 'T1 - Agent CCT/Frontliner',\n",
    "#     'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team', 'Total SLA', 'New SLA', 'Status', 'GIC', 'Maker',\n",
    "#     'Checker', 'Signer', 'Courtesy Awal', 'Kebutuhan Data', 'Detail BRICare', 'Unnamed: 58', 'external_id', 'Cleaned Team Name',\n",
    "#     'Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team', 'Level 1 OLA', 'Level 2 OLA', 'Level 3 OLA', 'Level 4 OLA'\n",
    "# ]\n",
    "\n",
    "# # Selecting the specified columns\n",
    "# final_dataset_with_all_columns = combined_cleaned_call_type[columns_to_keep]\n",
    "\n",
    "# # Saving the final dataset to a new CSV file\n",
    "# final_dataset_with_all_columns.to_csv('Final_Cleaned_Escalation_Teams_All_Columns.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\Final_Revised_Combined_Cleaned_Call_Type_with_Specific_Rules_Corrected.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "columns_to_keep = [\n",
    "    'No', 'Case Types', 'active', 'User Grouping_New', 'Segment', 'Product', 'Sub Product', 'Case Category', 'Case Description',\n",
    "    'New Case Description', 'Required Sub-Description', 'Sub', 'Doc Intake', 'List Kebutuhan Dokumen', 'Suggestion', 'Suggestion - Breakdown',\n",
    "    'Level of Effort', 'Range of Days \\n(Actual SLA)', 'Actual SLA\\n(days)', 'Target SLA \\n(days)', 'BRI Notes', 'Team Name', 'T1 - Agent CCT/Frontliner',\n",
    "    'T2.1 - Supporting Unit', 'T2.2 - Supporting Unit', 'T3.1 - Product Owner', 'T4.1 - Technical Team', 'Total SLA', 'New SLA', 'Status', 'GIC', 'Maker',\n",
    "    'Checker', 'Signer', 'Courtesy Awal', 'Kebutuhan Data', 'Detail BRICare', 'Unnamed: 58', 'external_id', 'Cleaned Team Name',\n",
    "    'Level 1 Escalation Team', 'Level 2 Escalation Team', 'Level 3 Escalation Team', 'Level 4 Escalation Team'\n",
    "]\n",
    "\n",
    "# Selecting the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "columns_to_process = [\n",
    "    'Level 1 Escalation Team',\n",
    "    'Level 2 Escalation Team',\n",
    "    'Level 3 Escalation Team',\n",
    "    'Level 4 Escalation Team'\n",
    "]\n",
    "\n",
    "# Replace \"Vendor\" and \"Insurance Company\" with an empty string in the specified columns\n",
    "for column in columns_to_process:\n",
    "    df[column] = df[column].replace(['Vendor', 'Insurance Company'], '')\n",
    "    \n",
    "df['Detail BRICare'] = df['Detail BRICare'].fillna('-')\n",
    "\n",
    "df.to_csv('final_cleaned_callType.csv', index=False)\n",
    "# Save to Excel\n",
    "df.to_excel('final_cleaned_callType.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call type mapping for columns 'Produk', 'Jenis Produk', 'Jenis Laporan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "user_dataset_path = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "user_df = pd.read_csv(user_dataset_path)\n",
    "master_df_path = r\"D:\\dataquality\\master_calltype.csv\"\n",
    "master_df = pd.read_csv(master_df_path)\n",
    "\n",
    "\n",
    "master_df = master_df.rename(columns={\n",
    "    'Case Types': 'Call_Type_ID', \n",
    "    'Product': 'Produk', \n",
    "    'Sub Product': 'Jenis_Produk', \n",
    "    'Case Category': 'Jenis_Laporan'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "user_df['Call_Type_ID'] = user_df['Call_Type_ID'].astype(str)\n",
    "master_df['Call_Type_ID'] = master_df['Call_Type_ID'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(user_df, master_df[['Call_Type_ID', 'Produk', 'Jenis_Produk', 'Jenis_Laporan']], on='Call_Type_ID', how='left')\n",
    "\n",
    "user_df['Produk'] = merged_df['Produk_y']\n",
    "user_df['Jenis_Produk'] = merged_df['Jenis_Produk_y']\n",
    "user_df['Jenis_Laporan'] = merged_df['Jenis_Laporan_y']\n",
    "\n",
    "\n",
    "\n",
    "user_df.to_csv(user_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_text_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:  \n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]  \n",
    "                current_entry.append(parts[3].strip())  \n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def remove_bom_and_strip(df):\n",
    "    return df.applymap(lambda x: x.replace('\\ufeff', '').strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.txt\"\n",
    "processed_data = process_text_data(file_path)\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "# df_final=df_final.iloc[:10]\n",
    "df_final.iloc[:10].to_csv('details_uat_20230101_20230101.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File A.2 Details only. Please use this if the file is csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to details_20230101_20230101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:\n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]\n",
    "                current_entry.append(parts[3].strip())\n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.csv\"\n",
    "processed_data = process_csv_data(file_path)\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "\n",
    "df_final = df_final.iloc[:10]\n",
    "output_path = \"details_20230101_20230101.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the file A.1 and file A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#just take 10 lines for an example\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.iloc[:10].to_csv(path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path_1 = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "file_path_2 = r\"D:\\dataquality2\\details_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "df_tenline_bricare = pd.read_csv(file_path_1)\n",
    "df_detail_bricare_10line = pd.read_csv(file_path_2)\n",
    "\n",
    "df_detail_bricare_10line.columns = ['Ticket_ID', 'Details']\n",
    "\n",
    "merged_df = pd.merge(df_tenline_bricare, df_detail_bricare_10line, on='Ticket_ID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "column_to_move=\"Details\"\n",
    "merged_df = merged_df[[col for col in merged_df if col != column_to_move][:3] + [column_to_move] + [col for col in merged_df if col != column_to_move][3:]] \n",
    "\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Type B\n",
    "\n",
    "\n",
    "Data Extraction for File Type B (27 columns)\n",
    "\n",
    "\n",
    "Columns to be cleansed or Transform:\n",
    "- All columns with values \"None\", \"NaN, \"N/A\", \"NULL\"\n",
    "- These columns must follow this datetime format: format='%Y-%m-%d %H:%M:%S' or format='%Y-%m-%d %H:%M:%S.%f' \n",
    "\n",
    "['TanggalClosed', 'tanggalTransaksi','Create_Date']\n",
    "\n",
    "- Remove all unknown characters e.g. \\ufeff in column \"Ticket_ID\" if any\n",
    "\n",
    "- PLEASE ADD CIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_33500\\3452762791.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricare_20200101_20200101.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\BRICARE_25042024 masking.csv\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "\n",
    "data['Column1'] = data['Column1'].astype(str)\n",
    "data_cleaned = data[data['Column1'].str.match(r'TTB\\d+')]\n",
    "\n",
    "data_cleaned['Column2'] = data_cleaned['Column2'].astype(str)\n",
    "data_cleaned = data_cleaned[data_cleaned['Column2'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "data_cleaned['Column4'] = pd.to_datetime(data_cleaned['Column4'], errors='coerce')\n",
    "data_cleaned = data_cleaned.dropna(subset=['Column4'])\n",
    "\n",
    "\n",
    "data_to_drop = ['Column28', 'Column29', 'Column30', 'Column31', 'Column32']\n",
    "data_cleaned = data_cleaned.drop(columns=data_to_drop)\n",
    "\n",
    "\n",
    "if len(data_cleaned.columns) <= len(column_list):\n",
    "    data_cleaned.columns = column_list[:len(data_cleaned.columns)]\n",
    "\n",
    "\n",
    "data_cleaned.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "data_cleaned.fillna('', inplace=True)\n",
    "data_cleaned = data_cleaned.replace(['0', 0], '')\n",
    "\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    data_cleaned[column] = pd.to_datetime(data_cleaned[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    data_cleaned[column] = data_cleaned[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "# Just take 10 lines for an example\n",
    "data_cleaned = data_cleaned.iloc[:10]\n",
    "\n",
    "\n",
    "data_cleaned.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(data_cleaned['Create_Date']))\n",
    "enddate = pd.Timestamp(max(data_cleaned['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricare_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "data_cleaned.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "27\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Details', 'Create_Date',\n",
      "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
      "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
      "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
      "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
      "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
      "       'user_login_name', 'Jenis_Produk', 'Last_Modified_By', 'Merchant_ID',\n",
      "       'Modified_Date', 'NOTAS', 'Produk', 'SLA_Status', 'TID',\n",
      "       'tanggalAttachmentDone', 'Tgl_Assigned', 'Tgl_Eskalasi', 'AnalisaSkils',\n",
      "       'Attachment_', 'Bank_BRI', 'Biaya_Admin', 'Suku_Bunga', 'Bunga',\n",
      "       'Butuh_Attachment', 'Cicilan', 'Hasil_Kunjungan', 'Log_Name',\n",
      "       'MMS_Ticket_Id', 'Mass_Ticket_Upload_Flag', 'Nama_Supervisor',\n",
      "       'Nama_TL', 'Nama_Wakabag', 'Nasabah_Prioritas', 'Notify_By',\n",
      "       'Organization', 'Output_Settlement', 'phone_survey', 'Return_Ticket',\n",
      "       'Settlement_By', 'Settlement_ID', 'Settlement', 'Site_User',\n",
      "       'Status_Return', 'Status_Transaksi', 'Submitter_Region',\n",
      "       'Submitter_SiteGroup', 'Submitter_User_group_ID', 'Tanggal_Settlement',\n",
      "       'Tgl_Foward', 'Tgl_In_Progress', 'Tgl_Returned', 'Ticket_Referensi',\n",
      "       'Tiket_Urgency', 'Tipe_Remark', 'UniqueID', 'users', 'Usergroup_ID'],\n",
      "      dtype='object')\n",
      "Index(['Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date', 'gateway',\n",
      "       'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal', 'status',\n",
      "       'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur', 'Nomor_Kartu',\n",
      "       'user_group', 'assgined_to', 'attachment_done', 'email', 'full_name',\n",
      "       'no_telepon', 'approver_login', 'approver_name', 'SLAResolution',\n",
      "       'submitter_login_id', 'submitter_user_group', 'user_login_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path1=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "path2=r\"D:\\dataquality\\bricare_20200101_20200101.csv\"\n",
    "\n",
    "df1=pd.read_csv(path1)\n",
    "df2=pd.read_csv(path2)\n",
    "print(len(df1.columns))\n",
    "print(len(df2.columns))\n",
    "\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Compare two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape_match': True,\n",
       " 'columns_match': True,\n",
       " 'column_differences': {},\n",
       " 'value_differences': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the newly uploaded files for detailed comparison\n",
    "processed_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "expected_file_path_newest = r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "# Read the files\n",
    "processed_df_newest = pd.read_csv(processed_file_path_newest)\n",
    "expected_df_newest = pd.read_csv(expected_file_path_newest)\n",
    "\n",
    "# Check for exact match first\n",
    "exact_match = processed_df_newest.equals(expected_df_newest)\n",
    "\n",
    "# Initialize a dictionary to store detailed comparison results\n",
    "comparison_details = {\n",
    "    'shape_match': processed_df_newest.shape == expected_df_newest.shape,\n",
    "    'columns_match': processed_df_newest.columns.equals(expected_df_newest.columns),\n",
    "    'column_differences': {},\n",
    "    'value_differences': {}\n",
    "}\n",
    "\n",
    "# Compare each aspect if exact match is not true\n",
    "if not exact_match:\n",
    "    # Check shape\n",
    "    if not comparison_details['shape_match']:\n",
    "        comparison_details['shape_details'] = {\n",
    "            'processed_shape': processed_df_newest.shape,\n",
    "            'expected_shape': expected_df_newest.shape\n",
    "        }\n",
    "    \n",
    "    # Check columns\n",
    "    if not comparison_details['columns_match']:\n",
    "        comparison_details['column_details'] = {\n",
    "            'processed_columns': processed_df_newest.columns.tolist(),\n",
    "            'expected_columns': expected_df_newest.columns.tolist()\n",
    "        }\n",
    "\n",
    "    # Check column-by-column values\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            comparison_details['column_differences'][column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "\n",
    "# Summarize value differences\n",
    "if not comparison_details['columns_match']:\n",
    "    value_differences = {}\n",
    "    for column in processed_df_newest.columns:\n",
    "        if not processed_df_newest[column].equals(expected_df_newest[column]):\n",
    "            value_differences[column] = processed_df_newest[column].compare(expected_df_newest[column])\n",
    "    comparison_details['value_differences'] = value_differences\n",
    "\n",
    "comparison_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To collect all Error logs in a path\n",
    "\n",
    "directory_path is the error logs path as well as the location where the combined error log file will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined error logs saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_error_logs(directory_path):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.startswith(\"error\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Normalize the 'TICKET_ID' to ensure duplicates are identified\n",
    "    combined_df['TICKET_ID'] = combined_df['TICKET_ID'].str.upper()\n",
    "\n",
    "    combined_df = combined_df.drop_duplicates(subset='TICKET_ID')\n",
    "\n",
    "    output_path = os.path.join(directory_path, 'error_logs.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Combined error logs saved to: {output_path}\")\n",
    "    return output_path, combined_df\n",
    "\n",
    "directory_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\"\n",
    "output_path, combined_df = combine_error_logs(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to C:\\Users\\maste\\OneDrive\\file\\log\\log\\combined_errors.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "directory = r\"C:\\Users\\maste\\OneDrive\\file\\log\\log\"\n",
    "\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if 'error' in filename:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read {filename}: {e}\")\n",
    "\n",
    "\n",
    "combined_file_path = os.path.join(directory, 'combined_errors.csv')\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "\n",
    "print(f\"Combined file saved to {combined_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To cleanse user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_43760\\4048578058.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import html\n",
    "\n",
    "\n",
    "# Function to decode HTML entities in a DataFrame\n",
    "def decode_html_entities(df):\n",
    "    df_decoded = df.applymap(lambda x: html.unescape(x) if isinstance(x, str) else x)\n",
    "    return df_decoded\n",
    "\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\USER.txt\" \n",
    "\n",
    "# Read the raw file content\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_data = file.readlines()\n",
    "    \n",
    "# Split headers and data    \n",
    "header = raw_data[0].replace(\"&#124;\", \"|\").strip()\n",
    "data = [line.replace(\"&#124;\", \"|\").strip() for line in raw_data[1:]]\n",
    "\n",
    "# Read the cleaned data into a pandas DataFrame\n",
    "df_cleaned = pd.DataFrame([line.split('|') for line in data], columns=header.split('|'))\n",
    "\n",
    "# Decode HTML entities\n",
    "df_cleaned = decode_html_entities(df_cleaned)\n",
    "df_cleaned\n",
    "\n",
    "# Remove the quotes in Dataframe\n",
    "def remove_quotes(df):\n",
    "    df.columns = df.columns.str.replace('\"', '')\n",
    "    df = df.apply(lambda col: col.str.replace('\"', '', regex=True))\n",
    "    return df\n",
    "\n",
    "df_cleaned = remove_quotes(df_cleaned)\n",
    "df_cleaned\n",
    "df_cleaned.to_csv(\"user_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_15156\\3036651791.py:7: DtypeWarning: Columns (1,8,17,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep=',', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Option 1: Check for quoted fields or use different delimiter\n",
    "try:\n",
    "    df = pd.read_csv(path, sep=',', on_bad_lines='skip') \n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bricaredatareal_20200101_20200201.csv\n",
      "Problematic data saved to bricaredatareal_problematic_20200101_20200201.csv\n",
      "Number of rows in dataframe: 463581\n",
      "Number of problematic lines: 881\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column list\n",
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"\n",
    "]\n",
    "\n",
    "path = r\"D:\\Salesforce\\archive\\dataquality\\gx\\BRICARE_25042024.csv\"\n",
    "\n",
    "# Initialize lists to store data and problematic lines\n",
    "data = []\n",
    "problematic_lines = []\n",
    "\n",
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    try:\n",
    "        # Identifying Ticket_ID\n",
    "        ticket_id_index = next(i for i, part in enumerate(parts) if part.startswith('TTB'))\n",
    "        ticket_id = parts[ticket_id_index]\n",
    "        \n",
    "        # Identifying Call_Type_ID\n",
    "        call_type_id_index = ticket_id_index + 1\n",
    "        call_type_id = parts[call_type_id_index]\n",
    "        \n",
    "        # Identifying Create_Date (the part that looks like a datetime string)\n",
    "        create_date_index = next(i for i, part in enumerate(parts) if '-' in part and ':' in part)\n",
    "        create_date = parts[create_date_index]\n",
    "        \n",
    "        # Call_Type is everything between Call_Type_ID and Create_Date\n",
    "        call_type = ' '.join(parts[call_type_id_index + 1:create_date_index])\n",
    "        \n",
    "        # The rest of the fields in order\n",
    "        rest = parts[create_date_index + 1:]\n",
    "        \n",
    "        # Combine into a single list in the correct order\n",
    "        structured_line = [ticket_id, call_type_id, call_type, create_date] + rest\n",
    "        \n",
    "        # If the length is correct, return it\n",
    "        if len(structured_line) == len(column_list):\n",
    "            return structured_line\n",
    "        # If there are extra fields, handle them (for example, by merging or ignoring)\n",
    "        elif len(structured_line) > len(column_list):\n",
    "            return structured_line[:len(column_list)]\n",
    "        else:\n",
    "            return None\n",
    "    except StopIteration:\n",
    "        # If any part of the parsing fails, consider the line problematic\n",
    "        return None\n",
    "\n",
    "# Read the file line by line and parse it\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        parsed_line = parse_line(lines[i])\n",
    "        if parsed_line:\n",
    "            data.append(parsed_line)\n",
    "            i += 1\n",
    "        else:\n",
    "            # Check if merging the next line solves the issue\n",
    "            if i + 1 < len(lines):\n",
    "                merged_line = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "                parsed_line = parse_line(merged_line)\n",
    "                if parsed_line:\n",
    "                    data.append(parsed_line)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    problematic_lines.append((i, lines[i]))\n",
    "                    i += 1\n",
    "            else:\n",
    "                problematic_lines.append((i, lines[i]))\n",
    "                i += 1\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=column_list)\n",
    "\n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].astype(str)\n",
    "df = df[df['Ticket_ID'].str.match(r'TTB\\d+')]\n",
    "\n",
    "df['Call_Type_ID'] = df['Call_Type_ID'].astype(str)\n",
    "df = df[df['Call_Type_ID'].str.match(r'^\\d{4}$')]\n",
    "\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "df = df.dropna(subset=['Create_Date'])\n",
    "\n",
    "df.replace(['NULL', 'None', 'N/A'], np.nan, inplace=True)\n",
    "df.fillna('', inplace=True)\n",
    "df = df.replace(['0', 0], '')\n",
    "\n",
    "columns_to_convert = ['TanggalClosed', 'tanggalTransaksi', 'Create_Date']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "    df[column] = df[column].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "startdate = pd.Timestamp(min(df['Create_Date']))\n",
    "enddate = pd.Timestamp(max(df['Create_Date']))\n",
    "\n",
    "formatted_startdate = startdate.strftime('%Y%m%d')\n",
    "formatted_enddate = enddate.strftime('%Y%m%d')\n",
    "\n",
    "filename = f\"bricaredatareal_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n",
    "problematic_df = pd.DataFrame(problematic_lines, columns=['Line_Number', 'Data'])\n",
    "\n",
    "\n",
    "problematic_filename = f\"bricaredatareal_problematic_{formatted_startdate}_{formatted_enddate}.csv\"\n",
    "problematic_df.to_csv(problematic_filename, index=False)\n",
    "\n",
    "print(f\"Problematic data saved to {problematic_filename}\")\n",
    "\n",
    "\n",
    "print(f\"Number of rows in dataframe: {len(df)}\")\n",
    "print(f\"Number of problematic lines: {len(problematic_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create masked Data for UAT\n",
    "\n",
    "\n",
    "To Mask:\n",
    "\n",
    "Nama_Nasabah\n",
    "No_Rekening\n",
    "full_name\n",
    "no_telepon\n",
    "approver_name = beda dengan Nama_Nasabah\n",
    "user_login_name = beda \n",
    "Log_Name\n",
    "Nama_Supervisor\n",
    "Nama_TL\n",
    "Nama_Wakabag\n",
    "Remark\n",
    "\n",
    "Kolom Detail:\n",
    "\n",
    "Kode Cabang          : 0307 4 digits\n",
    "No Kartu             : 5221843130736932 16 digits\n",
    "No Rekening          : 030701098507501 15 digits\n",
    "Nama                 : SITI SHOLEHA\n",
    "No ID                : 3316022012770004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random \n",
    "\n",
    "fake = Faker('id_ID')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "def mask_data(df):\n",
    "    df['Nama_Nasabah'] = df['Nama_Nasabah'].apply(lambda x: fake.name())\n",
    "    df['No_Rekening'] = df['No_Rekening'].apply(lambda x: fake.bban())\n",
    "    df['full_name'] = df['full_name'].apply(lambda x: fake.name())\n",
    "    df['no_telepon'] = df['no_telepon'].apply(lambda x: fake.phone_number())\n",
    "    df['approver_name'] = df['approver_name'].apply(lambda x: fake.name())\n",
    "    df['user_login_name'] = df['user_login_name'].apply(lambda x: fake.user_name())\n",
    "    df['Log_Name'] = df['Log_Name'].apply(lambda x: fake.user_name())\n",
    "    df['Nama_Supervisor'] = df['Nama_Supervisor'].apply(lambda x: fake.name())\n",
    "    df['Nama_TL'] = df['Nama_TL'].apply(lambda x: fake.name())\n",
    "    df['Nama_Wakabag'] = df['Nama_Wakabag'].apply(lambda x: fake.name())\n",
    "    return df\n",
    "\n",
    "\n",
    "df_masked = mask_data(df)\n",
    "\n",
    "def generate_nik():\n",
    "    return f'{fake.random_number(digits=16)}'\n",
    "\n",
    "def mask_detail(detail):\n",
    "    if isinstance(detail, float) and pd.isna(detail):\n",
    "        return detail\n",
    "    lines = str(detail).split('\\n')\n",
    "    masked_lines = []\n",
    "    for line in lines:\n",
    "        if 'Kode Cabang' in line:\n",
    "            line = f'Kode Cabang          : {fake.random_number(digits=4)}'\n",
    "        elif 'No Kartu' in line:\n",
    "            line = f'No Kartu             : {fake.credit_card_number()}'\n",
    "        elif 'No Rekening' in line:\n",
    "            line = f'No Rekening          : {fake.bban()}'\n",
    "        elif 'Nama' in line:\n",
    "            line = f'Nama                 : {fake.name()}'\n",
    "        elif 'No ID' in line:\n",
    "            line = f'No ID                : {generate_nik()}'\n",
    "        masked_lines.append(line)\n",
    "    return '\\n'.join(masked_lines)\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "\n",
    "\n",
    "# Ensure 'Details' column is treated as string and apply mask_detail function\n",
    "df_masked['Details'] = df_masked['Details'].astype(str).apply(mask_detail)\n",
    "\n",
    "\n",
    "\n",
    "# gateway_options = [\n",
    "#     'Email', 'Phone', 'Instagram', 'Walk-In', 'MMS', \n",
    "#     'Twitter', 'Facebook', 'BRImo', 'BRILink', 'Sabrina', 'Ceria'\n",
    "# ]\n",
    "\n",
    "#UAT\n",
    "gateway_options = [\n",
    "    'Email', 'Phone', 'Web', 'Facebook', 'Twitter'\n",
    "]\n",
    "# Assign random values to the 'gateway' column\n",
    "df_masked['gateway'] = df_masked['gateway'].apply(lambda x: random.choice(gateway_options))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_masked = df_masked.iloc[:10]\n",
    "df_masked\n",
    "output_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101_masking.csv\"\n",
    "df_masked.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"bricare_uat_20230101_20230101_masking.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['status']='New'\n",
    "df.to_csv(r\"bricare_uat_20230101_20230101_masking_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in SIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\SIT\\dummy_data_case1100000.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "# df['User*']='00GMR0000000dCf2AI'\n",
    "df['User*']='00GMR0000000dEH2AY'\n",
    "df['Status']='Escalated'\n",
    "df['Status'] = df['Status'].replace('New', 'Escalated')\n",
    "df['Case Origin']='Email'\n",
    "df['Case Type']='a1BMR00000010h72AA'\n",
    "df['Account']='001MR000004C7l2YAC'\n",
    "\n",
    "df.to_csv('dummy_data_uat60k.csv', index=False)\n",
    "\n",
    "# df['Merchant ID']\n",
    "# df['TID']\n",
    "# df['User*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To create data to test in UAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "\n",
    "df=pd.read_csv(path) \n",
    "df=df.iloc[:60000]\n",
    "df.to_csv('dummy_data_case_uat60k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\CHM\\CHM\\Part2\\bricare_get_cleansing_77_kolom(1).csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "\n",
    "output_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\bricare_get_cleansing_77_kolom_fixed_return.csv\"\n",
    "df=df.iloc[:1]\n",
    "df['status']='Return'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# output_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature ID        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "ref_fiturid_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\ref_fiturid.csv\"\n",
    "id_calltype_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_calltype_uat.csv\"\n",
    "\n",
    "ref_fiturid = pd.read_csv(ref_fiturid_path)\n",
    "id_calltype = pd.read_csv(id_calltype_path)\n",
    "\n",
    "# Convert calltype and NAME columns to string type\n",
    "ref_fiturid['calltype'] = ref_fiturid['calltype'].astype(str)\n",
    "id_calltype['NAME'] = id_calltype['NAME'].astype(str)\n",
    "\n",
    "# Merge the dataframes to add the Call_Type_ID column based on the calltype column\n",
    "ref_fiturid = ref_fiturid.merge(id_calltype[['NAME', 'ID']], left_on='calltype', right_on='NAME', how='left')\n",
    "\n",
    "# Rename the ID column to Call_Type_ID\n",
    "ref_fiturid.rename(columns={'ID': 'Call_Type_ID'}, inplace=True)\n",
    "\n",
    "# Drop the unnecessary NAME column from the merge\n",
    "ref_fiturid.drop(columns=['NAME'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\Call Type\\updated_fiturid_uat.csv\"\n",
    "# ref_fiturid=ref_fiturid.iloc[62:63]\n",
    "ref_fiturid.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Updated CSV file saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see the characteristics of all files and save it as excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_32596\\2866064037.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_32596\\2866064037.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_32596\\2866064037.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_32596\\2866064037.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_32596\\2866064037.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to find the longest length of values in each column\n",
    "def find_longest_lengths_per_column(df):\n",
    "    lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    max_lengths = lengths.max()\n",
    "    return max_lengths\n",
    "\n",
    "# Set the folder path\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\"\n",
    "\n",
    "# Get all Excel files in the folder\n",
    "file_names = [file for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "# Load the files dynamically\n",
    "dfs = []\n",
    "rows_count = []\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    dfs.append((file_name, df))\n",
    "    rows_count.append((file_name, len(df)))\n",
    "\n",
    "# Calculate the longest length for each column in each file\n",
    "longest_lengths = {}\n",
    "for file_name, df in dfs:\n",
    "    longest_lengths[file_name] = find_longest_lengths_per_column(df)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "results = pd.DataFrame(longest_lengths).transpose()\n",
    "\n",
    "# Create DataFrame for rows count\n",
    "rows_count_df = pd.DataFrame(rows_count, columns=[\"File Name\", \"Total Rows\"])\n",
    "\n",
    "# Print the results\n",
    "results = results.transpose()\n",
    "results.to_excel('allcolumns_artikel2.xlsx')\n",
    "\n",
    "# Save rows count to a new Excel file\n",
    "rows_count_df.to_excel('allcolumns_artikel_rows_count.xlsx', index=False)\n",
    "\n",
    "# Find the longest column and its length for each file\n",
    "longest_columns_info = []\n",
    "for file_name, df in dfs:\n",
    "    lengths = find_longest_lengths_per_column(df)\n",
    "    longest_col = lengths.idxmax()\n",
    "    longest_length = lengths.max()\n",
    "    longest_columns_info.append({\"File\": file_name, \"Column\": longest_col, \"Length\": longest_length})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "longest_columns_df = pd.DataFrame(longest_columns_info)\n",
    "longest_columns_df.to_excel('allcolumns_artikel_col.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_32596\\2514165187.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to find the longest length of values in each column\n",
    "def find_longest_lengths_per_column(df):\n",
    "    lengths = df.applymap(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    max_lengths = lengths.max()\n",
    "    return max_lengths\n",
    "\n",
    "# Set the folder path\n",
    "# folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_wise\\sample_wise\"\n",
    "\n",
    "folder_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\"\n",
    "\n",
    "# List of file names\n",
    "file_names = [\n",
    "    \"sample_wi_100.xlsx\",\n",
    "    \"sample_wise_produk_58.xlsx\",\n",
    "    \"sample_wise_program_41.xlsx\",\n",
    "    \"sample_wise_promo_90.xlsx\"\n",
    "]\n",
    "\n",
    "# Load the files dynamically\n",
    "dfs = []\n",
    "rows_count = []\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    dfs.append((file_name, df))\n",
    "    rows_count.append((file_name, len(df)))\n",
    "\n",
    "# Calculate the longest length for each column in each file\n",
    "longest_lengths = {}\n",
    "for file_name, df in dfs:\n",
    "    longest_lengths[file_name] = find_longest_lengths_per_column(df)\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "results = pd.DataFrame(longest_lengths).transpose()\n",
    "\n",
    "# Create DataFrame for rows count\n",
    "rows_count_df = pd.DataFrame(rows_count, columns=[\"File Name\", \"Total Rows\"])\n",
    "\n",
    "# Print the results\n",
    "results = results.transpose()\n",
    "results.to_excel('allcolumns_artikel2.xlsx')\n",
    "\n",
    "# Save rows count to a new Excel file\n",
    "rows_count_df.to_excel('allcolumns_artikel_rows_count.xlsx', index=False)\n",
    "\n",
    "# Optional: Find the longest column and its length for each file\n",
    "# longest_columns_info = []\n",
    "# for file_name, lengths in results.iterrows():\n",
    "#     longest_col = lengths.idxmax()\n",
    "#     longest_length = lengths.max()\n",
    "#     longest_columns_info.append({\"File\": file_name, \"Column\": longest_col, \"Length\": longest_length})\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "# longest_columns_df = pd.DataFrame(longest_columns_info)\n",
    "# longest_columns_df.to_excel('longest_columns_info.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_36520\\4243200304.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[text_columns] = df[text_columns].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the Excel file\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\\sample_wise_wi_900.xlsx\"\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Replace \\n and \\t with actual new lines and tabs\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        # Remove trailing and leading spaces\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns = df.select_dtypes(include=[object]).columns\n",
    "df[text_columns] = df[text_columns].applymap(clean_text)\n",
    "\n",
    "# Convert the date format\n",
    "# df['modified_time'] = pd.to_datetime(df['modified_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"WI-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df))]\n",
    "\n",
    "# Check the 'calltype' column and replace \"-\" or empty values with \"No Call Type\"\n",
    "\n",
    "df['Article_Type']='Working Instruction'\n",
    "df['record_type']='Working Instructions'\n",
    "\n",
    "# Add the new 'title' column\n",
    "df.insert(0, 'title', df['calltype'] + ' - ' + df['kategori'] + ' - ' + df['sub_kategori'])\n",
    "df['calltype'] = df['calltype'].apply(lambda x: 'No Call Type' if pd.isnull(x) or x.strip() == '-' else x)\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df.to_csv('cleaned_wise_wi_900.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Biaya' at row 39 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel\\json_table_39_Biaya.xlsx'.\n",
      "Column 'Biaya' at row 59 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel\\json_table_59_Biaya.xlsx'.\n",
      "Column 'Bunga' at row 59 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel\\json_table_59_Bunga.xlsx'.\n",
      "Column 'Bunga' at row 67 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel\\json_table_67_Bunga.xlsx'.\n",
      "Column 'Limit' at row 39 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel\\json_table_39_Limit.xlsx'.\n",
      "Column 'Limit' at row 59 contains JSON text. Saved to 'D:\\dataquality2\\product_artikel\\json_table_59_Limit.xlsx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_36520\\3705421417.py:42: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from prettytable import PrettyTable\n",
    "import openpyxl\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.drawing.image import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the new Excel file\n",
    "file_path_new = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\\sample_wise_produk_75.xlsx\"\n",
    "df_program = pd.read_excel(file_path_new)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df_program = df_program.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Add the new 'title' column\n",
    "df_program.insert(0, 'title', df_program['jenis_produk'] + ' - ' + df_program['produk'] + ' - ' + df_program['varian_produk'])\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"PD-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df_program['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_program))]\n",
    "df_program['calltype'] = 'No Call Type'\n",
    "df_program['Article_Type'] = 'Product'\n",
    "df_program['record_type'] = 'Product'\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns_program = df_program.select_dtypes(include=[object]).columns\n",
    "df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
    "\n",
    "# Function to save DataFrame to Excel with auto-fit columns and borders\n",
    "def save_df_to_excel(df, file_path):\n",
    "    writer = pd.ExcelWriter(file_path, engine='openpyxl')\n",
    "    df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "    # Auto-fit columns\n",
    "    for col in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column = col[0].column_letter\n",
    "        for cell in col:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(cell.value)\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = (max_length + 2)\n",
    "        worksheet.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "    # Add borders to cells\n",
    "    thin_border = openpyxl.styles.Border(\n",
    "        left=openpyxl.styles.Side(style='thin'),\n",
    "        right=openpyxl.styles.Side(style='thin'),\n",
    "        top=openpyxl.styles.Side(style='thin'),\n",
    "        bottom=openpyxl.styles.Side(style='thin')\n",
    "    )\n",
    "\n",
    "    for row in worksheet.iter_rows():\n",
    "        for cell in row:\n",
    "            cell.border = thin_border\n",
    "\n",
    "    workbook.save(file_path)\n",
    "\n",
    "# Function to generate a picture of the DataFrame\n",
    "def generate_df_picture(df, file_path):\n",
    "    fig, ax = plt.subplots(figsize=(12, len(df) // 2))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.2)\n",
    "    plt.savefig(file_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Function to check if a string is valid JSON and convert it to readable format\n",
    "def process_json(text, row_idx, col_name, save_path, generate_pictures):\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, list) and 'table_data' in data[0]:\n",
    "            # Save the JSON data to a separate Excel file\n",
    "            table_data = data[0]['table_data']\n",
    "            rows = []\n",
    "\n",
    "            for row in table_data:\n",
    "                row_values = [item['value'] for item in row]\n",
    "                rows.append(row_values)\n",
    "\n",
    "            json_df = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "            json_file_path = os.path.join(save_path, f'json_table_{row_idx}_{col_name}.xlsx')\n",
    "            save_df_to_excel(json_df, json_file_path)\n",
    "\n",
    "            if generate_pictures:\n",
    "                # Generate picture of the table\n",
    "                img_file_path = json_file_path.replace('.xlsx', '.png')\n",
    "                generate_df_picture(json_df, img_file_path)\n",
    "\n",
    "            table = PrettyTable()\n",
    "            table.align = \"l\"  # Align all text to the left\n",
    "\n",
    "            headers = [item['value'] for item in table_data[0]]\n",
    "            table.field_names = headers\n",
    "            \n",
    "            # Calculate the maximum width for each column\n",
    "            max_widths = {header: len(header) for header in headers}\n",
    "            for row in table_data[1:]:\n",
    "                for i, item in enumerate(row):\n",
    "                    value_length = len(str(item['value']).strip())\n",
    "                    if value_length > max_widths[headers[i]]:\n",
    "                        max_widths[headers[i]] = value_length\n",
    "            \n",
    "            for row in table_data[1:]:\n",
    "                row_values = [item['value'].strip() if isinstance(item['value'], str) else item['value'] for item in row]\n",
    "                table.add_row(row_values)\n",
    "            \n",
    "            # Adjust column widths\n",
    "            for field in table.field_names:\n",
    "                table.max_width[field] = max_widths[field]\n",
    "            \n",
    "            return str(table)\n",
    "        return text\n",
    "    except (ValueError, TypeError, KeyError):\n",
    "        return text\n",
    "\n",
    "# Process and clean JSON-like text in all columns, saving JSON data to separate files\n",
    "def process_all_json_texts(df, save_path, generate_pictures):\n",
    "    json_columns_rows = []\n",
    "    text_columns_program = df.select_dtypes(include=[object]).columns\n",
    "    for column in text_columns_program:\n",
    "        for idx, value in enumerate(df[column]):\n",
    "            if isinstance(value, str) and value.strip().startswith('[') and value.strip().endswith(']'):\n",
    "                df.at[idx, column] = process_json(value, idx, column, save_path, generate_pictures)\n",
    "                json_columns_rows.append((column, idx))\n",
    "    return json_columns_rows\n",
    "\n",
    "# Define the path where to save JSON files and whether to generate pictures\n",
    "save_path = r\"D:\\dataquality2\\product_artikel\"\n",
    "generate_pictures = False\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Process all JSON texts in the DataFrame\n",
    "json_columns_rows = process_all_json_texts(df_program, save_path, generate_pictures)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file_path = 'cleaned_product_75.csv'\n",
    "df_program.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display lines 39 to 42 for user reference\n",
    "# lines_39_to_42 = df_program.iloc[38:42]\n",
    "# print(lines_39_to_42['Biaya'].values)\n",
    "\n",
    "# Add comments indicating columns and rows with JSON texts\n",
    "for column, row in json_columns_rows:\n",
    "    print(f\"Column '{column}' at row {row} contains JSON text. Saved to '{os.path.join(save_path, f'json_table_{row}_{column}.xlsx')}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # Load the new Excel file\n",
    "# file_path_new = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\\sample_wise_produk_75.xlsx\"\n",
    "# df_new = pd.read_excel(file_path_new)\n",
    "\n",
    "# # Drop the 'Unnamed: 0' column if it exists\n",
    "# df_new = df_new.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# # Function to clean text by removing HTML tags and unwanted characters\n",
    "# def clean_text(text):\n",
    "#     if isinstance(text, str):\n",
    "#         # Remove HTML tags\n",
    "#         clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "#         # Replace \\n and \\t with actual new lines and tabs\n",
    "#         clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "#         # Remove trailing and leading spaces\n",
    "#         clean_text = clean_text.strip()\n",
    "#         return clean_text\n",
    "#     return text\n",
    "\n",
    "# # Apply the clean_text function to all text columns\n",
    "# text_columns_new = df_new.select_dtypes(include=[object]).columns\n",
    "# df_new[text_columns_new] = df_new[text_columns_new].applymap(clean_text)\n",
    "\n",
    "# # Convert the date format\n",
    "# # df_new['modified_time'] = pd.to_datetime(df_new['modified_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# # Add the new 'title' column\n",
    "# df_new.insert(0, 'title', df_new['jenis_produk'] + ' - ' + df_new['produk'] + ' - ' + df_new['varian_produk'] + ' - ' + df_new['nama_knowledge'])\n",
    "\n",
    "# # Function to generate sequential URL names\n",
    "# def generate_url_name(index):\n",
    "#     return f\"PD-{index:07d}\"\n",
    "\n",
    "# # Apply the function to generate URL names based on the row index + 1\n",
    "# df_new['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_new))]\n",
    "\n",
    "# # Save the cleaned DataFrame to a CSV file\n",
    "# df_new['calltype'] = 'No Call Type'\n",
    "# df_new['Article_Type']='Product'\n",
    "# df_new['record_type']='Product'\n",
    "# # df_new=df_new.iloc[:1]\n",
    "# df_new.to_csv('cleaned_wise_produk_75.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_36520\\2102008483.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the new Excel file\n",
    "file_path_program = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\\sample_wise_program_50.xlsx\"\n",
    "df_program = pd.read_excel(file_path_program)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df_program = df_program.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Replace \\n and \\t with actual new lines and tabs\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        # Remove trailing and leading spaces\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns_program = df_program.select_dtypes(include=[object]).columns\n",
    "df_program[text_columns_program] = df_program[text_columns_program].applymap(clean_text)\n",
    "\n",
    "# Convert the date format for 'modified_time', 'start', and 'end' columns\n",
    "# df_program['modified_time'] = pd.to_datetime(df_program['modified_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_program['start'] = pd.to_datetime(df_program['start'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "df_program['end'] = pd.to_datetime(df_program['end'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Add the new 'title' column\n",
    "df_program.insert(0, 'title', df_program['nama_program'])\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"PG-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df_program['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_program))]\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df_program['calltype'] = 'No Call Type'\n",
    "df_program['Article_Type']='Program'\n",
    "df_program['record_type']='Program'\n",
    "# df_program=df_program.iloc[:1]\n",
    "df_program.to_csv('cleaned_wise_program_50.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_36520\\698870232.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_promo[text_columns_promo] = df_promo[text_columns_promo].applymap(clean_text)\n",
      "c:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the new Excel file\n",
    "file_path_promo = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\data-wi-produk-promo-program\\samples_15_07_24\\sample_wise_promo_107.xlsx\"\n",
    "df_promo = pd.read_excel(file_path_promo)\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "df_promo = df_promo.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Function to clean text by removing HTML tags and unwanted characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Replace \\n and \\t with actual new lines and tabs\n",
    "        clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "        # Remove trailing and leading spaces\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to all text columns\n",
    "text_columns_promo = df_promo.select_dtypes(include=[object]).columns\n",
    "df_promo[text_columns_promo] = df_promo[text_columns_promo].applymap(clean_text)\n",
    "\n",
    "# Convert the date format for 'modified_time', 'start', and 'end' columns\n",
    "# df_promo['modified_time'] = pd.to_datetime(df_promo['modified_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_promo['start'] = pd.to_datetime(df_promo['start'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_promo['end'] = pd.to_datetime(df_promo['end'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Add the new 'title' column\n",
    "df_promo.insert(0, 'title', df_promo['kategori'])\n",
    "\n",
    "# Function to generate sequential URL names\n",
    "def generate_url_name(index):\n",
    "    return f\"PM-{index:07d}\"\n",
    "\n",
    "# Apply the function to generate URL names based on the row index + 1\n",
    "df_promo['URL_Name'] = [generate_url_name(i + 1) for i in range(len(df_promo))]\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df_promo['calltype'] = 'No Call Type'\n",
    "df_promo['Article_Type']='Promotion'\n",
    "df_promo['record_type']='Promotion'\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "# df_promo=df_promo.iloc[:1]\n",
    "df_promo.to_csv('cleaned_wise_promo_90.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>Call Type Info</th>\n",
       "      <th>Type</th>\n",
       "      <th>User</th>\n",
       "      <th>Title</th>\n",
       "      <th>Detail Bricare</th>\n",
       "      <th>Deskripsi</th>\n",
       "      <th>Empati</th>\n",
       "      <th>Konfirmasi</th>\n",
       "      <th>...</th>\n",
       "      <th>Verifikasi</th>\n",
       "      <th>Gali Informasi</th>\n",
       "      <th>Pembuatan Laporan</th>\n",
       "      <th>Konfirmasi Ulang</th>\n",
       "      <th>Percepatan Komplain</th>\n",
       "      <th>Solusi, Informasi</th>\n",
       "      <th>Edukasi &amp; Cross Selling</th>\n",
       "      <th>Closing</th>\n",
       "      <th>Article Type</th>\n",
       "      <th>URL_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>Nasabah Menanyakan Informasi Pengajuan Terkait...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1000 - Nasabah Menanyakan Informasi Pengajuan ...</td>\n",
       "      <td>nomor ponsel nasabah yang bisa dihubungi\\nID C...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...</td>\n",
       "      <td>Jika Nasabah menanyakan Cara &amp; Syarat Pengajua...</td>\n",
       "      <td>...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nVerifikas...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nGali Info...</td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nBuat lapo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1\\tCara &amp; Syarat pengajuan Aplikasi\\nInformasi...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1002</td>\n",
       "      <td>Nasabah Menyakan Terkait Promo dan Program CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1002 - Nasabah Menyakan Terkait Promo dan Prog...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\nJika...</td>\n",
       "      <td>Konfirmasi :\\nJika Nasabah menanyakan promo ce...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi\\nTidak dilakukan Verifikasi\\n</td>\n",
       "      <td>Gali Informasi\\t\\n\\tÂ  i.Â Â Â Â Â Â Â Â Â  Jenis promo/...</td>\n",
       "      <td>Pembuatan Laporan\\t\\n\\ti.Â Â Â Â  Nomor laporan ti...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\t\\n\\t  i.              \\...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1003</td>\n",
       "      <td>Nasabah Mengajukan Pelunasan Awal Cicilan CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1003 - Nasabah Mengajukan Pelunasan Awal Cicil...</td>\n",
       "      <td></td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati sesuai kondisi (pilih salah satu)\\n\"Bap...</td>\n",
       "      <td>Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...</td>\n",
       "      <td>Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi\\n1.\\tCicilan telah berjalan...</td>\n",
       "      <td></td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1005</td>\n",
       "      <td>Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1005 - Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>Nasabah mengajukan pemblokiran Sementara Akun ...</td>\n",
       "      <td>Nasabah menghubungi Contact BRI / datang ke Un...</td>\n",
       "      <td>Empati\\nJika nasabah infokan ingin melakukan p...</td>\n",
       "      <td>Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...</td>\n",
       "      <td>...</td>\n",
       "      <td>Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...</td>\n",
       "      <td>Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...</td>\n",
       "      <td>Pembuatan Laporan\\nNasabah mengajukan pembloki...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Informasi &amp; Solusi \\t\\n1.\\tAgent Konfirmasi ke...</td>\n",
       "      <td>Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...</td>\n",
       "      <td>Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...</td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1005-Nasabah-Mengajukan-Pemblokiran-CERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1008</td>\n",
       "      <td>Nasabah Mengajukan Pengaktifan Akun Ceria Terb...</td>\n",
       "      <td>CERIA</td>\n",
       "      <td>Agent Contact Center</td>\n",
       "      <td>1008 - Nasabah Mengajukan Pengaktifan Akun Cer...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td>1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Working Instruction</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 Call Type                                     Call Type Info  \\\n",
       "0            1      1000  Nasabah Menanyakan Informasi Pengajuan Terkait...   \n",
       "1            2      1002   Nasabah Menyakan Terkait Promo dan Program CERIA   \n",
       "2            3      1003    Nasabah Mengajukan Pelunasan Awal Cicilan CERIA   \n",
       "3            4      1005               Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4            5      1008  Nasabah Mengajukan Pengaktifan Akun Ceria Terb...   \n",
       "..         ...       ...                                                ...   \n",
       "437                                                                           \n",
       "438                                                                           \n",
       "439                                                                           \n",
       "440                                                                           \n",
       "441                                                                           \n",
       "\n",
       "      Type                  User  \\\n",
       "0    CERIA  Agent Contact Center   \n",
       "1    CERIA  Agent Contact Center   \n",
       "2    CERIA  Agent Contact Center   \n",
       "3    CERIA  Agent Contact Center   \n",
       "4    CERIA  Agent Contact Center   \n",
       "..     ...                   ...   \n",
       "437                                \n",
       "438                                \n",
       "439                                \n",
       "440                                \n",
       "441                                \n",
       "\n",
       "                                                 Title  \\\n",
       "0    1000 - Nasabah Menanyakan Informasi Pengajuan ...   \n",
       "1    1002 - Nasabah Menyakan Terkait Promo dan Prog...   \n",
       "2    1003 - Nasabah Mengajukan Pelunasan Awal Cicil...   \n",
       "3          1005 - Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4    1008 - Nasabah Mengajukan Pengaktifan Akun Cer...   \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Detail Bricare  \\\n",
       "0    nomor ponsel nasabah yang bisa dihubungi\\nID C...   \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Nasabah mengajukan pemblokiran Sementara Akun ...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                             Deskripsi  \\\n",
       "0    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "1    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "2    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "3    Nasabah menghubungi Contact BRI / datang ke Un...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                                Empati  \\\n",
       "0    Empati sesuai kondisi (pilih salah satu)\\t\\t\\t...   \n",
       "1    Empati sesuai kondisi (pilih salah satu)\\nJika...   \n",
       "2    Empati sesuai kondisi (pilih salah satu)\\n\"Bap...   \n",
       "3    Empati\\nJika nasabah infokan ingin melakukan p...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                            Konfirmasi  ...  \\\n",
       "0    Jika Nasabah menanyakan Cara & Syarat Pengajua...  ...   \n",
       "1    Konfirmasi :\\nJika Nasabah menanyakan promo ce...  ...   \n",
       "2    Konfirmasi :\\n\"Untuk Permintaan Pelunasan awal...  ...   \n",
       "3    Konfirmasi: \\n\"Untuk pemblokiran bersifat seme...  ...   \n",
       "4                                                       ...   \n",
       "..                                                 ...  ...   \n",
       "437                                                     ...   \n",
       "438                                                     ...   \n",
       "439                                                     ...   \n",
       "440                                                     ...   \n",
       "441                                                     ...   \n",
       "\n",
       "                                            Verifikasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nVerifikas...   \n",
       "1             Verifikasi\\nTidak dilakukan Verifikasi\\n   \n",
       "2    Verifikasi ( Buka CLOS/ Finnachel )\\t\\nUntuk v...   \n",
       "3    Verifikasi ( Buka WBS/ CLOS )\\t\\nUntuk verifik...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                        Gali Informasi  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nGali Info...   \n",
       "1    Gali Informasi\\t\\n\\tÂ  i.Â Â Â Â Â Â Â Â Â  Jenis promo/...   \n",
       "2    Gali Informasi\\t\\n1.\\tTanyakan alasan pelunasa...   \n",
       "3    Gali Informasi\\t\\n1.\\tTanyakan alasan perminta...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                     Pembuatan Laporan Konfirmasi Ulang  \\\n",
       "0    1\\tCara & Syarat pengajuan Aplikasi\\nBuat lapo...                    \n",
       "1    Pembuatan Laporan\\t\\n\\ti.Â Â Â Â  Nomor laporan ti...                    \n",
       "2    Pembuatan Laporan \\t\\n1.\\tBuat Laporan dengan ...                    \n",
       "3    Pembuatan Laporan\\nNasabah mengajukan pembloki...                    \n",
       "4                                                                         \n",
       "..                                                 ...              ...   \n",
       "437                                                                       \n",
       "438                                                                       \n",
       "439                                                                       \n",
       "440                                                                       \n",
       "441                                                                       \n",
       "\n",
       "    Percepatan Komplain                                  Solusi, Informasi  \\\n",
       "0                        1\\tCara & Syarat pengajuan Aplikasi\\nInformasi...   \n",
       "1                        Informasi & Solusi \\t\\t\\n\\t  i.              \\...   \n",
       "2                        Informasi & Solusi\\n1.\\tCicilan telah berjalan...   \n",
       "3                        Informasi & Solusi \\t\\n1.\\tAgent Konfirmasi ke...   \n",
       "4                                                                            \n",
       "..                  ...                                                ...   \n",
       "437                                                                          \n",
       "438                                                                          \n",
       "439                                                                          \n",
       "440                                                                          \n",
       "441                                                                          \n",
       "\n",
       "                               Edukasi & Cross Selling  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3    Kalimat Edukasi\\n\"Kami informasikan Bank BRI t...   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "437                                                      \n",
       "438                                                      \n",
       "439                                                      \n",
       "440                                                      \n",
       "441                                                      \n",
       "\n",
       "                                               Closing         Article Type  \\\n",
       "0    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "1    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "2    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "3    Closing\\t\\t\\n1\\tFeedback\\t\\n\\tApakah informasi...  Working Instruction   \n",
       "4                                                       Working Instruction   \n",
       "..                                                 ...                  ...   \n",
       "437                                                     Working Instruction   \n",
       "438                                                     Working Instruction   \n",
       "439                                                     Working Instruction   \n",
       "440                                                     Working Instruction   \n",
       "441                                                     Working Instruction   \n",
       "\n",
       "                                              URL_name  \n",
       "0    1000-Nasabah-Menanyakan-Informasi-Pengajuan-Te...  \n",
       "1    1002-Nasabah-Menyakan-Terkait-Promo-dan-Progra...  \n",
       "2    1003-Nasabah-Mengajukan-Pelunasan-Awal-Cicilan...  \n",
       "3            1005-Nasabah-Mengajukan-Pemblokiran-CERIA  \n",
       "4    1008-Nasabah-Mengajukan-Pengaktifan-Akun-Ceria...  \n",
       "..                                                 ...  \n",
       "437                                                     \n",
       "438                                                     \n",
       "439                                                     \n",
       "440                                                     \n",
       "441                                                     \n",
       "\n",
       "[375 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare(WI).csv\"\n",
    "df=pd.read_csv(path, encoding='ISO-8859-1')\n",
    "df = df.fillna('')\n",
    "df['Article Type']='Working Instruction'\n",
    "\n",
    "output=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\artikel_all.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Change 'Brilink' to 'BRILink' in 'Type' column\n",
    "df['Type'] = df['Type'].replace('Brilink', 'BRILink')\n",
    "\n",
    "# Remove rows with 'Wholesale' in 'Type' column\n",
    "df = df[df['Type'] != 'Wholesale']\n",
    "\n",
    "# df['Type'].unique()\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('artikel2.csv', index= False)\n",
    "# df.iloc[:1].to_csv(output, index= False)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df= pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060424041601915.csv\")\n",
    "df.to_csv('error.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\knowledge\\BRI - Detail BRICare 2.xlsx\"\n",
    "\n",
    "df = pd.read_excel(path)\n",
    "df=df.iloc[:1]\n",
    "# df['Gali Informasi']\n",
    "\n",
    "df['URL_name'] = df['Call Type'].astype(str) + '-' + df['Call Type Info'].astype(str)\n",
    "\n",
    "import re\n",
    "\n",
    "# Define a function to clean the URL name\n",
    "def clean_url_name(url_name):\n",
    "    # Remove leading and trailing hyphens\n",
    "    url_name = url_name.strip('-')\n",
    "    # Replace invalid characters with hyphens and remove multiple hyphens\n",
    "    url_name = re.sub(r'[^a-zA-Z0-9\\u00C0-\\u017F-]', '-', url_name)\n",
    "    url_name = re.sub(r'-+', '-', url_name)\n",
    "    # Remove leading and trailing hyphens again after replacement\n",
    "    url_name = url_name.strip('-')\n",
    "    return url_name\n",
    "\n",
    "# Apply the cleaning function to the URL_name column\n",
    "df['URL_name'] = df['URL_name'].apply(clean_url_name)\n",
    "\n",
    "df.to_csv('artikel_bullet.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate 1k roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/extract_role.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the uploaded CSV file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/extract_role.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m roles_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the content of the CSV file to understand its structure\u001b[39;00m\n\u001b[0;32m      8\u001b[0m roles_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/extract_role.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '/mnt/data/extract_role.csv'\n",
    "roles_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the content of the CSV file to understand its structure\n",
    "roles_df.head()\n",
    "\n",
    "\n",
    "# Number of additional roles needed to reach a total of 1000\n",
    "num_additional_roles_needed = 1000 - len(expanded_roles_df)\n",
    "\n",
    "# Duplicate the existing roles to meet the required number of roles\n",
    "additional_roles_more = expanded_roles_df.sample(num_additional_roles_needed, replace=True).reset_index(drop=True)\n",
    "\n",
    "# Generate unique names for the additional roles\n",
    "additional_roles_more['NAME'] = additional_roles_more['NAME'] + \"_more_\" + (additional_roles_more.index + 1).astype(str)\n",
    "\n",
    "# Combine the original roles with the additional roles\n",
    "expanded_roles_df_more = pd.concat([expanded_roles_df, additional_roles_more], ignore_index=True)\n",
    "\n",
    "# Ensure we have a total of 1000 roles\n",
    "expanded_roles_df_more = expanded_roles_df_more.head(1000)\n",
    "\n",
    "# Save the expanded roles to a new CSV file\n",
    "output_file_path_more = 'expanded_roles_1000.csv'\n",
    "expanded_roles_df_more.to_csv(output_file_path_more, index=False)\n",
    "\n",
    "output_file_path_more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Extraction Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check data before 2023 = 27 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns in DataFrame: {'Tgl_Assigned'}\n",
      "Extra columns in DataFrame: {'TglAssigned', 'cifno', 'Details'}\n"
     ]
    }
   ],
   "source": [
    "column_list = [\n",
    "    \"Ticket_ID\",  \n",
    "    \"Call_Type_ID\",  \n",
    "    \"Call_Type\", \n",
    "    \"Create_Date\",  \n",
    "    \"gateway\",  \n",
    "    \"Jenis_Laporan\",  \n",
    "    \"Nama_Nasabah\",  \n",
    "    \"No_Rekening\", \n",
    "    \"Nominal\",  \n",
    "    \"status\",  \n",
    "    \"TanggalClosed\", \n",
    "    \"tanggalTransaksi\",  \n",
    "    \"Chanel\",  \n",
    "    \"Fitur\",  \n",
    "    \"Nomor_Kartu\", \n",
    "    \"user_group\",  \n",
    "    \"assgined_to\",  \n",
    "    \"attachment_done\",  \n",
    "    \"email\",  \n",
    "    \"full_name\",  \n",
    "    \"no_telepon\",  \n",
    "    \"approver_login\",  \n",
    "    \"approver_name\",  \n",
    "    \"SLAResolution\",  \n",
    "    \"submitter_login_id\",  \n",
    "    \"submitter_user_group\", \n",
    "    \"user_login_name\"  \n",
    "]\n",
    "\n",
    "column_names = [\n",
    "    \"Ticket_ID\", \"Call_Type_ID\", \"Call_Type\", \"Create_Date\", \"gateway\", \"Jenis_Laporan\", \"Nama_Nasabah\", \n",
    "    \"No_Rekening\", \"Nominal\", \"status\", \"TanggalClosed\", \"tanggalTransaksi\", \"Chanel\", \"Fitur\", \"Nomor_Kartu\", \n",
    "    \"user_group\", \"assgined_to\", \"attachment_done\", \"email\", \"full_name\", \"no_telepon\", \"approver_login\", \n",
    "    \"approver_name\", \"SLAResolution\", \"submitter_login_id\", \"submitter_user_group\", \"user_login_name\", \n",
    "    \"Jenis_Produk\", \"Last_Modified_By\", \"Merchant_ID\", \"Modified_Date\", \"NOTAS\", \"Produk\", \"SLA_Status\", \"TID\", \n",
    "    \"tanggalAttachmentDone\", \"Tgl_Assigned\", \"Tgl_Eskalasi\", \"AnalisaSkils\", \"Attachment_\", \"Bank_BRI\", \n",
    "    \"Biaya_Admin\", \"Suku_Bunga\", \"Bunga\", \"Butuh_Attachment\", \"Cicilan\", \"Hasil_Kunjungan\", \"Log_Name\", \n",
    "    \"MMS_Ticket_Id\", \"Mass_Ticket_Upload_Flag\", \"Nama_Supervisor\", \"Nama_TL\", \"Nama_Wakabag\", \"Nasabah_Prioritas\", \n",
    "    \"Notify_By\", \"Organization\", \"Output_Settlement\", \"phone_survey\", \"Return_Ticket\", \"Settlement_By\", \n",
    "    \"Settlement_ID\", \"Settlement\", \"Site_User\", \"Status_Return\", \"Status_Transaksi\", \"Submitter_Region\", \n",
    "    \"Submitter_SiteGroup\", \"Submitter_User_group_ID\", \"Tanggal_Settlement\", \"Tgl_Foward\", \"Tgl_In_Progress\", \n",
    "    \"Tgl_Returned\", \"Ticket_Referensi\", \"Tiket_Urgency\", \"Tipe_Remark\", \"UniqueID\", \"users\", \"Usergroup_ID\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_27_kolom.csv\"\n",
    "path2=r\"D:\\dataquality2\\new as per 5 June\\bricare_20221213_20240604_79_kolom.csv\"\n",
    "# df= pd.read_csv(path, delimiter=';')\n",
    "df= pd.read_csv(path2, delimiter=';')\n",
    "\n",
    "\n",
    "# column_list_set = set(column_list)\n",
    "column_list_set = set(column_names)\n",
    "df_columns_set = set(df.columns)\n",
    "\n",
    "missing_columns = column_list_set - df_columns_set\n",
    "extra_columns = df_columns_set - column_list_set\n",
    "\n",
    "if not missing_columns and not extra_columns:\n",
    "    print(\"The column names match.\")\n",
    "else:\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "    if extra_columns:\n",
    "        print(f\"Extra columns in DataFrame: {extra_columns}\")\n",
    "\n",
    "################################################\n",
    "# column_list_set2 = set(column_names)\n",
    "# df_columns_set2 = set(df2.columns)\n",
    "\n",
    "# missing_columns2 = column_list_set2 - df_columns_set2\n",
    "# extra_columns2 = df_columns_set2 - column_list_set2\n",
    "\n",
    "# if not missing_columns and not extra_columns:\n",
    "#     print(\"The column names match.\")\n",
    "# else:\n",
    "#     if missing_columns2:\n",
    "#         print(f\"Missing columns in DataFrame: {missing_columns2}\")\n",
    "#     if extra_columns2:\n",
    "#         print(f\"Extra columns in DataFrame: {extra_columns2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data dengan 27 kolom:\n",
    "- jumlah kolom sudah ok\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- attachment\n",
    "- penambahan nama file diakhir \"27\"\n",
    "\n",
    "\n",
    "\n",
    "Data dengan 79 kolom:\n",
    "- picklist Walk In harus diganti ke Walk-In\n",
    "- penambahan nama file diakhir \"79\"\n",
    "\n",
    "\n",
    "\n",
    "Data Zendesk:\n",
    "- harus hilangkan double quotes \"\"\n",
    "- delimiternya ;\n",
    "- nama file zendesk\n",
    "\n",
    "\n",
    "Data Omni:\n",
    "- SMS: harus format csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Unable to create/update fields: LastModifiedDa...\n",
       "1    Unable to create/update fields: LastModifiedDa...\n",
       "2    Unable to create/update fields: LastModifiedDa...\n",
       "3    Unable to create/update fields: LastModifiedDa...\n",
       "4    Unable to create/update fields: LastModifiedDa...\n",
       "5    Unable to create/update fields: LastModifiedDa...\n",
       "6    Unable to create/update fields: LastModifiedDa...\n",
       "7    Unable to create/update fields: LastModifiedDa...\n",
       "8    Unable to create/update fields: LastModifiedDa...\n",
       "9    Unable to create/update fields: LastModifiedDa...\n",
       "Name: ERROR, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error060524083553095.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df['ERROR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zendesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
      "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
      "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
      "\n",
      "     Requester name                                     Ticket subject  \\\n",
      "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
      "1  Masruroh Sodikin                                                  P   \n",
      "\n",
      "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
      "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
      "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
      "\n",
      "  Ticket solved - Timestamp                 Tickets  \n",
      "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
      "1       2024-01-01T00:46:27  1.00000000000000000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket ID</th>\n",
       "      <th>Ticket channel</th>\n",
       "      <th>Assignee ID</th>\n",
       "      <th>Assignee name</th>\n",
       "      <th>Requester ID</th>\n",
       "      <th>Requester name</th>\n",
       "      <th>Ticket subject</th>\n",
       "      <th>Requester created - Timestamp</th>\n",
       "      <th>Ticket created - Timestamp</th>\n",
       "      <th>Ticket solved - Timestamp</th>\n",
       "      <th>Tickets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3435272</td>\n",
       "      <td>Any channel</td>\n",
       "      <td>405257525413</td>\n",
       "      <td>Agent Sosmed 4</td>\n",
       "      <td>27064719563033</td>\n",
       "      <td>vianovia94</td>\n",
       "      <td>[IGC] Minimal opo boloo... ......#relsvideo #r...</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:03:30</td>\n",
       "      <td>2024-01-01T00:30:56</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3435273</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>405257335633</td>\n",
       "      <td>Agent Sosmed 3</td>\n",
       "      <td>27064768687769</td>\n",
       "      <td>Masruroh Sodikin</td>\n",
       "      <td>P</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:04:15</td>\n",
       "      <td>2024-01-01T00:46:27</td>\n",
       "      <td>1.00000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticket ID Ticket channel   Assignee ID   Assignee name    Requester ID  \\\n",
       "0   3435272    Any channel  405257525413  Agent Sosmed 4  27064719563033   \n",
       "1   3435273       Facebook  405257335633  Agent Sosmed 3  27064768687769   \n",
       "\n",
       "     Requester name                                     Ticket subject  \\\n",
       "0        vianovia94  [IGC] Minimal opo boloo... ......#relsvideo #r...   \n",
       "1  Masruroh Sodikin                                                  P   \n",
       "\n",
       "  Requester created - Timestamp Ticket created - Timestamp  \\\n",
       "0           2024-01-01T00:03:30        2024-01-01T00:03:30   \n",
       "1           2024-01-01T00:04:15        2024-01-01T00:04:15   \n",
       "\n",
       "  Ticket solved - Timestamp                 Tickets  \n",
       "0       2024-01-01T00:30:56  1.00000000000000000000  \n",
       "1       2024-01-01T00:46:27  1.00000000000000000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\Data Zendesk 1-15 Januari 2024 - sampel.csv\"\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "# Read the CSV file with proper handling of quotes and custom separator\n",
    "with open(file_path, 'r', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "    rows = [row for row in reader]\n",
    "\n",
    "# Split the combined columns that have commas within them\n",
    "split_rows = []\n",
    "for row in rows:\n",
    "    split_row = row[0].split(',') + row[1:]  # Split the first column and add the rest as they are\n",
    "    split_rows.append(split_row)\n",
    "\n",
    "# Create DataFrame from the processed rows\n",
    "df = pd.DataFrame(split_rows[1:], columns=split_rows[0])\n",
    "\n",
    "# Remove double quotes from column names\n",
    "df.columns = df.columns.str.replace('\"', '')\n",
    "\n",
    "# Remove double quotes from all data in the DataFrame\n",
    "df = df.replace('\"', '', regex=True)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "print(df.head())\n",
    "df\n",
    "\n",
    "# If you need to save the cleaned DataFrame to a new CSV file\n",
    "# cleaned_file_path = '/mnt/data/Cleaned_Zendesk_Data.csv'\n",
    "# df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# df=pd.read_csv(path)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# df\u001b[39;00m\n\u001b[0;32m     10\u001b[0m path2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaste\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataloader_v60.0.2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata check\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnew as per 5 June\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcase.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 86, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\bricare_20221213_20240604_case_account.csv\"\n",
    "# df=pd.read_csv(path)\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "path2=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\data check\\new as per 5 June\\case.csv\"\n",
    "df=pd.read_csv(path2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy data for Alex7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Account Alex7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for account with quoting (DONE)\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_dummy_data(file_path, file_name, num_rows):\n",
    "    # Define possible values for each column\n",
    "    account_names = ['John Doe', 'Jane Smith', 'Mike Brown', 'Lisa Green', 'Mark Taylor']\n",
    "    account_owners = ['Owner A', 'Owner B', 'Owner C', 'Owner D']\n",
    "    nasabah_types = ['Nasabah', 'Non Nasabah']\n",
    "    account_record_types = ['Personal', 'Non Personal']\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Writing to the CSV file\n",
    "    with open(os.path.join(file_path, file_name), mode='w', newline='') as file:\n",
    "        writer = csv.writer(file, quotechar='\"', quoting=csv.QUOTE_ALL)  # Enforce quoting for all fields\n",
    "        writer.writerow(['Account Name', 'CIF No', 'Account Owner', 'No Telp', 'Email', 'Nasabah Type', 'Account Record Type'])\n",
    "        \n",
    "        for _ in range(num_rows):\n",
    "            account_name = random.choice(account_names)\n",
    "            email = f\"{account_name.split(' ')[0].lower()}.{account_name.split(' ')[1].lower()}@example.com\"\n",
    "            writer.writerow([\n",
    "                account_name,\n",
    "                ''.join([\"{}\".format(random.randint(0, 9)) for _ in range(10)]),  # 10-digit CIF No\n",
    "                random.choice(account_owners),\n",
    "                f'+62{random.randint(1000000000, 9999999999)}',  # Phone number\n",
    "                email,\n",
    "                random.choice(nasabah_types),\n",
    "                random.choice(account_record_types)\n",
    "            ])\n",
    "\n",
    "\n",
    "file_path = 'D:\\dataquality2'  # Adjust the path as needed\n",
    "file_name = 'dummy_data_account.csv'\n",
    "num_rows = 1  # Adjust the number of rows as needed\n",
    "\n",
    "generate_dummy_data(file_path, file_name, num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Case ALex 7\n",
    "\n",
    "\n",
    "num of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dummy_casefor_alex11.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_salesforce_case_dummy_data(num_rows=100, start_ttb=1):\n",
    "    import pandas as pd\n",
    "    import random\n",
    "\n",
    "    # Define the columns and possible values\n",
    "    statuses = [\"New\", \"Working\", \"Escalated\"]\n",
    "    types = [\"Electronic\", \"Electrical\", \"Mechanical\"]\n",
    "    case_reasons = [\"Performance\", \"Breakdown\"]\n",
    "\n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        \"Status\": [random.choice(statuses) for _ in range(num_rows)],\n",
    "        \"Type\": [random.choice(types) for _ in range(num_rows)],\n",
    "        \"Case Reason\": [random.choice(case_reasons) for _ in range(num_rows)],\n",
    "        \"Subject\": [f\"Subject {i+1}\" for i in range(num_rows)],\n",
    "        \"Description\": [f\"Description of issue {i+1}\" for i in range(num_rows)],\n",
    "        \"Legacy_Ticket_ID\": [f\"TTB{start_ttb + i:06d}\" for i in range(num_rows)],  # Generating 6-digit random numbers in order\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = 'dummy_casefor_alex11.csv'\n",
    "    df['record_type'] = 'Case Migration'\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "# Example usage\n",
    "file_path = create_salesforce_case_dummy_data(num_rows=5, start_ttb=1006)\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_5108\\2256201397.py:5: DtypeWarning: Columns (14,16,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\bricaredatareal_20200101_20200201.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df=df.iloc[:100]\n",
    "df.to_csv('bricare_100_pakpondah.csv',index=False)\n",
    "\n",
    "# data dummy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_data_casenumber60_part1.csv with 500000 rows created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_dummy_files(base_filename, total_rows, start_ttb='TTB0000000001'):\n",
    "    \n",
    "    rows_per_file = 500000\n",
    "    num_files = math.ceil(total_rows / rows_per_file)\n",
    "    \n",
    "    \n",
    "    start_number = int(start_ttb[3:])\n",
    "    \n",
    "    for file_index in range(num_files):\n",
    "       \n",
    "        if file_index == num_files - 1:\n",
    "            current_rows = total_rows % rows_per_file or rows_per_file\n",
    "        else:\n",
    "            current_rows = rows_per_file\n",
    "        \n",
    "        \n",
    "        data = {\n",
    "            'Status': ['Cancelled'] * current_rows,\n",
    "            'Priority': ['Low'] * current_rows,\n",
    "            'Call Type': ['1000'] * current_rows,\n",
    "            'record_type': ['Case Migration'] * current_rows,\n",
    "            'Legacy_ticket_id': [f'TTB{str(i).zfill(10)}' for i in range(start_number + file_index * rows_per_file, start_number + file_index * rows_per_file + current_rows)]\n",
    "        }\n",
    "        \n",
    "       \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "       \n",
    "        current_filename = f\"{base_filename}_part{file_index + 1}.csv\"\n",
    "        \n",
    "        \n",
    "        df.to_csv(current_filename, index=False)\n",
    "        print(f\"{current_filename} with {current_rows} rows created successfully.\")\n",
    "    \n",
    "\n",
    "\n",
    "base_filename = 'dummy_data_casenumber60'\n",
    "total_rows = 500000\n",
    "start_ttb = 'TTB0000000000'\n",
    "generate_dummy_files(base_filename, total_rows, start_ttb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASENUMBER</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>New_SCC_LEGACY_TICKET_ID__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TTB0000092801</td>\n",
       "      <td>TTB0005092801</td>\n",
       "      <td>TTB0000092801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TTB0000092802</td>\n",
       "      <td>TTB0005092802</td>\n",
       "      <td>TTB0000092802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TTB0000092803</td>\n",
       "      <td>TTB0005092803</td>\n",
       "      <td>TTB0000092803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TTB0000092804</td>\n",
       "      <td>TTB0005092804</td>\n",
       "      <td>TTB0000092804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TTB0000092805</td>\n",
       "      <td>TTB0005092805</td>\n",
       "      <td>TTB0000092805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714216</th>\n",
       "      <td>TTB0001714196</td>\n",
       "      <td>TTB0006714196</td>\n",
       "      <td>TTB0001714196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714217</th>\n",
       "      <td>TTB0001714197</td>\n",
       "      <td>TTB0006714197</td>\n",
       "      <td>TTB0001714197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714218</th>\n",
       "      <td>TTB0001714198</td>\n",
       "      <td>TTB0006714198</td>\n",
       "      <td>TTB0001714198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714219</th>\n",
       "      <td>TTB0001714199</td>\n",
       "      <td>TTB0006714199</td>\n",
       "      <td>TTB0001714199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714220</th>\n",
       "      <td>TTB0001714200</td>\n",
       "      <td>TTB0006714200</td>\n",
       "      <td>TTB0001714200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1714200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CASENUMBER SCC_LEGACY_TICKET_ID__C New_SCC_LEGACY_TICKET_ID__C\n",
       "21       TTB0000092801           TTB0005092801               TTB0000092801\n",
       "22       TTB0000092802           TTB0005092802               TTB0000092802\n",
       "23       TTB0000092803           TTB0005092803               TTB0000092803\n",
       "24       TTB0000092804           TTB0005092804               TTB0000092804\n",
       "25       TTB0000092805           TTB0005092805               TTB0000092805\n",
       "...                ...                     ...                         ...\n",
       "1714216  TTB0001714196           TTB0006714196               TTB0001714196\n",
       "1714217  TTB0001714197           TTB0006714197               TTB0001714197\n",
       "1714218  TTB0001714198           TTB0006714198               TTB0001714198\n",
       "1714219  TTB0001714199           TTB0006714199               TTB0001714199\n",
       "1714220  TTB0001714200           TTB0006714200               TTB0001714200\n",
       "\n",
       "[1714200 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\Case2.csv\"\n",
    "\n",
    "df =pd.read_csv(path)\n",
    "df = df[21:]\n",
    "df['New_SCC_LEGACY_TICKET_ID__C']=df['CASENUMBER']\n",
    "df\n",
    "df.to_csv('case_okta.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Case ALex 7\n",
    "Join the two files based on Ticket ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Ticket_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m extract_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCASENUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicket_ID\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Merge the dataframes based on 'Ticket_ID'\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data_dummy_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTicket_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Reorder the columns to place 'ID' as the first column\u001b[39;00m\n\u001b[0;32m     31\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m merged_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:794\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_left_right_on(left_on, right_on)\n\u001b[0;32m    788\u001b[0m (\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[0;32m    792\u001b[0m     left_drop,\n\u001b[0;32m    793\u001b[0m     right_drop,\n\u001b[1;32m--> 794\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_drop_labels_or_levels(left_drop)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1310\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m     lk \u001b[38;5;241m=\u001b[39m cast(Hashable, lk)\n\u001b[1;32m-> 1310\u001b[0m     left_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1311\u001b[0m     join_names\u001b[38;5;241m.\u001b[39mappend(lk)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:1911\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1909\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1911\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Ticket_ID'"
     ]
    }
   ],
   "source": [
    "#extract the ID and CaseNumber:\n",
    "# SELECT Id, CaseNumber\n",
    "# FROM Case \n",
    "# WHERE CreatedById = '005MR0000004GJ7YAM'\n",
    "\n",
    "# from the real data to join the ID or Case Number and update the records in Case \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded files\n",
    "extract_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_150_casenumberandID.csv\")\n",
    "# test_data_dummy_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\Test Data Dummy.csv\")\n",
    "test_data_dummy_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error062124052713145.csv\")\n",
    "\n",
    "\n",
    "# Split the 'ID;\"CASENUMBER\"' column into two separate columns\n",
    "extract_df[['ID', 'CASENUMBER']] = extract_df['ID;\"CASENUMBER\"'].str.split(';', expand=True)\n",
    "\n",
    "# Drop the original combined column\n",
    "extract_df.drop(columns=['ID;\"CASENUMBER\"'], inplace=True)\n",
    "\n",
    "# Remove leading and trailing quotes from the columns\n",
    "extract_df['ID'] = extract_df['ID'].str.strip('\"')\n",
    "extract_df['CASENUMBER'] = extract_df['CASENUMBER'].str.strip('\"')\n",
    "\n",
    "# Rename columns for easier merging\n",
    "extract_df.rename(columns={'CASENUMBER': 'Ticket_ID'}, inplace=True)\n",
    "\n",
    "# Merge the dataframes based on 'Ticket_ID'\n",
    "merged_df = test_data_dummy_df.merge(extract_df, on='Ticket_ID', how='left')\n",
    "\n",
    "# Reorder the columns to place 'ID' as the first column\n",
    "columns = ['ID'] + [col for col in merged_df.columns if col != 'ID']\n",
    "merged_df = merged_df[columns]\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "# merged_df=merged_df.iloc[:1]\n",
    "merged_df['Call_Type_ID']='1000'\n",
    "merged_df=merged_df.drop('ID', axis=1)\n",
    "merged_df.to_csv('Updated_Test_Data_Dummy_2row.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_17788\\1435779221.py:11: DtypeWarning: Columns (47,48,84,108,124,130,132,189,200,203,204,229,247) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(path, delimiter=';')\n"
     ]
    }
   ],
   "source": [
    "# the extracted data from Salesforce after being updated \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\case_150.csv\"\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_case_uat_120k.csv\"\n",
    "\n",
    "df=pd.read_csv(path, delimiter=';')\n",
    "df2 = df[['ID', 'STATUS','CASENUMBER', 'SCC_LEGACY_TICKET_ID__C']]\n",
    "\n",
    "# df3 = df2[df2['SCC_LEGACY_TICKET_ID__C'].notna()]\n",
    "# df2.iloc[70990:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Supported Objects in Queue Name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First Extract all the Queue ID from Group Object, you may use this query:\n",
    "# SELECT Id\n",
    "# where Type ='Queue'\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_group_queueid.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# add one column 'SObjectType' = Case, see the example below:\n",
    "# QueueId,SObjectType\n",
    "# 00GMR0000000trT2AQ,Case\n",
    "\n",
    "df['SObjectType']='Case'\n",
    "df.to_csv('extract_group_queueid.csv', index=False)\n",
    "\n",
    "# Go to Data Loader 'insert' and choose  'QueueSobject' and insert the file with two columns: Id and SObjectType = Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\log\\error062124041536955.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "# df= df.drop('ERROR', axis=1)\n",
    "# df= df.drop('ERROR.1', axis=1)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cifno', 'Ticket_ID', 'Call_Type_ID', 'Call_Type', 'Create_Date',\n",
       "       'gateway', 'Jenis_Laporan', 'Nama_Nasabah', 'No_Rekening', 'Nominal',\n",
       "       'status', 'TanggalClosed', 'tanggalTransaksi', 'Chanel', 'Fitur',\n",
       "       'Nomor_Kartu', 'user_group', 'assgined_to', 'attachment_done', 'email',\n",
       "       'full_name', 'no_telepon', 'approver_login', 'approver_name',\n",
       "       'SLAResolution', 'submitter_login_id', 'submitter_user_group',\n",
       "       'user_login_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_case (2)\\bricare_20200806_20240430_0_27.csv\"\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the file path\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_case (2)\\bricare_20200806_20240430_0_27.csv\"\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\sample_case (2)\\bricare_20200806_20240430_1000001_27.csv\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "# df = df.iloc[:1]\n",
    "# Change the year to 2019\n",
    "df['Create_Date'] = pd.to_datetime(df['Create_Date'])\n",
    "df['Create_Date'] = df['Create_Date'].apply(lambda x: x.replace(year=2019))\n",
    "\n",
    "def generate_random_16_digits():\n",
    "    return ''.join([str(np.random.randint(0, 10)) for _ in range(16)])\n",
    "\n",
    "# Add a new column 'Nomor_Kartu' with random 16-digit numbers\n",
    "df['Nomor_Kartu'] = df.apply(lambda x: generate_random_16_digits(), axis=1)\n",
    "\n",
    "# Remove the .0 from Call_Type_ID by converting it to an integer\n",
    "df['Call_Type_ID'] = '8412'\n",
    "df['Call_Type_ID'] = df['Call_Type_ID'].astype(int)\n",
    "\n",
    "def generate_random_15_to_18_digits():\n",
    "    return ''.join([str(np.random.randint(0, 10)) for _ in range(np.random.randint(15, 19))])\n",
    "\n",
    "# Update the 'No_Rekening' column with random 15 to 18 digit numbers\n",
    "df['No_Rekening'] = df.apply(lambda x: generate_random_15_to_18_digits(), axis=1)\n",
    "\n",
    "# add recordtype\n",
    "df['record_type']='Case Migration'\n",
    "\n",
    "df.to_csv('bricare_27col_100k_2.csv', index=False)\n",
    "\n",
    "\n",
    "# df=df[df['Ticket_ID'] == 'TTB000025269322']\n",
    "# df\n",
    "\n",
    "# Extract just the 'Ticket_ID' column and sort it\n",
    "# df2 = df['Ticket_ID'].sort_values(ascending=True)\n",
    "# df2\n",
    "\n",
    "# Display the sorted Series\n",
    "# df2.to_csv('tiket_bricare.csv', index=False)\n",
    "\n",
    "\n",
    "#dummy 120k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "\n",
    "\n",
    "df['Ticket_ID'] = df['Ticket_ID'].str.replace('TTB', 'TICKET')\n",
    "\n",
    "df.to_csv('bricare_120k_ticket.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket_ID values have been successfully replaced and saved to 'updated_bricare_120k_ticket.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cifno</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>Nominal</th>\n",
       "      <th>...</th>\n",
       "      <th>email</th>\n",
       "      <th>full_name</th>\n",
       "      <th>no_telepon</th>\n",
       "      <th>approver_login</th>\n",
       "      <th>approver_name</th>\n",
       "      <th>SLAResolution</th>\n",
       "      <th>submitter_login_id</th>\n",
       "      <th>submitter_user_group</th>\n",
       "      <th>user_login_name</th>\n",
       "      <th>record_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LD86852</td>\n",
       "      <td>TICKET000998035</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b2714defcfef8c65d3154367beb604aa</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>23300800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RCAW297</td>\n",
       "      <td>TICKET000998036</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal tarik tunai &amp; terdebet di AT...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b69e3868bc8b6c3231df7d691a5595fe</td>\n",
       "      <td>5073214431798791</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AOY4038</td>\n",
       "      <td>TICKET000998037</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>948421d448fb51f1cf27584b1256a120</td>\n",
       "      <td>6430217575043687</td>\n",
       "      <td>246300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUA0263</td>\n",
       "      <td>TICKET000998038</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal transfer &amp; terdebet jaringan...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>a4aa532c65d1503dcc41ef396d3c42b8</td>\n",
       "      <td>4067829670996616</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOL2556</td>\n",
       "      <td>TICKET000998039</td>\n",
       "      <td>8412</td>\n",
       "      <td>Kartu ATM BRI Tertelan di MESIN ATM</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>95d4f5421b5b387726b5d619dbe24155</td>\n",
       "      <td>272778316775473552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90119221.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cifno        Ticket_ID  Call_Type_ID  \\\n",
       "0  LD86852  TICKET000998035          8412   \n",
       "1  RCAW297  TICKET000998036          8412   \n",
       "2  AOY4038  TICKET000998037          8412   \n",
       "3  AUA0263  TICKET000998038          8412   \n",
       "4  HOL2556  TICKET000998039          8412   \n",
       "\n",
       "                                           Call_Type          Create_Date  \\\n",
       "0  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "1  Nasabah BRI gagal tarik tunai & terdebet di AT...  2019-08-06 16:52:14   \n",
       "2  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "3  Nasabah BRI gagal transfer & terdebet jaringan...  2019-08-06 16:52:14   \n",
       "4                Kartu ATM BRI Tertelan di MESIN ATM  2019-08-06 16:52:14   \n",
       "\n",
       "  gateway            Jenis_Laporan                      Nama_Nasabah  \\\n",
       "0   Phone  Complaint - Transaction  b2714defcfef8c65d3154367beb604aa   \n",
       "1   Phone  Complaint - Transaction  b69e3868bc8b6c3231df7d691a5595fe   \n",
       "2   Phone  Complaint - Transaction  948421d448fb51f1cf27584b1256a120   \n",
       "3   Phone  Complaint - Transaction  a4aa532c65d1503dcc41ef396d3c42b8   \n",
       "4   Phone                  Request  95d4f5421b5b387726b5d619dbe24155   \n",
       "\n",
       "          No_Rekening     Nominal  ... email  \\\n",
       "0     878667067646102  23300800.0  ...   NaN   \n",
       "1    5073214431798791   1000000.0  ...   NaN   \n",
       "2    6430217575043687    246300.0  ...   NaN   \n",
       "3    4067829670996616    100000.0  ...   NaN   \n",
       "4  272778316775473552         0.0  ...   NaN   \n",
       "\n",
       "                          full_name no_telepon approver_login  \\\n",
       "0  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "1  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "2  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "3  d41d8cd98f00b204e9800998ecf8427e        NaN            NaN   \n",
       "4  fcc2dc136c7c3d69fb080cdd2b1648f1        NaN            NaN   \n",
       "\n",
       "                      approver_name  SLAResolution submitter_login_id  \\\n",
       "0  d41d8cd98f00b204e9800998ecf8427e           20.0                NaN   \n",
       "1  d41d8cd98f00b204e9800998ecf8427e           10.0                NaN   \n",
       "2  d41d8cd98f00b204e9800998ecf8427e           20.0                NaN   \n",
       "3  d41d8cd98f00b204e9800998ecf8427e           20.0                NaN   \n",
       "4  d41d8cd98f00b204e9800998ecf8427e           20.0         90119221.0   \n",
       "\n",
       "   submitter_user_group                   user_login_name     record_type  \n",
       "0                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "1                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "2                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "3                   NaN  d41d8cd98f00b204e9800998ecf8427e  Case Migration  \n",
       "4           LCC-CCTCALL  fcc2dc136c7c3d69fb080cdd2b1648f1  Case Migration  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "extract_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\extract_dummyfromserver_120k.csv.xls\")\n",
    "bricare_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\")\n",
    "\n",
    "# Check the lengths of the CaseNumber and Ticket_ID columns\n",
    "extract_length = len(extract_df['CaseNumber'])\n",
    "bricare_length = len(bricare_df['Ticket_ID'])\n",
    "\n",
    "# Ensure the number of CaseNumber matches the number of Ticket_ID\n",
    "if extract_length >= bricare_length:\n",
    "    # Trim the extract dataframe to match the length of the bricare dataframe\n",
    "    trimmed_extract_df = extract_df.iloc[:bricare_length]\n",
    "    \n",
    "    # Replace Ticket_ID values with CaseNumber values\n",
    "    bricare_df['Ticket_ID'] = trimmed_extract_df['CaseNumber'].values\n",
    "    \n",
    "    # Save the updated bricare dataframe to a new CSV file\n",
    "    output_file_path = 'updated_bricare_120k_ticket.csv'\n",
    "    # bricare_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(\"Ticket_ID values have been successfully replaced and saved to 'updated_bricare_120k_ticket.csv'.\")\n",
    "else:\n",
    "    print(\"Error: The number of CaseNumber entries is less than the number of Ticket_ID entries. Ensure they are one-to-one.\")\n",
    "\n",
    "# Display the first few rows of the updated bricare dataframe\n",
    "bricare_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\updated_bricare_120k_ticket_with_id.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df=df.iloc[:1]\n",
    "df.to_csv('updated_bricare_1_ticket.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket_ID values have been successfully replaced and the Id column has been added and saved to 'updated_bricare_120k_ticket_with_id.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>cifno</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>...</th>\n",
       "      <th>email</th>\n",
       "      <th>full_name</th>\n",
       "      <th>no_telepon</th>\n",
       "      <th>approver_login</th>\n",
       "      <th>approver_name</th>\n",
       "      <th>SLAResolution</th>\n",
       "      <th>submitter_login_id</th>\n",
       "      <th>submitter_user_group</th>\n",
       "      <th>user_login_name</th>\n",
       "      <th>record_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500MR000004Q2wkYAC</td>\n",
       "      <td>LD86852</td>\n",
       "      <td>TICKET000998035</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b2714defcfef8c65d3154367beb604aa</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500MR000004Q2wlYAC</td>\n",
       "      <td>RCAW297</td>\n",
       "      <td>TICKET000998036</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal tarik tunai &amp; terdebet di AT...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>b69e3868bc8b6c3231df7d691a5595fe</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500MR000004Q2wmYAC</td>\n",
       "      <td>AOY4038</td>\n",
       "      <td>TICKET000998037</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI Gagal Transaksi Belanja di EDC BRI...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>948421d448fb51f1cf27584b1256a120</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500MR000004Q2wnYAC</td>\n",
       "      <td>AUA0263</td>\n",
       "      <td>TICKET000998038</td>\n",
       "      <td>8412</td>\n",
       "      <td>Nasabah BRI gagal transfer &amp; terdebet jaringan...</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>a4aa532c65d1503dcc41ef396d3c42b8</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500MR000004Q2woYAC</td>\n",
       "      <td>HOL2556</td>\n",
       "      <td>TICKET000998039</td>\n",
       "      <td>8412</td>\n",
       "      <td>Kartu ATM BRI Tertelan di MESIN ATM</td>\n",
       "      <td>2019-08-06 16:52:14</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>95d4f5421b5b387726b5d619dbe24155</td>\n",
       "      <td>878667067646102</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90119221.0</td>\n",
       "      <td>LCC-CCTCALL</td>\n",
       "      <td>fcc2dc136c7c3d69fb080cdd2b1648f1</td>\n",
       "      <td>Case Migration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id    cifno        Ticket_ID  Call_Type_ID  \\\n",
       "0  500MR000004Q2wkYAC  LD86852  TICKET000998035          8412   \n",
       "1  500MR000004Q2wlYAC  RCAW297  TICKET000998036          8412   \n",
       "2  500MR000004Q2wmYAC  AOY4038  TICKET000998037          8412   \n",
       "3  500MR000004Q2wnYAC  AUA0263  TICKET000998038          8412   \n",
       "4  500MR000004Q2woYAC  HOL2556  TICKET000998039          8412   \n",
       "\n",
       "                                           Call_Type          Create_Date  \\\n",
       "0  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "1  Nasabah BRI gagal tarik tunai & terdebet di AT...  2019-08-06 16:52:14   \n",
       "2  Nasabah BRI Gagal Transaksi Belanja di EDC BRI...  2019-08-06 16:52:14   \n",
       "3  Nasabah BRI gagal transfer & terdebet jaringan...  2019-08-06 16:52:14   \n",
       "4                Kartu ATM BRI Tertelan di MESIN ATM  2019-08-06 16:52:14   \n",
       "\n",
       "  gateway            Jenis_Laporan                      Nama_Nasabah  \\\n",
       "0   Phone  Complaint - Transaction  b2714defcfef8c65d3154367beb604aa   \n",
       "1   Phone  Complaint - Transaction  b69e3868bc8b6c3231df7d691a5595fe   \n",
       "2   Phone  Complaint - Transaction  948421d448fb51f1cf27584b1256a120   \n",
       "3   Phone  Complaint - Transaction  a4aa532c65d1503dcc41ef396d3c42b8   \n",
       "4   Phone                  Request  95d4f5421b5b387726b5d619dbe24155   \n",
       "\n",
       "       No_Rekening  ...  email                         full_name no_telepon  \\\n",
       "0  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "1  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "2  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "3  878667067646102  ...    NaN  d41d8cd98f00b204e9800998ecf8427e        NaN   \n",
       "4  878667067646102  ...    NaN  fcc2dc136c7c3d69fb080cdd2b1648f1        NaN   \n",
       "\n",
       "  approver_login                     approver_name SLAResolution  \\\n",
       "0            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "1            NaN  d41d8cd98f00b204e9800998ecf8427e          10.0   \n",
       "2            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "3            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "4            NaN  d41d8cd98f00b204e9800998ecf8427e          20.0   \n",
       "\n",
       "  submitter_login_id submitter_user_group                   user_login_name  \\\n",
       "0                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "1                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "2                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "3                NaN                  NaN  d41d8cd98f00b204e9800998ecf8427e   \n",
       "4         90119221.0          LCC-CCTCALL  fcc2dc136c7c3d69fb080cdd2b1648f1   \n",
       "\n",
       "      record_type  \n",
       "0  Case Migration  \n",
       "1  Case Migration  \n",
       "2  Case Migration  \n",
       "3  Case Migration  \n",
       "4  Case Migration  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "extract_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\extract_dummyfromserver_120k.csv.xls\")\n",
    "bricare_df = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\")\n",
    "\n",
    "# Check the lengths of the CaseNumber and Ticket_ID columns\n",
    "extract_length = len(extract_df['CaseNumber'])\n",
    "bricare_length = len(bricare_df['Ticket_ID'])\n",
    "\n",
    "# Ensure the number of CaseNumber matches the number of Ticket_ID\n",
    "if extract_length >= bricare_length:\n",
    "    # Trim the extract dataframe to match the length of the bricare dataframe\n",
    "    trimmed_extract_df = extract_df.iloc[:bricare_length]\n",
    "    \n",
    "    # Replace Ticket_ID values with CaseNumber values\n",
    "    bricare_df['Ticket_ID'] = trimmed_extract_df['CaseNumber'].values\n",
    "    \n",
    "    # Add the Id column from the extract dataframe to the bricare dataframe\n",
    "    bricare_df['Id'] = trimmed_extract_df['Id'].values\n",
    "    \n",
    "    # Reorder the columns to place Id as the first column\n",
    "    cols = bricare_df.columns.tolist()\n",
    "    cols = ['Id'] + [col for col in cols if col != 'Id']\n",
    "    bricare_df = bricare_df[cols]\n",
    "    \n",
    "    # Save the updated bricare dataframe to a new CSV file\n",
    "    output_file_path = 'updated_bricare_120k_ticket_with_id.csv'\n",
    "    bricare_df['Nomor_Kartu']='1706269727199000'\n",
    "    bricare_df['No_Rekening']='878667067646102'\n",
    "    \n",
    "    bricare_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(\"Ticket_ID values have been successfully replaced and the Id column has been added and saved to 'updated_bricare_120k_ticket_with_id.csv'.\")\n",
    "else:\n",
    "    print(\"Error: The number of CaseNumber entries is less than the number of Ticket_ID entries. Ensure they are one-to-one.\")\n",
    "\n",
    "# Display the first few rows of the updated bricare dataframe\n",
    "bricare_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-27 04:30:20'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Original date and time in UTC\n",
    "utc_time_str_2 = \"2024-06-26T21:30:20.000+0000\"\n",
    "utc_time_2 = datetime.strptime(utc_time_str_2, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "# Convert to Jakarta time\n",
    "jakarta_tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "jakarta_time_2 = utc_time_2.astimezone(jakarta_tz)\n",
    "\n",
    "# Format the Jakarta time as a string\n",
    "jakarta_time_str_2 = jakarta_time_2.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "jakarta_time_str_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-26T18:10:20.000000+0000'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Jakarta date and time string\n",
    "jakarta_time_str_2 = \"2024-06-27 01:10:20\"\n",
    "\n",
    "# Define Jakarta timezone\n",
    "jakarta_tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "\n",
    "# Parse the Jakarta time string into a datetime object\n",
    "jakarta_time_2 = datetime.strptime(jakarta_time_str_2, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Localize the Jakarta time\n",
    "jakarta_time_2 = jakarta_tz.localize(jakarta_time_2)\n",
    "\n",
    "# Convert to UTC\n",
    "utc_time_2 = jakarta_time_2.astimezone(pytz.utc)\n",
    "\n",
    "# Format the UTC time as a string\n",
    "utc_time_str_2 = utc_time_2.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "utc_time_str_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the bricare files and the 220k file\n",
    "file_220k = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\220K.csv.xls\")\n",
    "bricare_1 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k.csv\")\n",
    "bricare_2 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\bricare_27col_100k_2.csv\")\n",
    "\n",
    "# Merge Id and CaseNumber into bricare files\n",
    "bricare_1 = bricare_1.merge(file_220k[['Id', 'CaseNumber']], left_index=True, right_index=True)\n",
    "bricare_2 = bricare_2.merge(file_220k[['Id', 'CaseNumber']], left_index=True, right_index=True)\n",
    "\n",
    "# Replace Ticket_ID with CaseNumber from 220K file\n",
    "bricare_1['Ticket_ID'] = bricare_1['CaseNumber']\n",
    "bricare_2['Ticket_ID'] = bricare_2['CaseNumber']\n",
    "\n",
    "# Drop the temporary 'CaseNumber' column\n",
    "bricare_1.drop(columns=['CaseNumber'], inplace=True)\n",
    "bricare_2.drop(columns=['CaseNumber'], inplace=True)\n",
    "\n",
    "# Combine the Ticket_IDs from both files to identify overlaps\n",
    "combined_ticket_ids = pd.concat([bricare_1['Ticket_ID'], bricare_2['Ticket_ID']])\n",
    "\n",
    "# Identify duplicate Ticket_IDs\n",
    "duplicates = combined_ticket_ids[combined_ticket_ids.duplicated()]\n",
    "\n",
    "# Reassign duplicates in bricare_2 to ensure uniqueness\n",
    "for i, dup in enumerate(duplicates):\n",
    "    bricare_2.loc[bricare_2['Ticket_ID'] == dup, 'Ticket_ID'] += f'_DUP{i}'\n",
    "\n",
    "# Save the transformed files\n",
    "bricare_1.to_csv('bricare_27col_100k_transformed_no_suffix.csv', index=False)\n",
    "bricare_2.to_csv('bricare_27col_100k_2_transformed_no_suffix.csv', index=False)\n",
    "\n",
    "# Check if Ticket_IDs are completely different between the two files\n",
    "ticket_id_1 = set(bricare_1['Ticket_ID'])\n",
    "ticket_id_2 = set(bricare_2['Ticket_ID'])\n",
    "\n",
    "# Ensure no Ticket_ID is shared between the two files\n",
    "unique_across_files = ticket_id_1.isdisjoint(ticket_id_2)\n",
    "\n",
    "\n",
    "unique_across_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in the Ticket_ID column are completely different between the two files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files into pandas DataFrames\n",
    "file1 = pd.read_csv(r\"D:\\dataquality2\\bricare_27col_100k_transformed_no_suffix.csv\")\n",
    "file2 = pd.read_csv(r\"D:\\dataquality2\\bricare_27col_100k_2_transformed_no_suffix.csv\")\n",
    "\n",
    "# Extract the Ticket_ID column from both DataFrames\n",
    "ticket_ids_file1 = file1['Ticket_ID']\n",
    "ticket_ids_file2 = file2['Ticket_ID']\n",
    "\n",
    "# Check if there are any common values\n",
    "common_ids = set(ticket_ids_file1).intersection(set(ticket_ids_file2))\n",
    "\n",
    "if len(common_ids) == 0:\n",
    "    print(\"All values in the Ticket_ID column are completely different between the two files.\")\n",
    "else:\n",
    "    print(\"There are common values in the Ticket_ID column between the two files.\")\n",
    "    print(\"Common values:\", common_ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Group Name between SF and BRicare\n",
    "\n",
    "from pak Suyanto\n",
    "\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Group name ds_ej.csv\"\n",
    "\n",
    "from SF\n",
    "\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\CSV 2 - Group.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "file1_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\ds_ej.csv\"\n",
    "file2_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\CSV 2 - Group.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "\n",
    "user_groups_ds_ej = df1['usergrouptxt'].str.strip()\n",
    "queue_group_names = df2['Queue & Group Name'].str.strip()\n",
    "\n",
    "\n",
    "missing_groups = user_groups_ds_ej[~user_groups_ds_ej.isin(queue_group_names)]\n",
    "\n",
    "missing_groups_list = missing_groups.tolist()\n",
    "len(missing_groups_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 345)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV and Excel files\n",
    "csv_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Pak Hadi Green.xlsx\"\n",
    "excel_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Cleaned.xlsx\"\n",
    "\n",
    "# Read the files\n",
    "df_csv = pd.read_excel(csv_file_path)\n",
    "df_excel = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Extract the relevant columns for comparison\n",
    "case_types = set(df_csv['Case Types'].astype(str))\n",
    "call_type_numbers = set(df_excel['Call Type Number'].astype(str))\n",
    "\n",
    "# Find the missing and common case types\n",
    "missing_case_types = case_types - call_type_numbers\n",
    "common_case_types = case_types & call_type_numbers\n",
    "\n",
    "# Save the results to CSV files\n",
    "missing_case_types_df = pd.DataFrame(missing_case_types, columns=[\"Missing Case Types\"])\n",
    "common_case_types_df = pd.DataFrame(common_case_types, columns=[\"Common Case Types\"])\n",
    "\n",
    "missing_case_types_df.to_csv('case_types_missing.csv', index=False)\n",
    "common_case_types_df.to_csv('case_types_common.csv', index=False)\n",
    "\n",
    "# Provide the links to download the files\n",
    "len(missing_case_types_df), len(common_case_types_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call Type Number</th>\n",
       "      <th>Active</th>\n",
       "      <th>Send to Drone?</th>\n",
       "      <th>Additional Details (Formulir Detail)</th>\n",
       "      <th>Customer Segment</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub Product</th>\n",
       "      <th>Case Category</th>\n",
       "      <th>Case Type Description</th>\n",
       "      <th>Sub-Description</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 43</th>\n",
       "      <th>Unnamed: 44</th>\n",
       "      <th>Unnamed: 45</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>Unnamed: 47</th>\n",
       "      <th>Unnamed: 48</th>\n",
       "      <th>Unnamed: 49</th>\n",
       "      <th>Unnamed: 50</th>\n",
       "      <th>Unnamed: 51</th>\n",
       "      <th>external_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>nomor ponsel nasabah yang bisa dihubungi: \\nID CERIA:\\nNO HP yg terdaftar:\\nAlasan:</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Nasabah Menanyakan Informasi Pengajuan Terkait CERIA</td>\n",
       "      <td>1. Cara, Syarat &amp; Ketentuan\\n2. Status\\n3. Pembatalan</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Products / Promotion Inquiry</td>\n",
       "      <td>Nasabah Menyakan Terkait Promo dan Program CERIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Request</td>\n",
       "      <td>Nasabah Mengajukan Pelunasan Awal Cicilan CERIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1005</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Nasabah mengajukan pemblokiran Sementara Akun Ceria\\nNomor ID :\\nAlasan pemblokiran :\\n\\nMohon bantuan tindak lanjut\\nTerima kasih</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Request</td>\n",
       "      <td>Nasabah Mengajukan Pemblokiran CERIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1008</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Individu Umum</td>\n",
       "      <td>Loans</td>\n",
       "      <td>KTA - Digital</td>\n",
       "      <td>Request</td>\n",
       "      <td>Nasabah Mengajukan Pengaktifan Akun Ceria Terblokir karena Fraud Detection System</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>TRB-8</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korporasi</td>\n",
       "      <td>Servicing</td>\n",
       "      <td>Cash Management</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Nasabah Menanyakan Pengajuan Pendaftaran Product BRIVA tidak disetujui pada QLOLA Apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRB-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>TRB-9</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korporasi</td>\n",
       "      <td>Servicing</td>\n",
       "      <td>Cash Management</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Nasabah Menanyakan Pengajuan Pendaftaran Product Corporate Billing Management (BRI CBM) tidak disetujui pada QLOLA Apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRB-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>UMI-1</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMKM</td>\n",
       "      <td>Transaction Banking</td>\n",
       "      <td>Merchant Solutions</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Calon Mitra UMI Menanyakan Cara, Syarat, dan Ketentuan mengenai Pengajuan Menjadi Mitra UMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMI-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>UMI-2</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMKM</td>\n",
       "      <td>Transaction Banking</td>\n",
       "      <td>Merchant Solutions</td>\n",
       "      <td>Inquiry</td>\n",
       "      <td>Mitra UMI Menanyakan Informasi Fee Transaksi</td>\n",
       "      <td>1. Disbursment\\n2. Loan quality fee</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMI-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>UMI-3</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMKM</td>\n",
       "      <td>Transaction Banking</td>\n",
       "      <td>Merchant Solutions</td>\n",
       "      <td>Complaint - Transaction</td>\n",
       "      <td>Mitra UMI Komplain Fee Transaksi Belum Masuk ke Rekening BRI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UMI-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Call Type Number Active Send to Drone?  \\\n",
       "0               1000   TRUE          FALSE   \n",
       "1               1002   TRUE          FALSE   \n",
       "2               1003   TRUE          FALSE   \n",
       "3               1005   TRUE          FALSE   \n",
       "4               1008   TRUE          FALSE   \n",
       "..               ...    ...            ...   \n",
       "379            TRB-8   TRUE          FALSE   \n",
       "380            TRB-9   TRUE          FALSE   \n",
       "381            UMI-1   TRUE          FALSE   \n",
       "382            UMI-2   TRUE          FALSE   \n",
       "383            UMI-3   TRUE          FALSE   \n",
       "\n",
       "                                                                                                   Additional Details (Formulir Detail)  \\\n",
       "0                                                   nomor ponsel nasabah yang bisa dihubungi: \\nID CERIA:\\nNO HP yg terdaftar:\\nAlasan:   \n",
       "1                                                                                                                                   NaN   \n",
       "2                                                                                                                                   NaN   \n",
       "3    Nasabah mengajukan pemblokiran Sementara Akun Ceria\\nNomor ID :\\nAlasan pemblokiran :\\n\\nMohon bantuan tindak lanjut\\nTerima kasih   \n",
       "4                                                                                                                                   NaN   \n",
       "..                                                                                                                                  ...   \n",
       "379                                                                                                                                 NaN   \n",
       "380                                                                                                                                 NaN   \n",
       "381                                                                                                                                 NaN   \n",
       "382                                                                                                                                 NaN   \n",
       "383                                                                                                                                 NaN   \n",
       "\n",
       "    Customer Segment              Product         Sub Product  \\\n",
       "0      Individu Umum                Loans       KTA - Digital   \n",
       "1      Individu Umum                Loans       KTA - Digital   \n",
       "2      Individu Umum                Loans       KTA - Digital   \n",
       "3      Individu Umum                Loans       KTA - Digital   \n",
       "4      Individu Umum                Loans       KTA - Digital   \n",
       "..               ...                  ...                 ...   \n",
       "379        Korporasi            Servicing     Cash Management   \n",
       "380        Korporasi            Servicing     Cash Management   \n",
       "381             UMKM  Transaction Banking  Merchant Solutions   \n",
       "382             UMKM  Transaction Banking  Merchant Solutions   \n",
       "383             UMKM  Transaction Banking  Merchant Solutions   \n",
       "\n",
       "                    Case Category  \\\n",
       "0                         Inquiry   \n",
       "1    Products / Promotion Inquiry   \n",
       "2                         Request   \n",
       "3                         Request   \n",
       "4                         Request   \n",
       "..                            ...   \n",
       "379                       Inquiry   \n",
       "380                       Inquiry   \n",
       "381                       Inquiry   \n",
       "382                       Inquiry   \n",
       "383       Complaint - Transaction   \n",
       "\n",
       "                                                                                                       Case Type Description  \\\n",
       "0                                                                       Nasabah Menanyakan Informasi Pengajuan Terkait CERIA   \n",
       "1                                                                           Nasabah Menyakan Terkait Promo dan Program CERIA   \n",
       "2                                                                            Nasabah Mengajukan Pelunasan Awal Cicilan CERIA   \n",
       "3                                                                                       Nasabah Mengajukan Pemblokiran CERIA   \n",
       "4                                          Nasabah Mengajukan Pengaktifan Akun Ceria Terblokir karena Fraud Detection System   \n",
       "..                                                                                                                       ...   \n",
       "379                                   Nasabah Menanyakan Pengajuan Pendaftaran Product BRIVA tidak disetujui pada QLOLA Apps   \n",
       "380  Nasabah Menanyakan Pengajuan Pendaftaran Product Corporate Billing Management (BRI CBM) tidak disetujui pada QLOLA Apps   \n",
       "381                              Calon Mitra UMI Menanyakan Cara, Syarat, dan Ketentuan mengenai Pengajuan Menjadi Mitra UMI   \n",
       "382                                                                             Mitra UMI Menanyakan Informasi Fee Transaksi   \n",
       "383                                                             Mitra UMI Komplain Fee Transaksi Belum Masuk ke Rekening BRI   \n",
       "\n",
       "                                           Sub-Description  ... Unnamed: 43  \\\n",
       "0    1. Cara, Syarat & Ketentuan\\n2. Status\\n3. Pembatalan  ...         NaN   \n",
       "1                                                      NaN  ...         NaN   \n",
       "2                                                      NaN  ...         NaN   \n",
       "3                                                      NaN  ...         NaN   \n",
       "4                                                      NaN  ...         NaN   \n",
       "..                                                     ...  ...         ...   \n",
       "379                                                    NaN  ...         NaN   \n",
       "380                                                    NaN  ...         NaN   \n",
       "381                                                    NaN  ...         NaN   \n",
       "382                    1. Disbursment\\n2. Loan quality fee  ...         NaN   \n",
       "383                                                    NaN  ...         NaN   \n",
       "\n",
       "    Unnamed: 44  Unnamed: 45  Unnamed: 46 Unnamed: 47  Unnamed: 48  \\\n",
       "0           NaN          NaN          NaN         NaN          NaN   \n",
       "1           NaN          NaN          NaN         NaN          NaN   \n",
       "2           NaN          NaN          NaN         NaN          NaN   \n",
       "3           NaN          NaN          NaN         NaN          NaN   \n",
       "4           NaN          NaN          NaN         NaN          NaN   \n",
       "..          ...          ...          ...         ...          ...   \n",
       "379         NaN          NaN          NaN         NaN          NaN   \n",
       "380         NaN          NaN          NaN         NaN          NaN   \n",
       "381         NaN          NaN          NaN         NaN          NaN   \n",
       "382         NaN          NaN          NaN         NaN          NaN   \n",
       "383         NaN          NaN          NaN         NaN          NaN   \n",
       "\n",
       "    Unnamed: 49  Unnamed: 50 Unnamed: 51  external_id  \n",
       "0           NaN          NaN         NaN         1000  \n",
       "1           NaN          NaN         NaN         1002  \n",
       "2           NaN          NaN         NaN         1003  \n",
       "3           NaN          NaN         NaN         1005  \n",
       "4           NaN          NaN         NaN         1008  \n",
       "..          ...          ...         ...          ...  \n",
       "379         NaN          NaN         NaN        TRB-8  \n",
       "380         NaN          NaN         NaN        TRB-9  \n",
       "381         NaN          NaN         NaN        UMI-1  \n",
       "382         NaN          NaN         NaN        UMI-2  \n",
       "383         NaN          NaN         NaN        UMI-3  \n",
       "\n",
       "[384 rows x 53 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Cleaned - TBM.xlsx\"\n",
    "\n",
    "path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Call Type Cleaned - TBM.xlsx\"\n",
    "\n",
    "\n",
    "df= pd.read_excel(path)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# df = df[df['Call Type Number']== 'UMI-3'] # Modify this\n",
    "df['Send to Drone?'] = df['Send to Drone?'].replace({'No': 'FALSE', 'Yes': 'TRUE'})\n",
    "df['Active'] = df['Active'].astype(str).str.upper()\n",
    "df['external_id']=df['Call Type Number']\n",
    "\n",
    "\n",
    "# df.to_csv('CallType_umi3.csv', index=False)\n",
    "df.to_csv('CallType_cleaned.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casenumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found and saved to CSV.\n",
      "Gaps found and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_ticket_ids(file_path, case_column, output_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract the numeric part of the ticket IDs\n",
    "    df['Numeric_ID'] = df[case_column].str.extract(r'(\\d+)')\n",
    "    \n",
    "    # Drop rows with NaN values in 'Numeric_ID'\n",
    "    df = df.dropna(subset=['Numeric_ID'])\n",
    "    \n",
    "    # Convert the numeric part to integers\n",
    "    df['Numeric_ID'] = df['Numeric_ID'].astype(int)\n",
    "    \n",
    "    # Sort the DataFrame by the numeric ID\n",
    "    df_sorted = df.sort_values(by='Numeric_ID').reset_index(drop=True)\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = df_sorted[df_sorted.duplicated(subset='Numeric_ID', keep=False)]\n",
    "    \n",
    "    # Find gaps\n",
    "    df_sorted['Next_ID'] = df_sorted['Numeric_ID'].shift(-1)\n",
    "    df_sorted['Gap'] = df_sorted['Next_ID'] - df_sorted['Numeric_ID']\n",
    "    gaps = df_sorted[df_sorted['Gap'] != 1]\n",
    "    \n",
    "    # Detailed gap analysis\n",
    "    gap_details = []\n",
    "    for idx, row in gaps.iterrows():\n",
    "        if not pd.isna(row['Next_ID']):\n",
    "            gap_start = row[case_column]\n",
    "            gap_end = df_sorted.iloc[idx + 1][case_column]\n",
    "            gap_size = row['Gap'] - 1\n",
    "            gap_details.append({'Gap_Start': gap_start, 'Gap_End': gap_end, 'Gap_Size': gap_size})\n",
    "    \n",
    "    # Save results to CSV files\n",
    "    if not duplicates.empty:\n",
    "        duplicates.to_csv(output_path + '_duplicates.csv', index=False)\n",
    "        print(\"Duplicates found and saved to CSV.\")\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "    \n",
    "    if gap_details:\n",
    "        gaps_df = pd.DataFrame(gap_details)\n",
    "        gaps_df.to_csv(output_path + '_gaps.csv', index=False)\n",
    "        print(\"Gaps found and saved to CSV.\")\n",
    "    else:\n",
    "        print(\"No gaps found.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Case_2024-07-03_okta.csv\"\n",
    "case_column = 'CASENUMBER'\n",
    "output_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\output\"\n",
    "\n",
    "check_ticket_ids(file_path, case_column, output_path)\n",
    "#select SCC_Legacy_Ticket_ID__c, CaseNumber from case where SCC_Legacy_Ticket_ID__c ='TTB0000088100' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_1.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_2.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_3.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_4.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_5.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_6.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_7.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_8.csv\n",
      "Saved C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\\output_part_9.csv\n"
     ]
    }
   ],
   "source": [
    "# To spplit the extracted files, the file containing all Legacy ID having non-empty values\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_csv(input_file, output_directory, output_prefix, rows_per_file=200000):\n",
    "    # Read the CSV file with semicolon as the delimiter\n",
    "    df = pd.read_csv(input_file, delimiter=';')\n",
    "\n",
    "    # Ensure the SCC_LEGACY_TICKET_ID__C column is empty\n",
    "    df['SCC_LEGACY_TICKET_ID__C'] = ''\n",
    "\n",
    "    # Get the total number of rows in the DataFrame\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Calculate the number of files needed\n",
    "    num_files = (total_rows // rows_per_file) + (1 if total_rows % rows_per_file != 0 else 0)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for i in range(num_files):\n",
    "        start_row = i * rows_per_file\n",
    "        end_row = start_row + rows_per_file\n",
    "\n",
    "        # Slice the DataFrame to get the chunk\n",
    "        chunk = df.iloc[start_row:end_row]\n",
    "\n",
    "        # Create a filename for the chunk\n",
    "        output_file = os.path.join(output_directory, f\"{output_prefix}_part_{i+1}.csv\")\n",
    "\n",
    "        # Save the chunk to a CSV file, including the header\n",
    "        chunk.to_csv(output_file, index=False, sep=';')\n",
    "\n",
    "        print(f\"Saved {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\extract.csv\"       \n",
    "    output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\file2\"     \n",
    "    output_prefix = 'output'\n",
    "    rows_per_file = 200000                \n",
    "\n",
    "    split_csv(input_file, output_directory, output_prefix, rows_per_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Call Type</th>\n",
       "      <th>record_type</th>\n",
       "      <th>Legacy_ticket_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061614205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62995</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62996</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62997</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62998</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62999</th>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Low</td>\n",
       "      <td>1000</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>TTB0061677200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Status Priority  Call Type     record_type Legacy_ticket_id\n",
       "0      Cancelled      Low       1000  Case Migration    TTB0061614201\n",
       "1      Cancelled      Low       1000  Case Migration    TTB0061614202\n",
       "2      Cancelled      Low       1000  Case Migration    TTB0061614203\n",
       "3      Cancelled      Low       1000  Case Migration    TTB0061614204\n",
       "4      Cancelled      Low       1000  Case Migration    TTB0061614205\n",
       "...          ...      ...        ...             ...              ...\n",
       "62995  Cancelled      Low       1000  Case Migration    TTB0061677196\n",
       "62996  Cancelled      Low       1000  Case Migration    TTB0061677197\n",
       "62997  Cancelled      Low       1000  Case Migration    TTB0061677198\n",
       "62998  Cancelled      Low       1000  Case Migration    TTB0061677199\n",
       "62999  Cancelled      Low       1000  Case Migration    TTB0061677200\n",
       "\n",
       "[63000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\Dummy\\dummy_data_casenumber60_part600.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "#\n",
    "df=df.iloc[:63000]\n",
    "# df = df.drop(columns=['SUBJECT','PRIORITY','STATUS'])\n",
    "df.to_csv(path, index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Account files in server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TesseractNotFoundError",
     "evalue": "tesseract is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:255\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 255\u001b[0m     proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msubprocess_args())\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:969\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    967\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:1438\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1438\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1452\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Perform OCR on the image to extract text\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m extracted_text \u001b[38;5;241m=\u001b[39m \u001b[43mpytesseract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Split the text into lines\u001b[39;00m\n\u001b[0;32m     15\u001b[0m lines \u001b[38;5;241m=\u001b[39m extracted_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:423\u001b[0m, in \u001b[0;36mimage_to_string\u001b[1;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBYTES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDICT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:426\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    424\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[0;32m    425\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[1;32m--> 426\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    427\u001b[0m }[output_type]()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:288\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[0;32m    278\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    286\u001b[0m     }\n\u001b[1;32m--> 288\u001b[0m     run_tesseract(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytesseract\\pytesseract.py:260\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n",
      "\u001b[1;31mTesseractNotFoundError\u001b[0m: tesseract is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your image file\n",
    "image_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\WhatsApp Image 2024-07-05 at 15.51.41_53d1f445.jpg\"\n",
    "\n",
    "# Load the image using PIL\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Perform OCR on the image to extract text\n",
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Split the text into lines\n",
    "lines = extracted_text.split('\\n')\n",
    "\n",
    "# Filter out non-CSV lines and sort them in ascending order\n",
    "csv_files = sorted([line for line in lines if 'csv' in line])\n",
    "\n",
    "# Create a DataFrame to display the sorted list of CSV files\n",
    "csv_files_df = pd.DataFrame(csv_files, columns=['CSV Files'])\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "sorted_excel_file_path = \"sorted_csv_file_names.xlsx\"\n",
    "csv_files_df.to_excel(sorted_excel_file_path, index=False)\n",
    "\n",
    "print(f\"Excel file with sorted CSV names saved at: {sorted_excel_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the Legacy Ticket Id into blank or NuLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>CASENUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500Mg000005shaOIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500Mg000005shaPIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500Mg000005shaQIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500Mg000005shaRIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500Mg000005shaSIAQ</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000122843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID SCC_LEGACY_TICKET_ID__C     CASENUMBER\n",
       "0  500Mg000005shaOIAQ                          TTB0000122839\n",
       "1  500Mg000005shaPIAQ                          TTB0000122840\n",
       "2  500Mg000005shaQIAQ                          TTB0000122841\n",
       "3  500Mg000005shaRIAQ                          TTB0000122842\n",
       "4  500Mg000005shaSIAQ                          TTB0000122843"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_legacynonemppty77_cleaned2.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(path, delimiter=';')\n",
    "df['SCC_LEGACY_TICKET_ID__C']=''\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_legacynonemppty77_cleaned2.csv\",index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SCC_LEGACY_TICKET_ID__C</th>\n",
       "      <th>CASENUMBER</th>\n",
       "      <th>case_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500Mg000005sZYzIAM</td>\n",
       "      <td></td>\n",
       "      <td>TTB0000088100</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID SCC_LEGACY_TICKET_ID__C     CASENUMBER case_origin\n",
       "0  500Mg000005sZYzIAM                          TTB0000088100           X"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_legacynonempty_1line.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "\n",
    "df['case_origin']='X'\n",
    "df['SCC_LEGACY_TICKET_ID__C']=''\n",
    "df_cleaned_specific = df.dropna(subset=['ID'])\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\cleaned_prod_1line.csv\",index=False)\n",
    "df_cleaned_specific.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bricare_20200806_20240430_0_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_0_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_10400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_10900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_11000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_1900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_1900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_2900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_2900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_3900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_3900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_4900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_4900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_5900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_5900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_6900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_6900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_7900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_7900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_8900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_8900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9000001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9000001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_900001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9100001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9100001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9200001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9200001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9300001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9300001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9400001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9400001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9500001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9500001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9600001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9600001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9700001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9700001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9800001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9800001_bricare_case_account_person_account.csv\n",
      "bricare_20200806_20240430_9900001_bricare_case_account_business.csv\n",
      "bricare_20200806_20240430_9900001_bricare_case_account_person_account.csv\n",
      "Extracted filenames saved to extracted_filenames.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\list_acount.txt\"\n",
    "\n",
    "# Regular expression pattern to match the filenames\n",
    "pattern = re.compile(r'bricare_\\d{8}_\\d{8}_\\d+_bricare_case_account_\\w+\\.csv')\n",
    "\n",
    "# List to store the extracted filenames\n",
    "filenames = []\n",
    "\n",
    "# Read the file and extract filenames\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        matches = pattern.findall(line)\n",
    "        filenames.extend(matches)\n",
    "\n",
    "# Display the extracted filenames\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "\n",
    "# Optionally, save the filenames to a new file\n",
    "output_path = 'extracted_filenames.txt'\n",
    "with open(output_path, 'w') as output_file:\n",
    "    for filename in filenames:\n",
    "        output_file.write(f\"{filename}\\n\")\n",
    "\n",
    "print(f\"Extracted filenames saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected emails: ['john.doe@gmail.com', 'invalid-email.com', 'jane.doe@yahoo.com', 'test@invalid-domain.com', 'hilmi@gmail.com master']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import dns.resolver\n",
    "\n",
    "def is_valid_email(email):\n",
    "    regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    if re.match(regex, email) is None:\n",
    "        return False\n",
    "    \n",
    "    domain = email.split('@')[1]\n",
    "    try:\n",
    "        dns.resolver.resolve(domain, 'MX')\n",
    "    except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN, dns.resolver.Timeout):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def correct_email(email):\n",
    "    corrections = {\n",
    "        'gmial.com': 'gmail.com',\n",
    "        'gamil.com': 'gmail.com',\n",
    "        'yahooo.com': 'yahoo.com',\n",
    "        'hotmial.com': 'hotmail.com',\n",
    "        'hotmal.com': 'hotmail.com',\n",
    "    }\n",
    "    \n",
    "    if '@' not in email:\n",
    "        return email\n",
    "    \n",
    "    local_part, domain = email.split('@')\n",
    "    \n",
    "    # Common typo corrections for domains\n",
    "    if domain in corrections:\n",
    "        domain = corrections[domain]\n",
    "    \n",
    "    # Repeated characters correction (e.g., \"john..doe@gmail.com\")\n",
    "    local_part = re.sub(r'\\.{2,}', '.', local_part)\n",
    "    \n",
    "    corrected_email = f\"{local_part}@{domain}\"\n",
    "    \n",
    "    if is_valid_email(corrected_email):\n",
    "        return corrected_email\n",
    "    else:\n",
    "        return email\n",
    "\n",
    "# Example usage\n",
    "emails = [\n",
    "    \"john..doe@gmial.com\",\n",
    "    \"invalid-email.com\",\n",
    "    \"jane.doe@yahooo.com\",\n",
    "    \"test@invalid-domain.com\",\n",
    "    \"hilmi@gmail.com master\"\n",
    "]\n",
    "\n",
    "corrected_emails = [correct_email(email) for email in emails]\n",
    "print(\"Corrected emails:\", corrected_emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Compare the Queue Name and Excalation Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Compare the Queue Name and Excalation Team\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file_path = '/mnt/data/(FINAL) SLA&OLA_NewUserGrouping_Ringkasan (3).xlsx'\n",
    "excel_data = pd.read_excel(excel_file_path, sheet_name='Appendix Team Name')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = '/mnt/data/extract_Que.csv'\n",
    "extract_que_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "appendix_names = excel_data['Full Name / Queue Name'].dropna().unique()\n",
    "extract_names = extract_que_df['NAME'].dropna().unique()\n",
    "\n",
    "# Find matches\n",
    "matches = {name: get_close_matches(name, extract_names, n=1, cutoff=0.6) for name in appendix_names}\n",
    "\n",
    "# Filter out names without matches\n",
    "matches = {k: v for k, v in matches.items() if v}\n",
    "\n",
    "# Convert matches to a DataFrame for better readability\n",
    "matches_df = pd.DataFrame([(k, v[0]) for k, v in matches.items()], columns=['Appendix Team Name', 'Extract Name'])\n",
    "\n",
    "# Define a function to describe the differences between two strings\n",
    "def describe_difference(str1, str2):\n",
    "    if str1 == str2:\n",
    "        return \"Exact match\"\n",
    "    differences = []\n",
    "    if len(str1) != len(str2):\n",
    "        differences.append(\"Different lengths\")\n",
    "    if str1.strip() != str2.strip():\n",
    "        differences.append(\"Whitespace differences\")\n",
    "    for i, (char1, char2) in enumerate(zip(str1, str2)):\n",
    "        if char1 != char2:\n",
    "            differences.append(f\"Different character at position {i}: '{char1}' vs '{char2}'\")\n",
    "    return \"; \".join(differences) if differences else \"Other differences\"\n",
    "\n",
    "# Add a column describing the differences\n",
    "matches_df['Difference Details'] = matches_df.apply(lambda row: describe_difference(row['Appendix Team Name'], row['Extract Name']), axis=1)\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Matched Names with Differences\", dataframe=matches_df)\n",
    "\n",
    "matches_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\Matched_Names_with_Differences.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df.to_excel('match_queue_name_vs_escalation_name.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all error logs into 2 different files: Business and Person Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business records saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\account_business_records.csv\n",
      "Person Account records saved to: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\account_person_account_records.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\"\n",
    "\n",
    "\n",
    "all_files = os.listdir(folder_path)\n",
    "\n",
    "\n",
    "error_files = [f for f in all_files if \"error\" in f]\n",
    "\n",
    "\n",
    "business_data = pd.DataFrame()\n",
    "person_account_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "for file_name in error_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    data = pd.read_csv(file_path, delimiter=';')\n",
    "    \n",
    "\n",
    "    data['Nama'] = data['Nama'].apply(lambda x: x[-250:] if isinstance(x, str) else x)\n",
    "    \n",
    "    # combine the columns \"Email\" and \"no telp\"\n",
    "    data['Description'] = data[['No_telp', 'Email']].apply(lambda x: ' | '.join(x.dropna().astype(str)), axis=1)\n",
    "    \n",
    "\n",
    "    data = data.drop(columns=['No_telp', 'Email'])\n",
    "    \n",
    "\n",
    "    business_data = pd.concat([business_data, data[data['record_type'] == 'Business']])\n",
    "    person_account_data = pd.concat([person_account_data, data[data['record_type'] == 'Person Account']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "business_file_path = os.path.join(folder_path, 'account_business_records.csv')\n",
    "person_account_file_path = os.path.join(folder_path, 'account_person_account_records.csv')\n",
    "\n",
    "\n",
    "business_data.to_csv(business_file_path, index=False)\n",
    "person_account_data.to_csv(person_account_file_path, index=False)\n",
    "\n",
    "print(f\"Business records saved to: {business_file_path}\")\n",
    "print(f\"Person Account records saved to: {person_account_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To check and analyze the Account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def detect_delimiter(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        header = file.readline()\n",
    "        for delimiter in [',', ';', '\\t', '|']:\n",
    "            if delimiter in header:\n",
    "                return delimiter\n",
    "    return None\n",
    "\n",
    "def analyze_file(file_path, expected_delimiter):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=expected_delimiter)\n",
    "        line_number = 0\n",
    "        inconsistent_lines = []\n",
    "        for row in reader:\n",
    "            line_number += 1\n",
    "            if len(row) != len(reader.fieldnames):\n",
    "                inconsistent_lines.append(line_number)\n",
    "        return inconsistent_lines\n",
    "\n",
    "def main():\n",
    "    file_paths = [\"path_to_your_first_csv_file.csv\", \"path_to_your_second_csv_file.csv\"]  \n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        detected_delimiter = detect_delimiter(file_path)\n",
    "        if detected_delimiter:\n",
    "            print(f\"Detected delimiter for {file_path}: {detected_delimiter}\")\n",
    "            inconsistent_lines = analyze_file(file_path, detected_delimiter)\n",
    "            if inconsistent_lines:\n",
    "                print(f\"Inconsistent lines in {file_path}: {inconsistent_lines}\")\n",
    "            else:\n",
    "                print(f\"No inconsistencies found in {file_path}\")\n",
    "        else:\n",
    "            print(f\"Could not detect delimiter for {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558030.csv\n",
      "Detected delimiter for C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558030.csv: ;\n",
      "Inconsistent lines in C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558030.csv: [1]\n",
      "Processing file: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558031.csv\n",
      "Detected delimiter for C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558031.csv: ;\n",
      "Inconsistent lines in C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\\error071124083558031.csv: [2, 3]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def detect_delimiter(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        header = file.readline()\n",
    "        for delimiter in [',', ';', '\\t', '|']:\n",
    "            if delimiter in header:\n",
    "                return delimiter\n",
    "    return None\n",
    "\n",
    "def analyze_file(file_path, expected_delimiter):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=expected_delimiter)\n",
    "        line_number = 0\n",
    "        inconsistent_lines = []\n",
    "        field_count = len(next(reader))\n",
    "        for row in reader:\n",
    "            line_number += 1\n",
    "            if len(row) != field_count:\n",
    "                inconsistent_lines.append(line_number)\n",
    "        return inconsistent_lines\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                detected_delimiter = detect_delimiter(file_path)\n",
    "                if detected_delimiter:\n",
    "                    print(f\"Detected delimiter for {file_path}: {detected_delimiter}\")\n",
    "                    inconsistent_lines = analyze_file(file_path, detected_delimiter)\n",
    "                    if inconsistent_lines:\n",
    "                        print(f\"Inconsistent lines in {file_path}: {inconsistent_lines}\")\n",
    "                    else:\n",
    "                        print(f\"No inconsistencies found in {file_path}\")\n",
    "                else:\n",
    "                    print(f\"Could not detect delimiter for {file_path}\")\n",
    "\n",
    "def main():\n",
    "    folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\test\"\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_folder(folder_path)\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\error071124083558031.csv\"\n",
    "output_file = r\"c:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\\error071124083558030_output.csv\"\n",
    "\n",
    "# Read the input CSV file\n",
    "with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile, delimiter=';')\n",
    "    rows = list(reader)\n",
    "\n",
    "# Process the rows to add double quotes where necessary\n",
    "processed_rows = []\n",
    "for row in rows:\n",
    "    if row == rows[0]:\n",
    "        # Keep the header row as is\n",
    "        processed_rows.append(row)\n",
    "    else:\n",
    "        # Add double quotes around non-empty values\n",
    "        processed_row = ['' if value == '\"\"' else f'\"{value}\"' if value else '' for value in row]\n",
    "        processed_rows.append(processed_row)\n",
    "\n",
    "# Write the processed rows to the output CSV file\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter=';')\n",
    "    writer.writerows(processed_rows)\n",
    "\n",
    "print(\"CSV file has been processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of CSV files saved to csv_shapes.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_shapes_to_excel(folder_path, output_file):\n",
    "    data = []\n",
    "\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        if filename.endswith('.csv'):\n",
    "     \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "      \n",
    "            df = pd.read_csv(file_path)\n",
    "    \n",
    "            rows, columns = df.shape\n",
    "         \n",
    "            data.append([filename, rows, columns])\n",
    "\n",
    " \n",
    "    df_shapes = pd.DataFrame(data, columns=['Name', 'Rows', 'Columns'])\n",
    "\n",
    "\n",
    "    df_shapes.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Shapes of CSV files saved to {output_file}\")\n",
    "\n",
    "\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\account\"\n",
    "output_file = 'csv_shapes.xlsx'\n",
    "\n",
    "\n",
    "save_shapes_to_excel(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\BRI - User Matrix v1.2.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Role2')\n",
    "\n",
    "df['']\n",
    "\n",
    "df.to_csv('Role2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ATM BANK LAIN', 'Aplikasi', 'EDC', 'CRM', 'ATM', 'ATM BRI'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "\n",
    "df= pd.read_csv(path)\n",
    "\n",
    "df['Chanel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def format_ticket_id(ticket_id):\n",
    "    if pd.notna(ticket_id) and ticket_id.startswith(\"TTB\"):\n",
    "        number_part = ticket_id[3:]\n",
    "        formatted_number = number_part[-10:].zfill(10)\n",
    "        return f\"TTB{formatted_number}\"\n",
    "    return ticket_id\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if \"Ticket_ID\" in df.columns:\n",
    "            df[\"Ticket_ID\"] = df[\"Ticket_ID\"].apply(format_ticket_id)\n",
    "        if \"Legacy_Ticket_ID\" in df.columns:\n",
    "            df[\"Legacy_Ticket_ID\"] = df[\"Legacy_Ticket_ID\"].apply(format_ticket_id)\n",
    "        output_path = file_path.replace(\".csv\", \"_processed.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "        return df\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_all_files_in_folder(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            process_file(file_path)\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\test_case - Copy\"\n",
    "process_all_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 27 - Copy_processed_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed_processed.csv\n",
      "Processed and saved: D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy_processed_processed_processed_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def format_ticket_id(ticket_id):\n",
    "    if ticket_id and ticket_id.startswith(\"TTB\"):\n",
    "        number_part = ticket_id[3:]\n",
    "        formatted_number = number_part[-10:].zfill(10)\n",
    "        return f\"TTB{formatted_number}\"\n",
    "    return ticket_id\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "        \n",
    "\n",
    "        for row in rows:\n",
    "            if \"Ticket_ID\" in row:\n",
    "                row[\"Ticket_ID\"] = format_ticket_id(row[\"Ticket_ID\"])\n",
    "            if \"Legacy_Ticket_ID\" in row:\n",
    "                row[\"Legacy_Ticket_ID\"] = format_ticket_id(row[\"Legacy_Ticket_ID\"])\n",
    "        \n",
    "\n",
    "        output_path = file_path.replace(\".csv\", \"_processed.csv\")\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "        \n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def process_all_files_in_folder(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            process_file(file_path)\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\test_case - Copy\"\n",
    "process_all_files_in_folder(folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>record_type</th>\n",
       "      <th>cif</th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Details</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>...</th>\n",
       "      <th>Tanggal_Settlement</th>\n",
       "      <th>Tgl_Foward</th>\n",
       "      <th>Tgl_In_Progress</th>\n",
       "      <th>Tgl_Returned</th>\n",
       "      <th>Ticket_Referensi</th>\n",
       "      <th>Tiket_Urgency</th>\n",
       "      <th>Tipe_Remark</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>users</th>\n",
       "      <th>Usergroup_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET001339022</td>\n",
       "      <td>Case Migration</td>\n",
       "      <td>STGD 980</td>\n",
       "      <td>TTB001339022</td>\n",
       "      <td>1000</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>Nasabah mengajukan pemblokiran kartu ATM BRI\\n...</td>\n",
       "      <td>2023-01-01 07:07:15</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       case_number     record_type       cif     Ticket_ID  Call_Type_ID  \\\n",
       "0  TICKET001339022  Case Migration  STGD 980  TTB001339022          1000   \n",
       "\n",
       "                              Call_Type  \\\n",
       "0  Blokir Kartu ATM karena kartu hilang   \n",
       "\n",
       "                                             Details          Create_Date  \\\n",
       "0  Nasabah mengajukan pemblokiran kartu ATM BRI\\n...  2023-01-01 07:07:15   \n",
       "\n",
       "  gateway Jenis_Laporan  ... Tanggal_Settlement  Tgl_Foward  Tgl_In_Progress  \\\n",
       "0   Phone       Request  ...                NaN         NaN              NaN   \n",
       "\n",
       "  Tgl_Returned  Ticket_Referensi  Tiket_Urgency  Tipe_Remark UniqueID  users  \\\n",
       "0          NaN               NaN            NaN        Notes      NaN   Call   \n",
       "\n",
       "  Usergroup_ID  \n",
       "0            4  \n",
       "\n",
       "[1 rows x 82 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\test_case - Copy\\dummy_data_case_uat - 79 - Copy.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part1.csv\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "# Collect Ticket_IDs and CSV files for suffix '_79'\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for suffix '_79'\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ';'  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs from TTB000000000001 to the maximum in the current data\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(12)}' for i in range(1, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv_files(input_folder, output_folder):\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Remove the 'Legacy_Ticket_ID' column if it exists\n",
    "            if 'Legacy_Ticket_ID' in data.columns:\n",
    "                data = data.drop(columns=['Legacy_Ticket_ID'])\n",
    "            \n",
    "            # Rename the 'Legacy_ticket_id' column to 'Legacy_Ticket_ID' if it exists\n",
    "            if 'Legacy_ticket_id' in data.columns:\n",
    "                data = data.rename(columns={'Legacy_ticket_id': 'Legacy_Ticket_ID'})\n",
    "            \n",
    "            # Replace all NaN values with empty strings\n",
    "            data = data.fillna('')\n",
    "            \n",
    "            # Remove '.0' at the end of all values\n",
    "            data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
    "            \n",
    "            \n",
    "            # Save the cleaned data to the output folder\n",
    "            cleaned_file_path = os.path.join(output_folder, filename)\n",
    "            data.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Processed and saved: {cleaned_file_path}\")\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "output_folder = r\"D:\\cleaned\"\n",
    "\n",
    "# Call the function to clean all CSV files\n",
    "clean_csv_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check number of rows\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "folder_path = 'path/to/your/folder'\n",
    "\n",
    "# Create a list to store the file details\n",
    "file_details = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Get the number of rows (excluding header)\n",
    "        num_rows = len(df)\n",
    "        # Append the details to the list\n",
    "        file_details.append({'Filename': filename, 'Number of Rows': num_rows})\n",
    "\n",
    "# Create a DataFrame from the list of file details\n",
    "file_details_df = pd.DataFrame(file_details)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file_path = 'file_details.xlsx'\n",
    "file_details_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f'File details saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('yourfile.csv', delimiter=';')\n",
    "\n",
    "# Check for duplicate 'cifno'\n",
    "duplicate_cifno = df.duplicated('cifno')\n",
    "if duplicate_cifno.any():\n",
    "    print(\"There are duplicate 'cifno' values.\")\n",
    "else:\n",
    "    print(\"All 'cifno' values are unique.\")\n",
    "\n",
    "# Fill empty 'Nama' fields with \"No Name\"\n",
    "df['Nama'].fillna('No Name', inplace=True)\n",
    "df['Nama'].replace('', 'No Name', inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('updated_yourfile.csv', index=False, sep=';')\n",
    "\n",
    "print(\"Processing complete. The updated file has been saved as 'updated_yourfile.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_dates_in_files_and_generate_excel(folder_path, output_excel_file):\n",
    "    # List to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):  # assuming the files are in CSV format\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Read the file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if the necessary columns are in the DataFrame\n",
    "            if 'Create_Date' in df.columns and 'TanggalClosed' in df.columns:\n",
    "                # Convert columns to datetime if they are not already\n",
    "                df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "                df['TanggalClosed'] = pd.to_datetime(df['TanggalClosed'], errors='coerce')\n",
    "\n",
    "                # Check if Create_Date is before TanggalClosed\n",
    "                condition = df['Create_Date'] > df['TanggalClosed']\n",
    "                \n",
    "                # Find the rows where the condition is not met\n",
    "                incorrect_rows = df[~condition].index.tolist()\n",
    "                \n",
    "                # Append result for the current file\n",
    "                result = {\n",
    "                    'Filename': filename,\n",
    "                    'All_Dates_Correct': condition.all(),\n",
    "                    'Incorrect_Rows': incorrect_rows  # List of incorrect rows\n",
    "                }\n",
    "                results.append(result)\n",
    "            else:\n",
    "                results.append({\n",
    "                    'Filename': filename,\n",
    "                    'All_Dates_Correct': False,\n",
    "                    'Incorrect_Rows': 'Missing required columns'\n",
    "                })\n",
    "    \n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    with pd.ExcelWriter(output_excel_file) as writer:\n",
    "        results_df.to_excel(writer, index=False, sheet_name='Summary')\n",
    "        \n",
    "        for result in results:\n",
    "            if result['All_Dates_Correct'] is False and isinstance(result['Incorrect_Rows'], list):\n",
    "                # Write the detailed incorrect rows for each file\n",
    "                incorrect_df = df.iloc[result['Incorrect_Rows']]\n",
    "                incorrect_df.to_excel(writer, sheet_name=result['Filename'], index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define the folder path and output Excel file\n",
    "folder_path = r\"D:\\cleaned\"\n",
    "output_excel_file = 'date_check_results.xlsx'\n",
    "check_dates_in_files_and_generate_excel(folder_path, output_excel_file)\n",
    "\n",
    "print(f\"Results have been saved to {output_excel_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def move_files_by_date(source_folder, destination_folder, start_date):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Get the creation date of the file\n",
    "            file_create_date = datetime.fromtimestamp(os.path.getctime(file_path))\n",
    "            # Format the creation date to match the given format\n",
    "            formatted_date = file_create_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            if datetime.strptime(formatted_date, '%Y-%m-%d %H:%M:%S') >= start_date:\n",
    "                shutil.move(file_path, destination_folder)\n",
    "                print(f'Moved: {file_path} to {destination_folder}')\n",
    "\n",
    "# Path of the folder to scan\n",
    "source_folder = r'path\\to\\source\\folder'\n",
    "# Path of the destination folder\n",
    "destination_folder = r'path\\to\\destination\\folder'\n",
    "\n",
    "# Start date\n",
    "start_date = datetime.strptime('2022-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "move_files_by_date(source_folder, destination_folder, start_date)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

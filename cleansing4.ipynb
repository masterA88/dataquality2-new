{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "input_directory  = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "dummy_directory = r\"D:\\output\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  \n",
    "\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_dummy_files(dummy_directory, delimiter):\n",
    "    dummy_row = None\n",
    "    for file in os.listdir(dummy_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(dummy_directory, file), delimiter=delimiter)\n",
    "            if dummy_row is None:\n",
    "                dummy_row = df.iloc[0].to_dict()\n",
    "    return dummy_row\n",
    "\n",
    "\n",
    "dummy_row = read_dummy_files(dummy_directory, delimiter)\n",
    "\n",
    "\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "\n",
    "all_ticket_ids_27 = set()\n",
    "all_ticket_ids_79 = set()\n",
    "csv_files_27 = []\n",
    "csv_files_79 = []\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith('_27.csv'):\n",
    "        csv_files_27.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_27.update(df['Ticket_ID'].unique())\n",
    "    elif file.endswith('_79.csv'):\n",
    "        csv_files_79.append(file)\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "        all_ticket_ids_79.update(df['Ticket_ID'].unique())\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    all_ticket_ids = sorted(all_ticket_ids)\n",
    "    min_id = int(all_ticket_ids[0][3:])  \n",
    "    max_id = int(all_ticket_ids[-1][3:])  \n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    \n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "   \n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = dummy_row.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  \n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "   \n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "   \n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    \n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  \n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    \n",
    "    combined_df.fillna('', inplace=True)\n",
    "\n",
    "    \n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(file).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27')\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79')\n",
    "\n",
    "\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv_files(input_folder, output_folder):\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Remove the 'Legacy_Ticket_ID' column if it exists\n",
    "            if 'Legacy_Ticket_ID' in data.columns:\n",
    "                data = data.drop(columns=['Legacy_Ticket_ID'])\n",
    "            \n",
    "            # Rename the 'Legacy_ticket_id' column to 'Legacy_Ticket_ID' if it exists\n",
    "            if 'Legacy_ticket_id' in data.columns:\n",
    "                data = data.rename(columns={'Legacy_ticket_id': 'Legacy_Ticket_ID'})\n",
    "            \n",
    "            # Replace all NaN values with empty strings\n",
    "            data = data.fillna('')\n",
    "            \n",
    "            # Remove '.0' at the end of all values\n",
    "            data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
    "            \n",
    "            \n",
    "            # Save the cleaned data to the output folder\n",
    "            cleaned_file_path = os.path.join(output_folder, filename)\n",
    "            data.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Processed and saved: {cleaned_file_path}\")\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "output_folder = r\"D:\\cleaned\"\n",
    "\n",
    "# Call the function to clean all CSV files\n",
    "clean_csv_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To cleanse the detail Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove BOM from each line\n",
    "    lines = [line.replace('\\ufeff', '') for line in lines]\n",
    "\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_ticket_id = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('TTB'):\n",
    "            if current_entry:\n",
    "                entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "                current_entry = []\n",
    "        \n",
    "            parts = line.split(',', 3)\n",
    "            if len(parts) > 3:\n",
    "                current_ticket_id = parts[0]\n",
    "                current_entry.append(parts[3].strip())\n",
    "            continue\n",
    "        current_entry.append(line.strip())\n",
    "\n",
    "    if current_entry:\n",
    "        entries.append((current_ticket_id, '\\n'.join(current_entry)))\n",
    "\n",
    "    return entries\n",
    "\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare_case_januari2023_2_details.csv\"\n",
    "processed_data = process_csv_data(file_path)\n",
    "\n",
    "df_final = pd.DataFrame(processed_data, columns=['Ticket ID', 'Details'])\n",
    "\n",
    "\n",
    "if df_final.iloc[0]['Ticket ID'] and df_final.iloc[0]['Details'].startswith(df_final.iloc[0]['Ticket ID']):\n",
    "    df_final.at[0, 'Details'] = df_final.iloc[0]['Details'][len(df_final.iloc[0]['Ticket ID'])+2:]\n",
    "\n",
    "\n",
    "df_final = df_final.iloc[:10]\n",
    "output_path = \"details_20230101_20230101.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#just take 10 lines for an example\n",
    "path=r\"D:\\dataquality\\bricare_20230101_20230101.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.iloc[:10].to_csv(path,index=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path_1 = r\"D:\\dataquality2\\bricare_uat20230101_20230101.csv\"\n",
    "file_path_2 = r\"D:\\dataquality2\\details_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "df_tenline_bricare = pd.read_csv(file_path_1)\n",
    "df_detail_bricare_10line = pd.read_csv(file_path_2)\n",
    "\n",
    "df_detail_bricare_10line.columns = ['Ticket_ID', 'Details']\n",
    "\n",
    "merged_df = pd.merge(df_tenline_bricare, df_detail_bricare_10line, on='Ticket_ID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r\"D:\\dataquality2\\bricare_uat_20230101_20230101.csv\"\n",
    "\n",
    "\n",
    "column_to_move=\"Details\"\n",
    "merged_df = merged_df[[col for col in merged_df if col != column_to_move][:3] + [column_to_move] + [col for col in merged_df if col != column_to_move][3:]] \n",
    "\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_save_csv_files(source_directory, destination_directory, summary_file_path):\n",
    "    s\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "    \n",
    "    \n",
    "    files = os.listdir(source_directory)\n",
    "    \n",
    "    # Filter for CSV files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "    \n",
    "    summary_data = []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "       \n",
    "        source_file_path = os.path.join(source_directory, csv_file)\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv(source_file_path)\n",
    "        \n",
    "      \n",
    "        destination_file_path = os.path.join(destination_directory, csv_file)\n",
    "        \n",
    "       \n",
    "        df.to_csv(destination_file_path, index=False)\n",
    "        print(f\"Saved {csv_file} to {destination_file_path}\")\n",
    "        \n",
    "       \n",
    "        summary_data.append({'Filename': csv_file, 'Total Rows': len(df)})\n",
    "    \n",
    "   \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "   \n",
    "    summary_df.to_excel(summary_file_path, index=False)\n",
    "    print(f\"Summary file saved to {summary_file_path}\")\n",
    "\n",
    "\n",
    "source_directory = \n",
    "destination_directory = \n",
    "summary_file_path = \n",
    "\n",
    "read_and_save_csv_files(source_directory, destination_directory, summary_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_14408\\2533492479.py:26: DtypeWarning: Columns (17,47,53,56,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to D:\\Python2\\case_2024\\30 july- 4 aug\\to upload\\clean\\bricare_20240730_20240804_0_79_closed - bricare_20240730_20240804_0_79_closed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_14408\\2533492479.py:26: DtypeWarning: Columns (8,17,21,23,36,47,53,56,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to D:\\Python2\\case_2024\\30 july- 4 aug\\to upload\\clean\\bricare_20240730_20240804_0_79_open - bricare_20240730_20240804_0_79_open.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_decimal_zero(val):\n",
    "    if isinstance(val, float) and val.is_integer():\n",
    "        return str(int(val))\n",
    "    elif isinstance(val, str):\n",
    "        try:\n",
    "            float_val = float(val)\n",
    "            if float_val.is_integer():\n",
    "                return str(int(float_val))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if val.endswith('.0'):\n",
    "            return val[:-2]\n",
    "    return val\n",
    "\n",
    "def process_files(input_folder, output_folder):\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Apply the function to each column of the DataFrame\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].apply(remove_decimal_zero)\n",
    "            \n",
    "            # Save the cleaned DataFrame to the output folder\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"Processed file saved to {output_path}\")\n",
    "\n",
    "# Define your input and output folders\n",
    "input_folder = r\"D:\\Python2\\case_2024\\30 july- 4 aug\\to upload\\clean\"\n",
    "output_folder =  r\"D:\\Python2\\case_2024\\30 july- 4 aug\\to upload\\clean\"\n",
    "# Process the files\n",
    "process_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove E+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_14408\\941130917.py:30: DtypeWarning: Columns (17,47,53,56,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_14408\\941130917.py:33: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(convert_sci_notation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete for file: bricare_20240730_20240804_0_79_closed - bricare_20240730_20240804_0_79_closed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_14408\\941130917.py:30: DtypeWarning: Columns (8,17,21,23,36,47,53,56,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_14408\\941130917.py:33: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(convert_sci_notation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete for file: bricare_20240730_20240804_0_79_open - bricare_20240730_20240804_0_79_open.csv\n",
      "All files have been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder containing the CSV files\n",
    "input_folder = r\"D:\\Python2\\case_2024\\30 july- 4 aug\\to upload\\clean\"\n",
    "output_folder = r\"D:\\Python2\\case_2024\\30 july- 4 aug\\to upload\\clean\"\n",
    "\n",
    "# Function to convert scientific notation to normal text\n",
    "def convert_sci_notation(value):\n",
    "    try:\n",
    "        # Check if the value is a number and in scientific notation\n",
    "        if 'E' in str(value) or 'e' in str(value):\n",
    "            return '{:.0f}'.format(float(value))\n",
    "        else:\n",
    "            return value\n",
    "    except:\n",
    "        return value\n",
    "\n",
    "# Ensure the output folder exists\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each CSV file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        \n",
    "        # Step 1: Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Step 2: Convert all values in the DataFrame from scientific notation to normal text\n",
    "        df = df.applymap(convert_sci_notation)\n",
    "        \n",
    "        # Step 3: Save the converted data back to a CSV file\n",
    "        df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"Conversion complete for file: {filename}\")\n",
    "\n",
    "print(\"All files have been processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb0 in position 37: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     15\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file)\n\u001b[1;32m---> 16\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m max_columns:\n\u001b[0;32m     18\u001b[0m         max_columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb0 in position 37: invalid start byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing CSV files (in this example, it's the current directory)\n",
    "folder_path = r\"D:\\Case\\case_2024\\M--\\Case_2024_Clean_1\"\n",
    "\n",
    "# Initialize variables\n",
    "max_columns = 0\n",
    "main_columns = None\n",
    "dataframes = []\n",
    "\n",
    "# First pass: Identify the file with the maximum columns\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df.shape[1] > max_columns:\n",
    "            max_columns = df.shape[1]\n",
    "            main_columns = df.columns\n",
    "\n",
    "# Second pass: Read and align all files to the main columns\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Align to main columns\n",
    "        df = df.reindex(columns=main_columns)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "output_file = 'merged_file.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All files merged successfully into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To check the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,36,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,17,30,33,36,47,49,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,46,47,51,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,31,46,47,51,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,31,46,47,51,53,59,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,31,33,47,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,47,59,63,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (15,36,46,47,51,53,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_30204\\3203062803.py:19: DtypeWarning: Columns (9,15,31,33,45,46,47,48,50,51,53,59,62,63,75,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File details saved to file_details_dase_2024_Clean_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "# to check number of rows\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "# folder_path = r\"D:\\cleaned\\file_dummy_all-20240720T225819Z-001\\file_dummy_all\"\n",
    "# folder_path = r\"D:\\Case\\case_2024\\M--\\Case_2024_Clean_1\"\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\bricare_2024_data_clean\"\n",
    "\n",
    "\n",
    "# Create a list to store the file details\n",
    "file_details = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "      \n",
    "        df = pd.read_csv(file_path)\n",
    "        # Get the number of rows (excluding header)\n",
    "        num_rows = len(df)\n",
    "        # Append the details to the list\n",
    "        file_details.append({'Filename': filename, 'Number of Rows': num_rows})\n",
    "\n",
    "\n",
    "file_details_df = pd.DataFrame(file_details)\n",
    "\n",
    "\n",
    "output_file_path = 'file_details_dase_2024_Clean_2.xlsx'\n",
    "file_details_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f'File details saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To check the number of rows 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,36,37,47,53,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,31,36,37,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (36,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,17,31,36,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,31,36,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (9,15,22,25,31,33,36,37,40,45,46,47,50,51,53,62,63,75,77,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,25,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,25,31,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,17,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,17,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,36,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,17,25,31,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (15,25,31,37,46,47,51,63,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (25,31,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (8,15,36,37,46,47,51,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_23984\\1385300087.py:15: DtypeWarning: Columns (31,36,37,47,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File details saved to file_details_case_2021_Clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "folder_path = r\"D:\\Case\\case_2021\"\n",
    "\n",
    "# Create a list to store the file details\n",
    "file_details = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin1')  # or encoding='iso-8859-1'\n",
    "            # Get the number of rows (excluding header)\n",
    "            num_rows = len(df)\n",
    "            # Append the details to the list\n",
    "            file_details.append({'Filename': filename, 'Number of Rows': num_rows})\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "file_details_df = pd.DataFrame(file_details)\n",
    "\n",
    "output_file_path = 'file_details_case_2021_Clean.xlsx'\n",
    "file_details_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f'File details saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To join all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been combined successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "\n",
    "\n",
    "files_27 = []\n",
    "files_79 = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        if \"_27\" in filename:\n",
    "            files_27.append(os.path.join(directory, filename))\n",
    "        elif \"_79\" in filename:\n",
    "            files_79.append(os.path.join(directory, filename))\n",
    "\n",
    "\n",
    "def concatenate_files(file_list, output_file):\n",
    "    combined_df = pd.concat([pd.read_csv(file) for file in file_list])\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "concatenate_files(files_27, os.path.join(directory, 'combined_27.csv'))\n",
    "concatenate_files(files_79, os.path.join(directory, 'combined_79.csv'))\n",
    "\n",
    "print(\"Files have been combined successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To exlcude invalid TTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and replaced bricare_20240501_20240720_0_field_dummy_new.csv\n",
      "Filtered and replaced bricare_20240501_20240720_1200001_field_dummy_new.csv\n",
      "Filtered and replaced bricare_20240501_20240720_300001_field_dummy_new.csv\n",
      "Filtered and replaced bricare_20240501_20240720_600001_field_dummy_new.csv\n",
      "Filtered and replaced bricare_20240501_20240720_900001_field_dummy_new.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\cleaned\\file_dummy_all_new\"\n",
    "\n",
    "\n",
    "# Define a function to filter the DataFrame\n",
    "def filter_ticket_id(df):\n",
    "    return df[df['Ticket_ID'].str.contains('TTB', na=False)]\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Assuming the files are CSVs\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        # Filter the DataFrame\n",
    "        filtered_df = filter_ticket_id(df)\n",
    "        \n",
    "        # Overwrite the original file with the filtered DataFrame\n",
    "        filtered_df.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f'Filtered and replaced {filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To correct TTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed combines_ticketID.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def correct_ticket_id(ticket_id):\n",
    "    if pd.isna(ticket_id):  # Handle NaN values\n",
    "        return ticket_id\n",
    "    ticket_id = str(ticket_id)\n",
    "    prefix = \"TTB\"\n",
    "    if ticket_id.startswith(prefix):\n",
    "        number_part = ticket_id[len(prefix):]\n",
    "        corrected_number_part = number_part.zfill(12)\n",
    "        return prefix + corrected_number_part\n",
    "    return ticket_id\n",
    "\n",
    "def process_files_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if 'Ticket_ID' in df.columns:\n",
    "                df['Ticket_ID'] = df['Ticket_ID'].apply(correct_ticket_id)\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"Processed {filename}\")\n",
    "\n",
    "folder_path = r\"D:\\Python2\\case_2024\\bricare_all_closed_batch2_2024\\New folder\"\n",
    "process_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_14408\\1892786993.py:7: DtypeWarning: Columns (17,47,53,56,59,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80935"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"D:\\Python2\\case_2024\\30 july- 4 aug\\to upload\\clean\\bricare_20240730_20240804_0_79_closed - bricare_20240730_20240804_0_79_closed.csv\"\n",
    "\n",
    "# Read the CSV file with the correct delimiter and handling multiline text\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the dataframe to check the content\n",
    "# tools.display_dataframe_to_user(name=\"CSV Data with Semicolon Delimiter\", dataframe=data)\n",
    "\n",
    "# Show the first few rows to understand the structure\n",
    "data.head()\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPython2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcase_2024\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m30 july- 4 aug\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcorrected TTB\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbricare_20240111_20240729_mod_less_4agustus_0_79_closed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path=r\"D:\\Python2\\case_2024\\30 july- 4 aug\\corrected TTB\\bricare_20240111_20240729_mod_less_4agustus_0_79_closed.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To filter out 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test\"\n",
    "\n",
    "# List all files in the folder\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "  \n",
    "    df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "\n",
    "\n",
    "    filtered_df = df[df['Create_Date'].dt.year == 2024]\n",
    "\n",
    "    filtered_file_path = os.path.join(folder_path, f'filtered_{file}')\n",
    "    filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "    print(f'Filtered data saved to {filtered_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To fill in Gap tickets to fill correctly gaps between the Ticket_IDs across different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the directories\n",
    "# input_directory = r\"D:\\cleaned\\file_dummy_all\"\n",
    "# output_directory = r\"D:\\cleaned\\output_cleaned_dummy\"\n",
    "\n",
    "# # Define the delimiter\n",
    "# delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# # Collect all Ticket_IDs from existing files\n",
    "# def collect_ticket_ids(input_directory, delimiter):\n",
    "#     ticket_ids = set()\n",
    "#     csv_files = []\n",
    "\n",
    "#     for file in os.listdir(input_directory):\n",
    "#         if file.endswith('.csv'):\n",
    "#             csv_files.append(file)\n",
    "#             df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "#             ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "#     return sorted(ticket_ids), csv_files\n",
    "\n",
    "# all_ticket_ids, csv_files = collect_ticket_ids(input_directory, delimiter)\n",
    "\n",
    "# def process_files(csv_files, all_ticket_ids, delimiter, output_directory):\n",
    "#     # Generate the full range of Ticket_IDs\n",
    "#     min_id = int(all_ticket_ids[0][3:])\n",
    "#     max_id = int(all_ticket_ids[-1][3:])\n",
    "#     full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "#     # Identify missing Ticket_IDs\n",
    "#     missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "#     # Define the fixed values for the gap rows\n",
    "#     fixed_values = {\n",
    "#         'Call_Type_ID': 1000,\n",
    "#         'Create_Date': '1/1/1999',\n",
    "#         'status': 'Cancelled',\n",
    "#         'TanggalClosed': '1/1/1999',\n",
    "#         'record_type': 'Case Migration',\n",
    "#         'Priority': 'High (Urgent/Critical)'\n",
    "#     }\n",
    "\n",
    "#     # Create placeholder rows for missing IDs\n",
    "#     missing_rows = []\n",
    "#     for ticket_id in missing_ids:\n",
    "#         placeholder_row = fixed_values.copy()\n",
    "#         placeholder_row['Ticket_ID'] = ticket_id\n",
    "#         placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "#         missing_rows.append(placeholder_row)\n",
    "#     missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "#     # Read and combine all files\n",
    "#     combined_df = pd.DataFrame()\n",
    "#     for file in csv_files:\n",
    "#         df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "#         combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "#     # Add missing rows to the combined dataframe\n",
    "#     combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "#     combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "#     # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "#     combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "#     # Ensure the \"Priority\" column is added and filled\n",
    "#     combined_df['Priority'] = 'High (Urgent/Critical)'\n",
    "\n",
    "#     # Remove .0 from all values except nominal column\n",
    "#     for col in combined_df.columns:\n",
    "#         if col != 'Nominal':  # Replace 'Nominal' with the actual name of your nominal column\n",
    "#             combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "#     # Replace NaN with empty strings\n",
    "#     combined_df.fillna('', inplace=True)\n",
    "#     combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "#     # Split combined dataframe into chunks of 300,000 rows\n",
    "#     chunk_size = 300000\n",
    "#     num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "#     base_filename = os.path.basename(csv_files[0]).split('_part')[0]\n",
    "#     for i in range(num_chunks):\n",
    "#         chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "#         output_filename = os.path.join(output_directory, f'{base_filename}_part{i + 1}.csv')\n",
    "#         chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "#     print(f\"Processing completed. Files saved to {output_directory}\")\n",
    "\n",
    "# # Process the files\n",
    "# process_files(csv_files, all_ticket_ids, delimiter, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for file 'D:\\cleaned\\test\\bricare_20240501_20240720_0_field_dummy_new.csv' completed.\n",
      "Processing for file 'D:\\cleaned\\test\\bricare_20240501_20240720_1200001_field_dummy_new.csv' completed.\n",
      "Processing for file 'D:\\cleaned\\test\\bricare_20240501_20240720_300001_field_dummy_new.csv' completed.\n",
      "Processing for file 'D:\\cleaned\\test\\bricare_20240501_20240720_600001_field_dummy_new.csv' completed.\n",
      "Processing for file 'D:\\cleaned\\test\\bricare_20240501_20240720_900001_field_dummy_new.csv' completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bricare_20190927_20240430_12300001_field_dummy_part3',\n",
       " 'bricare_20190927_20240430_12300001_field_dummy_part3.zip',\n",
       " 'bricare_20190927_20240430_21300001_field_dummy_part2.zip',\n",
       " 'bricare_20240501_20240720_0_field_dummy_new_part1.csv',\n",
       " 'bricare_20240501_20240720_0_field_dummy_new_part2.csv',\n",
       " 'bricare_20240501_20240720_1200001_field_dummy_new_part1.csv',\n",
       " 'bricare_20240501_20240720_300001_field_dummy_new_part1.csv',\n",
       " 'bricare_20240501_20240720_300001_field_dummy_new_part2.csv',\n",
       " 'bricare_20240501_20240720_600001_field_dummy_new_part1.csv',\n",
       " 'bricare_20240501_20240720_600001_field_dummy_new_part2.csv',\n",
       " 'bricare_20240501_20240720_900001_field_dummy_new_part1.csv',\n",
       " 'bricare_20240501_20240720_900001_field_dummy_new_part2.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"D:\\cleaned\\test\"\n",
    "output_directory = r\"D:\\cleaned\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "def process_file(file_path, delimiter, output_directory):\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        all_ticket_ids = sorted(df['Ticket_ID'].unique())\n",
    "\n",
    "        # Generate the full range of Ticket_IDs for this file\n",
    "        min_id = int(all_ticket_ids[0][3:])\n",
    "        max_id = int(all_ticket_ids[-1][3:])\n",
    "        full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "        # Identify missing Ticket_IDs\n",
    "        missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "        # Define the fixed values for the gap rows\n",
    "        fixed_values = {\n",
    "            'Call_Type_ID': 1000,\n",
    "            'Create_Date': '1/1/1999',\n",
    "            'status': 'New',\n",
    "            'TanggalClosed': '1/1/1999',\n",
    "            'record_type': 'Case Migration',\n",
    "            'Priority': 'High (Urgent/Critical)'\n",
    "        }\n",
    "\n",
    "        # Create placeholder rows for missing IDs\n",
    "        missing_rows = []\n",
    "        for ticket_id in missing_ids:\n",
    "            placeholder_row = fixed_values.copy()\n",
    "            placeholder_row['Ticket_ID'] = ticket_id\n",
    "            placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "            missing_rows.append(placeholder_row)\n",
    "        missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "        # Add missing rows to the dataframe\n",
    "        combined_df = pd.concat([df, missing_df], ignore_index=True)\n",
    "        combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "        # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "        combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "        # Remove .0 from all values except nominal column\n",
    "        for col in combined_df.columns:\n",
    "            if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "                combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "        # Replace NaN with empty strings\n",
    "        combined_df.fillna('', inplace=True)\n",
    "        combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "        # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "        chunk_size = 200000\n",
    "        num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "            output_filename = os.path.join(output_directory, f'{os.path.basename(file_path).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "            chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "        print(f\"Processing for file '{file_path}' completed.\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission denied for file '{file_path}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing file '{file_path}': {e}\")\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for file_name in os.listdir(input_directory):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(input_directory, file_name)\n",
    "        process_file(file_path, delimiter, output_directory)\n",
    "\n",
    "# List the output files and sort them correctly\n",
    "def sort_output_files(filename):\n",
    "    parts = filename.split('_part')\n",
    "    if len(parts) == 2:\n",
    "        return (parts[0], int(parts[1].split('.')[0]))\n",
    "    else:\n",
    "        return (filename, 0)  # Handle files not following the pattern\n",
    "\n",
    "output_files = sorted([f for f in os.listdir(output_directory) if '_part' in f], key=sort_output_files)\n",
    "\n",
    "# Displaying the names of the output files\n",
    "output_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To filter out dummy rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\Case\"\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        \n",
    "        df_filtered = df[df['Create_Date'] != '1/1/1999']\n",
    "        \n",
    "        \n",
    "        df_filtered.to_csv(file_path, index=False)\n",
    "        print(f\"Filtered file saved: {filename}\")\n",
    "\n",
    "print(\"Filtering completed for all CSV files in the folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split the file. Remaining records are saved in D:\\cleaned\\bricare_20190927_20240430_23400001_field_dummy_part1_remaining.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def split_file(input_file, num_success):\n",
    "    try:\n",
    "        base, ext = os.path.splitext(input_file)\n",
    "        output_remaining_file = f\"{base}_remaining{ext}\"\n",
    "        \n",
    "        with open(input_file, mode='r', newline='') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            header = next(reader)\n",
    "\n",
    "            remaining_records = []\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= num_success:\n",
    "                    remaining_records.append(row)\n",
    "\n",
    "        # Write the remaining records to the output_remaining_file\n",
    "        with open(output_remaining_file, mode='w', newline='') as remainingfile:\n",
    "            writer = csv.writer(remainingfile)\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(remaining_records)\n",
    "\n",
    "        print(f\"Successfully split the file. Remaining records are saved in {output_remaining_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"D:\\cleaned\\bricare_20190927_20240430_23400001_field_dummy_part1.csv\"\n",
    "    num_success = 8\n",
    "\n",
    "    split_file(input_file, num_success)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>TanggalClosed</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86993</th>\n",
       "      <td>TTB0000001300</td>\n",
       "      <td>2019-10-16 10:00:00</td>\n",
       "      <td>2021-10-27 10:28:40</td>\n",
       "      <td>1000</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Ticket_ID          Create_Date        TanggalClosed  Call_Type_ID  \\\n",
       "86993  TTB0000001300  2019-10-16 10:00:00  2021-10-27 10:28:40          1000   \n",
       "\n",
       "      status  \n",
       "86993    New  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\cleaned\\file_dummy_all-20240720T225819Z-001\\file_dummy_all\\bricare_20190927_20240430_0_field_dummy.csv\"\n",
    "\n",
    "df =pd.read_csv(path, sep=';')\n",
    "df[df['Ticket_ID']=='TTB0000001300']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part1.csv\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "# Collect Ticket_IDs and CSV files for suffix '_79'\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for suffix '_79'\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            df['Ticket_ID'] = df['Ticket_ID'].astype(str)  # Ensure Ticket_ID is a string\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "# Collect Ticket_IDs and CSV files for suffix '_79'\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        df['Ticket_ID'] = df['Ticket_ID'].astype(str)  # Ensure Ticket_ID is a string\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df['Ticket_ID'] = combined_df['Ticket_ID'].astype(str)  # Ensure Ticket_ID is a string\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for suffix '_79'\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ';'  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs from TTB000000000001 to the maximum in the current data\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(12)}' for i in range(1, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv_files(input_folder, output_folder):\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Remove the 'Legacy_Ticket_ID' column if it exists\n",
    "            if 'Legacy_Ticket_ID' in data.columns:\n",
    "                data = data.drop(columns=['Legacy_Ticket_ID'])\n",
    "            \n",
    "            # Rename the 'Legacy_ticket_id' column to 'Legacy_Ticket_ID' if it exists\n",
    "            if 'Legacy_ticket_id' in data.columns:\n",
    "                data = data.rename(columns={'Legacy_ticket_id': 'Legacy_Ticket_ID'})\n",
    "            \n",
    "            # Replace all NaN values with empty strings\n",
    "            data = data.fillna('')\n",
    "            \n",
    "            # Remove '.0' at the end of all values\n",
    "            data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
    "            \n",
    "            \n",
    "            # Save the cleaned data to the output folder\n",
    "            cleaned_file_path = os.path.join(output_folder, filename)\n",
    "            data.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Processed and saved: {cleaned_file_path}\")\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "output_folder = r\"D:\\cleaned\"\n",
    "\n",
    "# Call the function to clean all CSV files\n",
    "clean_csv_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('yourfile.csv', delimiter=';')\n",
    "\n",
    "# Check for duplicate 'cifno'\n",
    "duplicate_cifno = df.duplicated('cifno')\n",
    "if duplicate_cifno.any():\n",
    "    print(\"There are duplicate 'cifno' values.\")\n",
    "else:\n",
    "    print(\"All 'cifno' values are unique.\")\n",
    "\n",
    "# Fill empty 'Nama' fields with \"No Name\"\n",
    "df['Nama'].fillna('No Name', inplace=True)\n",
    "df['Nama'].replace('', 'No Name', inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('updated_yourfile.csv', index=False, sep=';')\n",
    "\n",
    "print(\"Processing complete. The updated file has been saved as 'updated_yourfile.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_dates_in_files_and_generate_excel(folder_path, output_excel_file):\n",
    "    # List to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):  # assuming the files are in CSV format\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Read the file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if the necessary columns are in the DataFrame\n",
    "            if 'Create_Date' in df.columns and 'TanggalClosed' in df.columns:\n",
    "                # Convert columns to datetime if they are not already\n",
    "                df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "                df['TanggalClosed'] = pd.to_datetime(df['TanggalClosed'], errors='coerce')\n",
    "\n",
    "                # Check if Create_Date is before TanggalClosed\n",
    "                condition = df['Create_Date'] > df['TanggalClosed']\n",
    "                \n",
    "                # Find the rows where the condition is not met\n",
    "                incorrect_rows = df[~condition].index.tolist()\n",
    "                \n",
    "                # Append result for the current file\n",
    "                result = {\n",
    "                    'Filename': filename,\n",
    "                    'All_Dates_Correct': condition.all(),\n",
    "                    'Incorrect_Rows': incorrect_rows  # List of incorrect rows\n",
    "                }\n",
    "                results.append(result)\n",
    "            else:\n",
    "                results.append({\n",
    "                    'Filename': filename,\n",
    "                    'All_Dates_Correct': False,\n",
    "                    'Incorrect_Rows': 'Missing required columns'\n",
    "                })\n",
    "    \n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    with pd.ExcelWriter(output_excel_file) as writer:\n",
    "        results_df.to_excel(writer, index=False, sheet_name='Summary')\n",
    "        \n",
    "        for result in results:\n",
    "            if result['All_Dates_Correct'] is False and isinstance(result['Incorrect_Rows'], list):\n",
    "                # Write the detailed incorrect rows for each file\n",
    "                incorrect_df = df.iloc[result['Incorrect_Rows']]\n",
    "                incorrect_df.to_excel(writer, sheet_name=result['Filename'], index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define the folder path and output Excel file\n",
    "folder_path = r\"D:\\cleaned\"\n",
    "output_excel_file = 'date_check_results.xlsx'\n",
    "check_dates_in_files_and_generate_excel(folder_path, output_excel_file)\n",
    "\n",
    "print(f\"Results have been saved to {output_excel_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def move_files_by_date(source_folder, destination_folder, start_date):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Get the creation date of the file\n",
    "            file_create_date = datetime.fromtimestamp(os.path.getctime(file_path))\n",
    "            # Format the creation date to match the given format\n",
    "            formatted_date = file_create_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            if datetime.strptime(formatted_date, '%Y-%m-%d %H:%M:%S') >= start_date:\n",
    "                shutil.move(file_path, destination_folder)\n",
    "                print(f'Moved: {file_path} to {destination_folder}')\n",
    "\n",
    "# Path of the folder to scan\n",
    "source_folder = r'path\\to\\source\\folder'\n",
    "# Path of the destination folder\n",
    "destination_folder = r'path\\to\\destination\\folder'\n",
    "\n",
    "# Start date\n",
    "start_date = datetime.strptime('2022-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "move_files_by_date(source_folder, destination_folder, start_date)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

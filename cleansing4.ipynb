{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To check the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check number of rows\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "# folder_path = r\"D:\\cleaned\\file_dummy_all-20240720T225819Z-001\\file_dummy_all\"\n",
    "folder_path = r\"D:\\cleaned\\file_dummy_all\"\n",
    "\n",
    "# Create a list to store the file details\n",
    "file_details = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Get the number of rows (excluding header)\n",
    "        num_rows = len(df)\n",
    "        # Append the details to the list\n",
    "        file_details.append({'Filename': filename, 'Number of Rows': num_rows})\n",
    "\n",
    "# Create a DataFrame from the list of file details\n",
    "file_details_df = pd.DataFrame(file_details)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file_path = 'file_details_dummy_cleaned.xlsx'\n",
    "file_details_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f'File details saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To exlcude invalid TTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and replaced bricare_20190927_20240430_0_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_10200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_10500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_10800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_11100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_11400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_11700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_12000001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_1200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_12300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_12600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_12900001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_13200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_13500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_13800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_14100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_14400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_14700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_15000001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_1500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_15300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_15600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_15900001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_16200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_16500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_16800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_17100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_17400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_17700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_18000001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_1800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_18300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_18600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_18900001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_19200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_19500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_19800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_20100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_20400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_20700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_21000001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_2100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_21300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_21600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_21900001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_22200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_22500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_22800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_23100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_23400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_23700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_2400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_2700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_3000001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_3300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_3600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_3900001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_4200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_4500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_4800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_5100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_5400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_5700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_6000001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_6300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_6600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_6900001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_7200001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_7500001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_7800001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_8100001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_8400001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_8700001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_9000001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_900001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_9300001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_9600001_field_dummy.csv\n",
      "Filtered and replaced bricare_20190927_20240430_9900001_field_dummy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\cleaned\\file_dummy_all\"\n",
    "\n",
    "\n",
    "# Define a function to filter the DataFrame\n",
    "def filter_ticket_id(df):\n",
    "    return df[df['Ticket_ID'].str.contains('TTB', na=False)]\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Assuming the files are CSVs\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        # Filter the DataFrame\n",
    "        filtered_df = filter_ticket_id(df)\n",
    "        \n",
    "        # Overwrite the original file with the filtered DataFrame\n",
    "        filtered_df.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f'Filtered and replaced {filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To correct TTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed bricare_20190927_20240430_0_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_10200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_10500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_10800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_11100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_11400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_11700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_12000001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_1200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_12300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_12600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_12900001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_13200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_13500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_13800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_14100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_14400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_14700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_15000001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_1500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_15300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_15600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_15900001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_16200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_16500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_16800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_17100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_17400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_17700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_18000001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_1800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_18300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_18600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_18900001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_19200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_19500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_19800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_20100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_20400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_20700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_21000001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_2100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_21300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_21600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_21900001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_22200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_22500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_22800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_23100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_23400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_23700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_2400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_2700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_3000001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_3300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_3600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_3900001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_4200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_4500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_4800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_5100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_5400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_5700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_6000001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_6300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_6600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_6900001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_7200001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_7500001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_7800001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_8100001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_8400001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_8700001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_9000001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_900001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_9300001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_9600001_field_dummy.csv\n",
      "Processed bricare_20190927_20240430_9900001_field_dummy.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def correct_ticket_id(ticket_id):\n",
    "    if pd.isna(ticket_id):  # Handle NaN values\n",
    "        return ticket_id\n",
    "    ticket_id = str(ticket_id)\n",
    "    prefix = \"TTB\"\n",
    "    if ticket_id.startswith(prefix):\n",
    "        number_part = ticket_id[len(prefix):]\n",
    "        corrected_number_part = number_part.zfill(12)\n",
    "        return prefix + corrected_number_part\n",
    "    return ticket_id\n",
    "\n",
    "def process_files_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if 'Ticket_ID' in df.columns:\n",
    "                df['Ticket_ID'] = df['Ticket_ID'].apply(correct_ticket_id)\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"Processed {filename}\")\n",
    "\n",
    "folder_path = r\"D:\\cleaned\\file_dummy_all\"\n",
    "process_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To fill in Gap tickets to fill correctly gaps between the Ticket_IDs across different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.71 GiB for an array with shape (28642696, 8) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 95>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing completed. Files saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Process the files\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_ticket_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mprocess_files\u001b[1;34m(csv_files, all_ticket_ids, delimiter, output_directory)\u001b[0m\n\u001b[0;32m     53\u001b[0m     placeholder_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLegacy_ticket_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ticket_id  \u001b[38;5;66;03m# Ensure Legacy_ticket_id matches Ticket_ID\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     missing_rows\u001b[38;5;241m.\u001b[39mappend(placeholder_row)\n\u001b[1;32m---> 55\u001b[0m missing_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissing_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Read and combine all files\u001b[39;00m\n\u001b[0;32m     58\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:837\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m--> 837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_list_of_dict_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m    839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001b[1;32mc:\\Users\\maste\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:924\u001b[0m, in \u001b[0;36m_list_of_dict_to_arrays\u001b[1;34m(data, columns)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;66;03m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# classes\u001b[39;00m\n\u001b[0;32m    922\u001b[0m data \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(d) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdicts_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mlib.pyx:395\u001b[0m, in \u001b[0;36mpandas._libs.lib.dicts_to_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.71 GiB for an array with shape (28642696, 8) and data type object"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"D:\\cleaned\\file_dummy_all\"\n",
    "output_directory = r\"D:\\cleaned\\output_cleaned_dummy\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids, csv_files = collect_ticket_ids(input_directory, delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration',\n",
    "        'Priority': 'High (Urgent/Critical)'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read and combine all files\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Ensure the \"Priority\" column is added and filled\n",
    "    combined_df['Priority'] = 'High (Urgent/Critical)'\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'Nominal':  # Replace 'Nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Split combined dataframe into chunks of 300,000 rows\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    base_filename = os.path.basename(csv_files[0]).split('_part')[0]\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{base_filename}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing completed. Files saved to {output_directory}\")\n",
    "\n",
    "# Process the files\n",
    "process_files(csv_files, all_ticket_ids, delimiter, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket_ID</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Create_Date</th>\n",
       "      <th>gateway</th>\n",
       "      <th>Jenis_Laporan</th>\n",
       "      <th>Nama_Nasabah</th>\n",
       "      <th>No_Rekening</th>\n",
       "      <th>Nominal</th>\n",
       "      <th>status</th>\n",
       "      <th>...</th>\n",
       "      <th>Tgl_In_Progress</th>\n",
       "      <th>Tgl_Returned</th>\n",
       "      <th>Ticket_Referensi</th>\n",
       "      <th>Tiket_Urgency</th>\n",
       "      <th>Tipe_Remark</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>users</th>\n",
       "      <th>Usergroup_ID</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Legacy_ticket_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTB000000000012</td>\n",
       "      <td>8701</td>\n",
       "      <td>Blokir Kartu ATM karena kartu hilang</td>\n",
       "      <td>2023-01-01 07:07:15</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Request</td>\n",
       "      <td>Anwar</td>\n",
       "      <td>1.234568e+14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4.0</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTB000000000013</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTB000000000014</td>\n",
       "      <td>8202</td>\n",
       "      <td>Informasi Product Banking</td>\n",
       "      <td>2023-01-01 07:06:39</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Products / Promotion Inquiry</td>\n",
       "      <td>Budi</td>\n",
       "      <td>1.234568e+14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call</td>\n",
       "      <td>4.0</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTB000000000015</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTB000000000016</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>TTB000000000105</td>\n",
       "      <td>8405</td>\n",
       "      <td>Kartu ATM BRI Tertelan di MESIN ATM</td>\n",
       "      <td>2020-01-01 07:19:30</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Information</td>\n",
       "      <td>Arif Budi Saputra</td>\n",
       "      <td>2.123457e+13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>TTB000000000106</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>TTB000000000107</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>TTB000000000108</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>TTB000000000109</td>\n",
       "      <td>8202</td>\n",
       "      <td>Informasi Product Banking</td>\n",
       "      <td>2020-01-01 07:19:27</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Information</td>\n",
       "      <td>Arif Budi Saputra</td>\n",
       "      <td>2.123457e+13</td>\n",
       "      <td>741700.0</td>\n",
       "      <td>Closed</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High (Urgent/Critical)</td>\n",
       "      <td>TTB000000000109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ticket_ID  Call_Type_ID                             Call_Type  \\\n",
       "0   TTB000000000012          8701  Blokir Kartu ATM karena kartu hilang   \n",
       "1   TTB000000000013          1000                                   NaN   \n",
       "2   TTB000000000014          8202             Informasi Product Banking   \n",
       "3   TTB000000000015          1000                                   NaN   \n",
       "4   TTB000000000016          1000                                   NaN   \n",
       "..              ...           ...                                   ...   \n",
       "93  TTB000000000105          8405   Kartu ATM BRI Tertelan di MESIN ATM   \n",
       "94  TTB000000000106          1000                                   NaN   \n",
       "95  TTB000000000107          1000                                   NaN   \n",
       "96  TTB000000000108          1000                                   NaN   \n",
       "97  TTB000000000109          8202             Informasi Product Banking   \n",
       "\n",
       "            Create_Date gateway                 Jenis_Laporan  \\\n",
       "0   2023-01-01 07:07:15   Phone                       Request   \n",
       "1              1/1/1999     NaN                           NaN   \n",
       "2   2023-01-01 07:06:39   Phone  Products / Promotion Inquiry   \n",
       "3              1/1/1999     NaN                           NaN   \n",
       "4              1/1/1999     NaN                           NaN   \n",
       "..                  ...     ...                           ...   \n",
       "93  2020-01-01 07:19:30   Phone                   Information   \n",
       "94             1/1/1999     NaN                           NaN   \n",
       "95             1/1/1999     NaN                           NaN   \n",
       "96             1/1/1999     NaN                           NaN   \n",
       "97  2020-01-01 07:19:27   Phone                   Information   \n",
       "\n",
       "         Nama_Nasabah   No_Rekening   Nominal     status  ... Tgl_In_Progress  \\\n",
       "0               Anwar  1.234568e+14       0.0     Closed  ...             NaN   \n",
       "1                 NaN           NaN       NaN  Cancelled  ...             NaN   \n",
       "2                Budi  1.234568e+14       0.0     Closed  ...             NaN   \n",
       "3                 NaN           NaN       NaN  Cancelled  ...             NaN   \n",
       "4                 NaN           NaN       NaN  Cancelled  ...             NaN   \n",
       "..                ...           ...       ...        ...  ...             ...   \n",
       "93  Arif Budi Saputra  2.123457e+13       0.0     Closed  ...             NaN   \n",
       "94                NaN           NaN       NaN  Cancelled  ...             NaN   \n",
       "95                NaN           NaN       NaN  Cancelled  ...             NaN   \n",
       "96                NaN           NaN       NaN  Cancelled  ...             NaN   \n",
       "97  Arif Budi Saputra  2.123457e+13  741700.0     Closed  ...             NaN   \n",
       "\n",
       "   Tgl_Returned Ticket_Referensi Tiket_Urgency  Tipe_Remark UniqueID  users  \\\n",
       "0           NaN              NaN           NaN        Notes      NaN   Call   \n",
       "1           NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "2           NaN              NaN           NaN        Notes      NaN   Call   \n",
       "3           NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "4           NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "..          ...              ...           ...          ...      ...    ...   \n",
       "93          NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "94          NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "95          NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "96          NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "97          NaN              NaN           NaN          NaN      NaN    NaN   \n",
       "\n",
       "    Usergroup_ID                Priority Legacy_ticket_id  \n",
       "0            4.0  High (Urgent/Critical)  TTB000000000012  \n",
       "1            NaN  High (Urgent/Critical)  TTB000000000013  \n",
       "2            4.0  High (Urgent/Critical)  TTB000000000014  \n",
       "3            NaN  High (Urgent/Critical)  TTB000000000015  \n",
       "4            NaN  High (Urgent/Critical)  TTB000000000016  \n",
       "..           ...                     ...              ...  \n",
       "93           NaN  High (Urgent/Critical)  TTB000000000105  \n",
       "94           NaN  High (Urgent/Critical)  TTB000000000106  \n",
       "95           NaN  High (Urgent/Critical)  TTB000000000107  \n",
       "96           NaN  High (Urgent/Critical)  TTB000000000108  \n",
       "97           NaN  High (Urgent/Critical)  TTB000000000109  \n",
       "\n",
       "[98 rows x 82 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\\bricare_20200101_20200101_27_kosong.csv_part1.csv\"\n",
    "\n",
    "df =pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '27' completed.\n",
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20200101_20200101_27_kosong_part1.csv\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for files with suffix '79' completed.\n",
      "Reading file: bricare_20230101_20230101_79_this_part1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "# Collect Ticket_IDs and CSV files for suffix '_79'\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for suffix '_79'\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ','  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            df['Ticket_ID'] = df['Ticket_ID'].astype(str)  # Ensure Ticket_ID is a string\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "# Collect Ticket_IDs and CSV files for suffix '_79'\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs\n",
    "    min_id = int(all_ticket_ids[0][3:])\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(len(all_ticket_ids[0]) - 3)}' for i in range(min_id, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        df['Ticket_ID'] = df['Ticket_ID'].astype(str)  # Ensure Ticket_ID is a string\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df['Ticket_ID'] = combined_df['Ticket_ID'].astype(str)  # Ensure Ticket_ID is a string\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for suffix '_79'\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\"\n",
    "output_directory = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "\n",
    "# Define the delimiter\n",
    "delimiter = ';'  # Change this to the appropriate delimiter if necessary (e.g., ';' for semicolon)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to generate the legacy ticket ID from the ticket ID\n",
    "def generate_legacy_ticket_id(ticket_id):\n",
    "    numeric_part = int(ticket_id[3:])\n",
    "    return f'TTB{str(numeric_part).zfill(len(ticket_id) - 3)}'\n",
    "\n",
    "# Collect all Ticket_IDs from existing files\n",
    "def collect_ticket_ids(input_directory, file_suffix, delimiter):\n",
    "    ticket_ids = set()\n",
    "    csv_files = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file_suffix in file and file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file), usecols=['Ticket_ID'], delimiter=delimiter)\n",
    "            ticket_ids.update(df['Ticket_ID'].unique())\n",
    "\n",
    "    return sorted(ticket_ids), csv_files\n",
    "\n",
    "all_ticket_ids_27, csv_files_27 = collect_ticket_ids(input_directory, '_27', delimiter)\n",
    "all_ticket_ids_79, csv_files_79 = collect_ticket_ids(input_directory, '_79', delimiter)\n",
    "\n",
    "def process_files(csv_files, all_ticket_ids, file_suffix, delimiter, output_directory):\n",
    "    # Generate the full range of Ticket_IDs from TTB000000000001 to the maximum in the current data\n",
    "    max_id = int(all_ticket_ids[-1][3:])\n",
    "    full_range_ids = [f'TTB{str(i).zfill(12)}' for i in range(1, max_id + 1)]\n",
    "\n",
    "    # Identify missing Ticket_IDs\n",
    "    missing_ids = set(full_range_ids) - set(all_ticket_ids)\n",
    "\n",
    "    # Define the fixed values for the gap rows\n",
    "    fixed_values = {\n",
    "        'Call_Type_ID': 1000,\n",
    "        'Create_Date': '1/1/1999',\n",
    "        'status': 'Cancelled',\n",
    "        'TanggalClosed': '1/1/1999',\n",
    "        'record_type': 'Case Migration'\n",
    "    }\n",
    "\n",
    "    # Create placeholder rows for missing IDs\n",
    "    missing_rows = []\n",
    "    for ticket_id in missing_ids:\n",
    "        placeholder_row = fixed_values.copy()\n",
    "        placeholder_row['Ticket_ID'] = ticket_id\n",
    "        placeholder_row['Legacy_ticket_id'] = ticket_id  # Ensure Legacy_ticket_id matches Ticket_ID\n",
    "        missing_rows.append(placeholder_row)\n",
    "    missing_df = pd.DataFrame(missing_rows)\n",
    "\n",
    "    # Read, combine, and split files if necessary\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(input_directory, file), delimiter=delimiter)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Add missing rows to the combined dataframe\n",
    "    combined_df = pd.concat([combined_df, missing_df], ignore_index=True)\n",
    "    combined_df.sort_values(by='Ticket_ID', inplace=True)\n",
    "\n",
    "    # Ensure all Legacy_ticket_id values match Ticket_ID\n",
    "    combined_df['Legacy_ticket_id'] = combined_df['Ticket_ID']\n",
    "\n",
    "    # Remove .0 from all values except nominal column\n",
    "    for col in combined_df.columns:\n",
    "        if col != 'nominal':  # Replace 'nominal' with the actual name of your nominal column\n",
    "            combined_df[col] = combined_df[col].apply(lambda x: str(x).replace('.0', '') if isinstance(x, float) else x)\n",
    "\n",
    "    # Replace NaN with empty strings\n",
    "    combined_df.fillna('', inplace=True)\n",
    "    combined_df.replace('nan', '', inplace=True)\n",
    "\n",
    "    # Ensure the combined dataframe doesn't exceed 300,000 rows per file\n",
    "    chunk_size = 300000\n",
    "    num_chunks = (len(combined_df) // chunk_size) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = combined_df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "        output_filename = os.path.join(output_directory, f'{os.path.basename(csv_files[0]).split(\".csv\")[0]}_part{i + 1}.csv')\n",
    "        chunk_df.to_csv(output_filename, index=False, sep=delimiter)\n",
    "\n",
    "    print(f\"Processing for files with suffix '{file_suffix}' completed.\")\n",
    "\n",
    "# Process the files for both suffixes\n",
    "process_files(csv_files_27, all_ticket_ids_27, '27', delimiter, output_directory)\n",
    "process_files(csv_files_79, all_ticket_ids_79, '79', delimiter, output_directory)\n",
    "\n",
    "# To read the files in order, you can sort the filenames programmatically\n",
    "output_files = sorted(os.listdir(output_directory), key=lambda x: (x.split('_')[1], int(x.split('part')[1].split('.')[0])))\n",
    "\n",
    "for file in output_files:\n",
    "    df = pd.read_csv(os.path.join(output_directory, file), delimiter=delimiter)\n",
    "    print(f\"Reading file: {file}\")\n",
    "    # Process the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv_files(input_folder, output_folder):\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Remove the 'Legacy_Ticket_ID' column if it exists\n",
    "            if 'Legacy_Ticket_ID' in data.columns:\n",
    "                data = data.drop(columns=['Legacy_Ticket_ID'])\n",
    "            \n",
    "            # Rename the 'Legacy_ticket_id' column to 'Legacy_Ticket_ID' if it exists\n",
    "            if 'Legacy_ticket_id' in data.columns:\n",
    "                data = data.rename(columns={'Legacy_ticket_id': 'Legacy_Ticket_ID'})\n",
    "            \n",
    "            # Replace all NaN values with empty strings\n",
    "            data = data.fillna('')\n",
    "            \n",
    "            # Remove '.0' at the end of all values\n",
    "            data = data.applymap(lambda x: str(x).replace('.0', '') if isinstance(x, (int, float)) else str(x))\n",
    "            \n",
    "            \n",
    "            # Save the cleaned data to the output folder\n",
    "            cleaned_file_path = os.path.join(output_folder, filename)\n",
    "            data.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Processed and saved: {cleaned_file_path}\")\n",
    "\n",
    "# Define the input and output folders\n",
    "input_folder = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\output\\test_casenumber - Copy\\output1\"\n",
    "output_folder = r\"D:\\cleaned\"\n",
    "\n",
    "# Call the function to clean all CSV files\n",
    "clean_csv_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('yourfile.csv', delimiter=';')\n",
    "\n",
    "# Check for duplicate 'cifno'\n",
    "duplicate_cifno = df.duplicated('cifno')\n",
    "if duplicate_cifno.any():\n",
    "    print(\"There are duplicate 'cifno' values.\")\n",
    "else:\n",
    "    print(\"All 'cifno' values are unique.\")\n",
    "\n",
    "# Fill empty 'Nama' fields with \"No Name\"\n",
    "df['Nama'].fillna('No Name', inplace=True)\n",
    "df['Nama'].replace('', 'No Name', inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('updated_yourfile.csv', index=False, sep=';')\n",
    "\n",
    "print(\"Processing complete. The updated file has been saved as 'updated_yourfile.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_dates_in_files_and_generate_excel(folder_path, output_excel_file):\n",
    "    # List to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):  # assuming the files are in CSV format\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Read the file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if the necessary columns are in the DataFrame\n",
    "            if 'Create_Date' in df.columns and 'TanggalClosed' in df.columns:\n",
    "                # Convert columns to datetime if they are not already\n",
    "                df['Create_Date'] = pd.to_datetime(df['Create_Date'], errors='coerce')\n",
    "                df['TanggalClosed'] = pd.to_datetime(df['TanggalClosed'], errors='coerce')\n",
    "\n",
    "                # Check if Create_Date is before TanggalClosed\n",
    "                condition = df['Create_Date'] > df['TanggalClosed']\n",
    "                \n",
    "                # Find the rows where the condition is not met\n",
    "                incorrect_rows = df[~condition].index.tolist()\n",
    "                \n",
    "                # Append result for the current file\n",
    "                result = {\n",
    "                    'Filename': filename,\n",
    "                    'All_Dates_Correct': condition.all(),\n",
    "                    'Incorrect_Rows': incorrect_rows  # List of incorrect rows\n",
    "                }\n",
    "                results.append(result)\n",
    "            else:\n",
    "                results.append({\n",
    "                    'Filename': filename,\n",
    "                    'All_Dates_Correct': False,\n",
    "                    'Incorrect_Rows': 'Missing required columns'\n",
    "                })\n",
    "    \n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    with pd.ExcelWriter(output_excel_file) as writer:\n",
    "        results_df.to_excel(writer, index=False, sheet_name='Summary')\n",
    "        \n",
    "        for result in results:\n",
    "            if result['All_Dates_Correct'] is False and isinstance(result['Incorrect_Rows'], list):\n",
    "                # Write the detailed incorrect rows for each file\n",
    "                incorrect_df = df.iloc[result['Incorrect_Rows']]\n",
    "                incorrect_df.to_excel(writer, sheet_name=result['Filename'], index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define the folder path and output Excel file\n",
    "folder_path = r\"D:\\cleaned\"\n",
    "output_excel_file = 'date_check_results.xlsx'\n",
    "check_dates_in_files_and_generate_excel(folder_path, output_excel_file)\n",
    "\n",
    "print(f\"Results have been saved to {output_excel_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def move_files_by_date(source_folder, destination_folder, start_date):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Get the creation date of the file\n",
    "            file_create_date = datetime.fromtimestamp(os.path.getctime(file_path))\n",
    "            # Format the creation date to match the given format\n",
    "            formatted_date = file_create_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            if datetime.strptime(formatted_date, '%Y-%m-%d %H:%M:%S') >= start_date:\n",
    "                shutil.move(file_path, destination_folder)\n",
    "                print(f'Moved: {file_path} to {destination_folder}')\n",
    "\n",
    "# Path of the folder to scan\n",
    "source_folder = r'path\\to\\source\\folder'\n",
    "# Path of the destination folder\n",
    "destination_folder = r'path\\to\\destination\\folder'\n",
    "\n",
    "# Start date\n",
    "start_date = datetime.strptime('2022-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "move_files_by_date(source_folder, destination_folder, start_date)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

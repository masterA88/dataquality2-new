{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attachment generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_exact_size_file(file_path, size_mb):\n",
    "    # Calculate the exact size in bytes\n",
    "    size_bytes = size_mb * 1024 * 1024\n",
    "\n",
    "    # Open the file in write binary ('wb') mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.seek(size_bytes - 1)  # Move to the position one byte before the desired size\n",
    "        file.write(b'\\0')  # Write a single zero byte at this position\n",
    "\n",
    "def generate_files(directory_path, excel_filename, pdf_filename, target_size_mb):\n",
    "    # Ensure the directory ends with a slash\n",
    "    if not directory_path.endswith('/'):\n",
    "        directory_path += '/'\n",
    "\n",
    "    # Generate exact-sized Excel and PDF files\n",
    "    create_exact_size_file(directory_path + excel_filename, target_size_mb)\n",
    "    create_exact_size_file(directory_path + pdf_filename, target_size_mb)\n",
    "\n",
    "    print(f'Generated Excel file at {directory_path + excel_filename} with size exactly {target_size_mb} MB')\n",
    "    print(f'Generated PDF file at {directory_path + pdf_filename} with size exactly {target_size_mb} MB')\n",
    "\n",
    "# Set the directory where files will be saved\n",
    "directory_path = \"/Users/ikawahyuni/dataquality/dummy_files\"\n",
    "\n",
    "# Set filenames and target size\n",
    "generate_files(directory_path, 'dummy_15mb.xlsx', 'dummy_15mb.pdf', 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Excel file at /Users/ikawahyuni/dataquality/dummy_files/dummy_14mb.xlsx with size exactly 14 MB\n",
      "Generated PDF file at /Users/ikawahyuni/dataquality/dummy_files/dummy_14mb.pdf with size exactly 14 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_exact_size_file(file_path, size_mb):\n",
    "    # Calculate the exact size in bytes\n",
    "    size_bytes = size_mb * 1024 * 1024\n",
    "\n",
    "    # Open the file in write binary ('wb') mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.seek(size_bytes - 1)  # Move to the position one byte before the desired size\n",
    "        file.write(b'\\0')  # Write a single zero byte at this position\n",
    "\n",
    "def generate_files(directory_path, excel_filename, pdf_filename, target_size_mb):\n",
    "    # Ensure the directory ends with a slash\n",
    "    if not directory_path.endswith('/'):\n",
    "        directory_path += '/'\n",
    "\n",
    "    # Generate exact-sized Excel and PDF files\n",
    "    create_exact_size_file(directory_path + excel_filename, target_size_mb)\n",
    "    create_exact_size_file(directory_path + pdf_filename, target_size_mb)\n",
    "\n",
    "    print(f'Generated Excel file at {directory_path + excel_filename} with size exactly {target_size_mb} MB')\n",
    "    print(f'Generated PDF file at {directory_path + pdf_filename} with size exactly {target_size_mb} MB')\n",
    "\n",
    "# Set the directory where files will be saved\n",
    "directory_path = \"/Users/ikawahyuni/dataquality/dummy_files\"\n",
    "\n",
    "# Set filenames and target size (change from 15 to 14)\n",
    "generate_files(directory_path, 'dummy_14mb.xlsx', 'dummy_14mb.pdf', 14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maximum size limit of 10,485,760 bytes (10 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Excel file at D:\\dataquality\\dummy_files/dummy.xlsx with size up to 10 MB\n",
      "Generated PDF file at D:\\dataquality\\dummy_files/dummy.pdf with size up to 10 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_exact_size_file(file_path, size_mb):\n",
    "    # Maximum size in bytes (10 MB)\n",
    "    max_size_bytes = 9485760\n",
    "\n",
    "    # Calculate the desired size in bytes\n",
    "    size_bytes = size_mb * 1024 * 1024\n",
    "\n",
    "    # Adjust size if it exceeds the maximum allowed\n",
    "    if size_bytes > max_size_bytes:\n",
    "        size_bytes = max_size_bytes\n",
    "\n",
    "    # Open the file in write binary ('wb') mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.seek(size_bytes - 1)  # Move to the position one byte before the desired size\n",
    "        file.write(b'\\0')  # Write a single zero byte at this position\n",
    "\n",
    "def generate_files(directory_path, excel_filename, pdf_filename, target_size_mb):\n",
    "    # Ensure the directory ends with a slash\n",
    "    if not directory_path.endswith('/'):\n",
    "        directory_path += '/'\n",
    "\n",
    "    # Generate exact-sized Excel and PDF files\n",
    "    create_exact_size_file(directory_path + excel_filename, target_size_mb)\n",
    "    create_exact_size_file(directory_path + pdf_filename, target_size_mb)\n",
    "\n",
    "    actual_size_mb = min(target_size_mb, 10)  # Limit the reported size to 10 MB max\n",
    "    print(f'Generated Excel file at {directory_path + excel_filename} with size up to {actual_size_mb} MB')\n",
    "    print(f'Generated PDF file at {directory_path + pdf_filename} with size up to {actual_size_mb} MB')\n",
    "\n",
    "# Set the directory where files will be saved\n",
    "directory_path = \"D:\\dataquality\\dummy_files\"\n",
    "\n",
    "# Set filenames and target size (e.g., 12 MB would be reduced to 10 MB)\n",
    "generate_files(directory_path, 'dummy.xlsx', 'dummy.pdf', 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool to collect all tickets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ReportTest.zip to D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\n",
      "Data from D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\\ReportTest.arx saved to CSV at D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest_files_info.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import rarfile  # pip install rarfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_process_files(directory):\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        extract_path = os.path.join(directory, filename[:-4]) \n",
    "\n",
    "   \n",
    "        if filename.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        elif filename.endswith(\".rar\"):\n",
    "            with rarfile.RarFile(filepath, 'r') as rar_ref:\n",
    "                rar_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    " \n",
    "        report_file_path = find_report_file(extract_path)\n",
    "        if report_file_path:\n",
    "            df = create_dataframe_from_file(report_file_path, extract_path)\n",
    "            csv_path = os.path.join(directory, filename[:-4] + '_files_info.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n",
    "        else:\n",
    "            print(f\"No 'ReportTest.arx' found in {extract_path}\")\n",
    "\n",
    "def find_report_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'ReportTest.arx':\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            full_path = os.path.join(extract_path, file_name)\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "\n",
    "directory_path = r'D:\\Salesforce\\archive\\dataquality\\Tool for attachments'\n",
    "extract_and_process_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool to collect all tickets 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Report(4).zip to D:\\attachment\\Report(4)\n",
      "Converted 20240805_100751_63.heic to D:\\attachment\\Report(4)\\20240805_100751_63..zip\n",
      "Converted 54363023_206.jfif to D:\\attachment\\Report(4)\\54363023_206..zip\n",
      "Converted authenticated_image (2)_340.jfif to D:\\attachment\\Report(4)\\authenticated_image (2)_340..zip\n",
      "Converted Dokumen Pelapor - IIN KURNIAWATI_166.rar to D:\\attachment\\Report(4)\\Dokumen Pelapor - IIN KURNIAWATI_166.zip\n",
      "Converted Dokumen pelapor_570.rar to D:\\attachment\\Report(4)\\Dokumen pelapor_570.zip\n",
      "Converted Dokumen Pengajuan - IIN KURNIAWATI_165.rar to D:\\attachment\\Report(4)\\Dokumen Pengajuan - IIN KURNIAWATI_165.zip\n",
      "Converted Dokumen pengajuan_571.rar to D:\\attachment\\Report(4)\\Dokumen pengajuan_571.zip\n",
      "Converted KHOPIDH PUTERA - 3565101180086206_406.jfif to D:\\attachment\\Report(4)\\KHOPIDH PUTERA - 3565101180086206_406..zip\n",
      "Converted MUHAMMAD ROMADAN - 4359720223618903_405.jfif to D:\\attachment\\Report(4)\\MUHAMMAD ROMADAN - 4359720223618903_405..zip\n",
      "Converted RAHMAN SISWANTOMO_150524_195.htm to D:\\attachment\\Report(4)\\RAHMAN SISWANTOMO_150524_195.zip\n",
      "Converted RAHMAN SISWANTOMO_150624_196.htm to D:\\attachment\\Report(4)\\RAHMAN SISWANTOMO_150624_196.zip\n",
      "Converted RAHMAN SISWANTOMO_150724_197.htm to D:\\attachment\\Report(4)\\RAHMAN SISWANTOMO_150724_197.zip\n",
      "Converted Salahtranfer iin liyana_367.txt to D:\\attachment\\Report(4)\\Salahtranfer iin liyana_367.zip\n",
      "Converted SUJAWATI PUTIYA ARUM WARDANI_020824_169.htm to D:\\attachment\\Report(4)\\SUJAWATI PUTIYA ARUM WARDANI_020824_169.zip\n",
      "Data from D:\\attachment\\Report(4)\\Report.arx saved to CSV at D:\\attachment\\Report(4)_files_info.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import rarfile  # pip install rarfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_process_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # Extract the big zip file\n",
    "        if filename.endswith(\".zip\"):\n",
    "            extract_path = os.path.join(directory, filename[:-4])\n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "\n",
    "            # Convert specific file types to zip within the extracted folder\n",
    "            convert_files_to_zip(extract_path)\n",
    "\n",
    "            # Process the extracted folder to find 'ReportTest.arx' and create the DataFrame\n",
    "            report_file_path = find_report_file(extract_path)\n",
    "            if report_file_path:\n",
    "                df = create_dataframe_from_file(report_file_path, extract_path)\n",
    "                csv_path = os.path.join(directory, filename[:-4] + '_files_info.csv')\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n",
    "            else:\n",
    "                print(f\"No 'ReportTest.arx' found in {extract_path}\")\n",
    "\n",
    "def convert_files_to_zip(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".txt\", \".rtp\", \".tif\", \".rar\",\".rptdesign\",\".js\",\".jfif\",\".crdownload\",\".heic\",\".htm\")):\n",
    "                file_path = os.path.join(root, file)\n",
    "                new_zip_path = file_path[:-4] + \".zip\"\n",
    "                with zipfile.ZipFile(new_zip_path, 'w') as zip_ref:\n",
    "                    zip_ref.write(file_path, file)\n",
    "                os.remove(file_path)  # Optionally remove the original file\n",
    "                print(f\"Converted {file} to {new_zip_path}\")\n",
    "\n",
    "def find_report_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'Report.arx':\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    path_on_client = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            # Check if the file was converted to zip\n",
    "            original_file_path = os.path.join(extract_path, file_name)\n",
    "            zip_file_path = original_file_path[:-4] + \".zip\"\n",
    "            if os.path.exists(zip_file_path):\n",
    "                full_path = zip_file_path\n",
    "            else:\n",
    "                full_path = original_file_path\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            path_on_client.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'PathOnClient': path_on_client,\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "directory_path = r\"D:\\attachment\"\n",
    "extract_and_process_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ReportTest.zip to D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\n",
      "Data from D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\\ReportTest.arx saved to CSV at D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest_files_info.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import rarfile  # pip install rarfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_process_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        extract_path = os.path.join(directory, filename[:-4]) \n",
    "\n",
    "        if filename.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        elif filename.endswith(\".rar\"):\n",
    "            with rarfile.RarFile(filepath, 'r') as rar_ref:\n",
    "                rar_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        report_file_path = find_report_file(extract_path)\n",
    "        if report_file_path:\n",
    "            df = create_dataframe_from_file(report_file_path, extract_path)\n",
    "            csv_path = os.path.join(directory, filename[:-4] + '_files_info.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n",
    "        else:\n",
    "            print(f\"No 'ReportTest.arx' found in {extract_path}\")\n",
    "\n",
    "def find_report_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'ReportTest.arx':\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            full_path = os.path.join(extract_path, file_name)\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'PathOnClient': version_data,  # New column added here\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "directory_path = r'D:\\Salesforce\\archive\\dataquality\\Tool for attachments'\n",
    "extract_and_process_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.arx saved to CSV at C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            full_path = os.path.join(r'E:\\Attachment\\report1', file_name)\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'PathOnClient': version_data,  # New column added here\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "# Assume this is the path to the report file you uploaded\n",
    "report_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.arx\"\n",
    "# Create dataframe\n",
    "df = create_dataframe_from_file(report_file_path, 'E:\\\\Attachment\\\\report1')\n",
    "# Save to CSV\n",
    "csv_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create copied 1000 ine\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\bricare\\file_for_attachment.csv\"\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "df = pd.concat([df] * 1000, ignore_index=True)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to change the column FirstPublishLocationId with all values in ID column taken from case ID in salesforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'FirstPublishLocationId' column has been replaced with the 'ID' column values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>VersionData</th>\n",
       "      <th>PathOnClient</th>\n",
       "      <th>FirstPublishLocationId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KLYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KMYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KNYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KOYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KPYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LtYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LuYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LvYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LwYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LxYAI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Title  Description                              VersionData  \\\n",
       "900  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "901  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "902  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "903  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "904  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "..          ...          ...                                      ...   \n",
       "995  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "996  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "997  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "998  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "999  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "\n",
       "                                          PathOnClient FirstPublishLocationId  \n",
       "900  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KLYAY  \n",
       "901  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KMYAY  \n",
       "902  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KNYAY  \n",
       "903  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KOYAY  \n",
       "904  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KPYAY  \n",
       "..                                                 ...                    ...  \n",
       "995  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LtYAI  \n",
       "996  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LuYAI  \n",
       "997  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LvYAI  \n",
       "998  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LwYAI  \n",
       "999  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LxYAI  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df1 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\bricare\\file_for_attachment.csv\")  # Assuming this file contains the columns Title, Description, VersionData, PathOnClient, FirstPublishLocationId\n",
    "df2 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\bricare\\extract_case_id_1000.csv\")  # Assuming this file contains the column ID\n",
    "\n",
    "# Check if both DataFrames have the same number of rows\n",
    "if len(df1) != len(df2):\n",
    "    raise ValueError(\"The number of rows in both files must be the same\")\n",
    "\n",
    "# Replace the 'FirstPublishLocationId' column in df1 with the 'ID' column from df2\n",
    "df1['FirstPublishLocationId'] = df2['ID']\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "# df1.to_csv('modified_file.csv', index=False)\n",
    "\n",
    "print(\"The 'FirstPublishLocationId' column has been replaced with the 'ID' column values.\")\n",
    "\n",
    "df1.iloc[900:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File split_file_1.csv created with rows from 0 to 100\n",
      "File split_file_2.csv created with rows from 100 to 200\n",
      "File split_file_3.csv created with rows from 200 to 300\n",
      "File split_file_4.csv created with rows from 300 to 400\n",
      "File split_file_5.csv created with rows from 400 to 500\n",
      "Files created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Slice into 5 files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare\\file_for_attachment.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Number of rows per split file\n",
    "rows_per_file = 100\n",
    "\n",
    "# Create 5 files with 200 rows each\n",
    "for i in range(5):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = (i + 1) * rows_per_file\n",
    "    split_df = df.iloc[start_row:end_row]\n",
    "    split_file_path = f'split_file_{i + 1}.csv'\n",
    "    split_df.to_csv(split_file_path, index=False)\n",
    "    print(f'File {split_file_path} created with rows from {start_row} to {end_row}')\n",
    "\n",
    "print('Files created successfully.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Report8.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\report7.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace 'report1' with 'Report2' in the 'VersionData' and 'PathOnClient' columns\n",
    "data['VersionData'] = data['VersionData'].str.replace('report1', 'Report8')\n",
    "data['PathOnClient'] = data['PathOnClient'].str.replace('report1', 'Report8')\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "modified_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Report8.csv\"\n",
    "data.to_csv(modified_file_path, index=False)\n",
    "\n",
    "# Provide the path to the modified file\n",
    "print(f'Modified file saved at: {modified_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report2.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report3.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report4.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report5.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report6.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report7.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report8.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\"  \n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Replace drive 'D' with 'E' in the 'VersionData' and 'PathOnClient' columns\n",
    "        data['VersionData'] = data['VersionData'].str.replace('D:', 'E:')\n",
    "        data['PathOnClient'] = data['PathOnClient'].str.replace('D:', 'E:')\n",
    "        \n",
    "        # Save the modified DataFrame to a new CSV file\n",
    "        modified_file_path = os.path.join(folder_path, f'Modified_{filename}')\n",
    "        data.to_csv(modified_file_path, index=False)\n",
    "\n",
    "        print(f'Modified file saved at: {modified_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python2\\\\Attachment\\\\attachments_log\\\\log_report6\\\\error080524014331715_done.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Attachment\\attachments_log\\log_report6\\error080524014331715.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the new folder path\n",
    "new_folder_path = \"D:\\Python2\\Attachment\\Report(5)\\Report(5)\"\n",
    "\n",
    "# Update the VERSIONDATA and PATHONCLIENT columns\n",
    "# df['VERSIONDATA'] = df['VERSIONDATA'].apply(lambda x: new_folder_path + x.split('Report(4)')[-1].replace('/', '\\\\'))\n",
    "# df['PATHONCLIENT'] = df['PATHONCLIENT'].apply(lambda x: new_folder_path + x.split('Report(4)')[-1].replace('/', '\\\\'))\n",
    "\n",
    "df['VersionData'] = df['VersionData'].apply(lambda x: os.path.join(new_folder_path, os.path.basename(x)).replace('/', '\\\\'))\n",
    "df['PathOnClient'] = df['PathOnClient'].apply(lambda x: os.path.join(new_folder_path, os.path.basename(x)).replace('/', '\\\\'))\n",
    "\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_path = r\"D:\\Python2\\Attachment\\attachments_log\\log_report6\\error080524014331715_done.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Provide the path to the updated file\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting into ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\authenticated_image (2) setor tunai_164.jfif\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\MUSTIKA RAHMAH_100624_397.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\MUSTIKA RAHMAH_100724_398.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\TITI ENDRI WAHYUNI_100724_399.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\ANGGA MAULIDIN_030524_400.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\ANGGA MAULIDIN_030624_401.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\ANGGA MAULIDIN_030724_402.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\SITI THERESA MIRANTI_200724_403.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\SEPVIKA SANTIKA_080724_404.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\POPY PRAPTININGTYAS SULISTIYAN_080724_405.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\ROSMAL FAUZA LUBIS_150624_407.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\ROSMAL FAUZA LUBIS_150724_408.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\SYILVIA_050724_409.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\SYILVIA_050624_410.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\SYILVIA_050524_411.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\RIDWAN_170624_412.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\RIDWAN_170724_413.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\BETY ROSALINA_150724_414.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\MARIA DELINA_020724_416.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\RIZKI WIRETNO_100724_418.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\SULISTYANINGRUM_100724_419.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\SUFIANI SUFIANI_150724_421.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\FITRIANA SUTRISNO_200724_422.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\CHIKITA FEBRI SUSANTI_150624_423.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(5)\\Report(5)\\CHIKITA FEBRI SUSANTI_150724_424.htm\n",
      "Updated CSV file saved as: D:\\Python2\\Attachment\\attachments_log\\log_report6\\error081224112012504_done2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Attachment\\attachments_log\\log_report6\\error081224112012504.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the file title and the directory from VERSIONDATA\n",
    "    title = row['TITLE']\n",
    "    version_data_path = row['VERSIONDATA']\n",
    "    \n",
    "    # Ensure both TITLE and VERSIONDATA are not null\n",
    "    if pd.notnull(title) and pd.notnull(version_data_path):\n",
    "        # Get the directory path from VERSIONDATA\n",
    "        dir_path = os.path.dirname(version_data_path)\n",
    "        # Define the full file path\n",
    "        full_file_path = os.path.join(dir_path, title)\n",
    "        \n",
    "        # Define the ZIP file path\n",
    "        zip_file_path = full_file_path.replace(os.path.splitext(title)[1], '.zip')\n",
    "        \n",
    "        try:\n",
    "            # Create the ZIP file in the same directory without deleting the original file\n",
    "            with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "                zipf.write(full_file_path, title)\n",
    "            print(f\"Successfully zipped: {full_file_path}\")\n",
    "            \n",
    "            # Update the VERSIONDATA and PATHONCLIENT columns with the new ZIP file path\n",
    "            df.at[index, 'VERSIONDATA'] = zip_file_path\n",
    "            df.at[index, 'PATHONCLIENT'] = zip_file_path\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {full_file_path}\")\n",
    "\n",
    "# Define the new file name by adding \"_done\" to the original file name\n",
    "done_file_path = file_path.replace('.csv', '_done2.csv')\n",
    "\n",
    "# Save the updated DataFrame to the new CSV file\n",
    "df.to_csv(done_file_path, index=False)\n",
    "\n",
    "# Output the path to the new CSV file\n",
    "print(f\"Updated CSV file saved as: {done_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\Attachment\\attachments_log\\log_report3\\error081224104300248.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "df.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attachment generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_exact_size_file(file_path, size_mb):\n",
    "    # Calculate the exact size in bytes\n",
    "    size_bytes = size_mb * 1024 * 1024\n",
    "\n",
    "    # Open the file in write binary ('wb') mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.seek(size_bytes - 1)  # Move to the position one byte before the desired size\n",
    "        file.write(b'\\0')  # Write a single zero byte at this position\n",
    "\n",
    "def generate_files(directory_path, excel_filename, pdf_filename, target_size_mb):\n",
    "    # Ensure the directory ends with a slash\n",
    "    if not directory_path.endswith('/'):\n",
    "        directory_path += '/'\n",
    "\n",
    "    # Generate exact-sized Excel and PDF files\n",
    "    create_exact_size_file(directory_path + excel_filename, target_size_mb)\n",
    "    create_exact_size_file(directory_path + pdf_filename, target_size_mb)\n",
    "\n",
    "    print(f'Generated Excel file at {directory_path + excel_filename} with size exactly {target_size_mb} MB')\n",
    "    print(f'Generated PDF file at {directory_path + pdf_filename} with size exactly {target_size_mb} MB')\n",
    "\n",
    "# Set the directory where files will be saved\n",
    "directory_path = \"/Users/ikawahyuni/dataquality/dummy_files\"\n",
    "\n",
    "# Set filenames and target size\n",
    "generate_files(directory_path, 'dummy_15mb.xlsx', 'dummy_15mb.pdf', 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Excel file at /Users/ikawahyuni/dataquality/dummy_files/dummy_14mb.xlsx with size exactly 14 MB\n",
      "Generated PDF file at /Users/ikawahyuni/dataquality/dummy_files/dummy_14mb.pdf with size exactly 14 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_exact_size_file(file_path, size_mb):\n",
    "    # Calculate the exact size in bytes\n",
    "    size_bytes = size_mb * 1024 * 1024\n",
    "\n",
    "    # Open the file in write binary ('wb') mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.seek(size_bytes - 1)  # Move to the position one byte before the desired size\n",
    "        file.write(b'\\0')  # Write a single zero byte at this position\n",
    "\n",
    "def generate_files(directory_path, excel_filename, pdf_filename, target_size_mb):\n",
    "    # Ensure the directory ends with a slash\n",
    "    if not directory_path.endswith('/'):\n",
    "        directory_path += '/'\n",
    "\n",
    "    # Generate exact-sized Excel and PDF files\n",
    "    create_exact_size_file(directory_path + excel_filename, target_size_mb)\n",
    "    create_exact_size_file(directory_path + pdf_filename, target_size_mb)\n",
    "\n",
    "    print(f'Generated Excel file at {directory_path + excel_filename} with size exactly {target_size_mb} MB')\n",
    "    print(f'Generated PDF file at {directory_path + pdf_filename} with size exactly {target_size_mb} MB')\n",
    "\n",
    "# Set the directory where files will be saved\n",
    "directory_path = \"/Users/ikawahyuni/dataquality/dummy_files\"\n",
    "\n",
    "# Set filenames and target size (change from 15 to 14)\n",
    "generate_files(directory_path, 'dummy_14mb.xlsx', 'dummy_14mb.pdf', 14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maximum size limit of 10,485,760 bytes (10 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Excel file at D:\\dataquality\\dummy_files/dummy.xlsx with size up to 10 MB\n",
      "Generated PDF file at D:\\dataquality\\dummy_files/dummy.pdf with size up to 10 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_exact_size_file(file_path, size_mb):\n",
    "    # Maximum size in bytes (10 MB)\n",
    "    max_size_bytes = 9485760\n",
    "\n",
    "    # Calculate the desired size in bytes\n",
    "    size_bytes = size_mb * 1024 * 1024\n",
    "\n",
    "    # Adjust size if it exceeds the maximum allowed\n",
    "    if size_bytes > max_size_bytes:\n",
    "        size_bytes = max_size_bytes\n",
    "\n",
    "    # Open the file in write binary ('wb') mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.seek(size_bytes - 1)  # Move to the position one byte before the desired size\n",
    "        file.write(b'\\0')  # Write a single zero byte at this position\n",
    "\n",
    "def generate_files(directory_path, excel_filename, pdf_filename, target_size_mb):\n",
    "    # Ensure the directory ends with a slash\n",
    "    if not directory_path.endswith('/'):\n",
    "        directory_path += '/'\n",
    "\n",
    "    # Generate exact-sized Excel and PDF files\n",
    "    create_exact_size_file(directory_path + excel_filename, target_size_mb)\n",
    "    create_exact_size_file(directory_path + pdf_filename, target_size_mb)\n",
    "\n",
    "    actual_size_mb = min(target_size_mb, 10)  # Limit the reported size to 10 MB max\n",
    "    print(f'Generated Excel file at {directory_path + excel_filename} with size up to {actual_size_mb} MB')\n",
    "    print(f'Generated PDF file at {directory_path + pdf_filename} with size up to {actual_size_mb} MB')\n",
    "\n",
    "# Set the directory where files will be saved\n",
    "directory_path = \"D:\\dataquality\\dummy_files\"\n",
    "\n",
    "# Set filenames and target size (e.g., 12 MB would be reduced to 10 MB)\n",
    "generate_files(directory_path, 'dummy.xlsx', 'dummy.pdf', 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool to collect all tickets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ReportTest.zip to D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\n",
      "Data from D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\\ReportTest.arx saved to CSV at D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest_files_info.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import rarfile  # pip install rarfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_process_files(directory):\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        extract_path = os.path.join(directory, filename[:-4]) \n",
    "\n",
    "   \n",
    "        if filename.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        elif filename.endswith(\".rar\"):\n",
    "            with rarfile.RarFile(filepath, 'r') as rar_ref:\n",
    "                rar_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    " \n",
    "        report_file_path = find_report_file(extract_path)\n",
    "        if report_file_path:\n",
    "            df = create_dataframe_from_file(report_file_path, extract_path)\n",
    "            csv_path = os.path.join(directory, filename[:-4] + '_files_info.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n",
    "        else:\n",
    "            print(f\"No 'ReportTest.arx' found in {extract_path}\")\n",
    "\n",
    "def find_report_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'ReportTest.arx':\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            full_path = os.path.join(extract_path, file_name)\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "\n",
    "directory_path = r'D:\\Salesforce\\archive\\dataquality\\Tool for attachments'\n",
    "extract_and_process_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool to collect all tickets 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Report(6).zip to D:\\Python2\\Attachment\\Report(6)\n",
      "Converted 20240727_231535_160.heic to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\20240727_231535_160..zip\n",
      "Converted 6e929343-c3a9-4de4-b96f-ba7c8f45cf51_640.jfif to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\6e929343-c3a9-4de4-b96f-ba7c8f45cf51_640..zip\n",
      "Converted a20983e5-4925-427f-a525-d2739548a59e+-+M.+Abdan+Alkhasbi_759.jfif to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\a20983e5-4925-427f-a525-d2739548a59e+-+M.+Abdan+Alkhasbi_759..zip\n",
      "Converted ADI REMANTO_100724_897.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ADI REMANTO_100724_897.zip\n",
      "Converted AGUSTINUS TANNOS_150724_665.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\AGUSTINUS TANNOS_150724_665.zip\n",
      "Converted ANDANG TJAHJANTOK_150724_885.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ANDANG TJAHJANTOK_150724_885.zip\n",
      "Converted ANDI HARTONO_wa150724_491.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ANDI HARTONO_wa150724_491.zip\n",
      "Converted cecilia perubahan no rek_975.rar to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\cecilia perubahan no rek_975.zip\n",
      "Converted CHRISTIAN SETIAWAN DOLOKSARIBU_150224_924.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\CHRISTIAN SETIAWAN DOLOKSARIBU_150224_924.zip\n",
      "Converted CHRISTIAN SETIAWAN DOLOKSARIBU_150324_925.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\CHRISTIAN SETIAWAN DOLOKSARIBU_150324_925.zip\n",
      "Converted CHRISTIAN SETIAWAN DOLOKSARIBU_150424_926.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\CHRISTIAN SETIAWAN DOLOKSARIBU_150424_926.zip\n",
      "Converted DEA JANE SUNGKONO_150624_576.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\DEA JANE SUNGKONO_150624_576.zip\n",
      "Converted DEA JANE SUNGKONO_150724_577.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\DEA JANE SUNGKONO_150724_577.zip\n",
      "Converted DESSY ROSMARIA_100724_517.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\DESSY ROSMARIA_100724_517.zip\n",
      "Converted ERI SOSIALISMAN_150624_574.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ERI SOSIALISMAN_150624_574.zip\n",
      "Converted ERIK AZARYA ERLAN_200724_561.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ERIK AZARYA ERLAN_200724_561.zip\n",
      "Converted FADLI HAQIQI MAGHRIBI_150724_600.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\FADLI HAQIQI MAGHRIBI_150724_600.zip\n",
      "Converted faf6b70c-8af8-441a-9864-8d8c1b336bae_641.jfif to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\faf6b70c-8af8-441a-9864-8d8c1b336bae_641..zip\n",
      "Converted FANI SYAHRIAL R_250624_550.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\FANI SYAHRIAL R_250624_550.zip\n",
      "Converted FANI SYAHRIAL R_250724_551.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\FANI SYAHRIAL R_250724_551.zip\n",
      "Converted GONAWAN EFENDY_020724_542.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\GONAWAN EFENDY_020724_542.zip\n",
      "Converted HANDY PRADHITYA TJHAN_150724_883.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\HANDY PRADHITYA TJHAN_150724_883.zip\n",
      "Converted hartono golam autodebet_987.rar to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\hartono golam autodebet_987.zip\n",
      "Converted haru no hp_990.rar to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\haru no hp_990.zip\n",
      "Converted JENNI MONALISA NABABAN_150724_899.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\JENNI MONALISA NABABAN_150724_899.zip\n",
      "Converted KRISTIANA SATIO_100724_572.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\KRISTIANA SATIO_100724_572.zip\n",
      "Converted MARGANDATB SITUMORANG_150724_904.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\MARGANDATB SITUMORANG_150724_904.zip\n",
      "Converted NEW_REKENING_KORAN_ONLINE_323401019590537_2024-07-28_2024-07-29_00343994.pdf_429.crdownload to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\NEW_REKENING_KORAN_ONLINE_323401019590537_2024-07-28_2024-07-29_00343994.pdf_429.crdown.zip\n",
      "Converted NEW_REKENING_KORAN_ONLINE_738701011779538_2024-07-25_2024-07-25_00362082.pdf_820.crdownload to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\NEW_REKENING_KORAN_ONLINE_738701011779538_2024-07-25_2024-07-25_00362082.pdf_820.crdown.zip\n",
      "Converted NIKEN NOVIANTY RISDIANASARI_020624_543.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\NIKEN NOVIANTY RISDIANASARI_020624_543.zip\n",
      "Converted RIDO TANAIR_100724_888.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIDO TANAIR_100724_888.zip\n",
      "Converted RIKA ANDINI_150524_940.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIKA ANDINI_150524_940.zip\n",
      "Converted RIKA ANDINI_150624_941.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIKA ANDINI_150624_941.zip\n",
      "Converted RIKA ANDINI_150724_945.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIKA ANDINI_150724_945.zip\n",
      "Converted RMOCHAMAD GANDHANI_150724_534.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RMOCHAMAD GANDHANI_150724_534.zip\n",
      "Converted RUDY YUWONO_plt150724_514.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RUDY YUWONO_plt150724_514.zip\n",
      "Converted RUDY YUWONO_visa150724_513.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RUDY YUWONO_visa150724_513.zip\n",
      "Converted SURYA PURNAMA TJENG_100724_485.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\SURYA PURNAMA TJENG_100724_485.zip\n",
      "Converted SYIFA ALAINA MARICAR_150624_563.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\SYIFA ALAINA MARICAR_150624_563.zip\n",
      "Converted SYIFA ALAINA MARICAR_150724_571.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\SYIFA ALAINA MARICAR_150724_571.zip\n",
      "Converted TEK SUYANTO_200724_508.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\TEK SUYANTO_200724_508.zip\n",
      "Converted THERESIA_150724_532.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\THERESIA_150724_532.zip\n",
      "Converted TITIN NURLAILA_170724_670.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\TITIN NURLAILA_170724_670.zip\n",
      "Converted TRI PANGESTU ADI NAGARA_170724_591.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\TRI PANGESTU ADI NAGARA_170724_591.zip\n",
      "Converted YUNITA SARI DEWI_150424_906.htm to D:\\Python2\\Attachment\\Report(6)\\Report(6)\\YUNITA SARI DEWI_150424_906.zip\n",
      "Converted ._20240727_231535_160.heic to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._20240727_231535_160..zip\n",
      "Converted ._6e929343-c3a9-4de4-b96f-ba7c8f45cf51_640.jfif to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._6e929343-c3a9-4de4-b96f-ba7c8f45cf51_640..zip\n",
      "Converted ._a20983e5-4925-427f-a525-d2739548a59e+-+M.+Abdan+Alkhasbi_759.jfif to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._a20983e5-4925-427f-a525-d2739548a59e+-+M.+Abdan+Alkhasbi_759..zip\n",
      "Converted ._ADI REMANTO_100724_897.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._ADI REMANTO_100724_897.zip\n",
      "Converted ._AGUSTINUS TANNOS_150724_665.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._AGUSTINUS TANNOS_150724_665.zip\n",
      "Converted ._ANDANG TJAHJANTOK_150724_885.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._ANDANG TJAHJANTOK_150724_885.zip\n",
      "Converted ._ANDI HARTONO_wa150724_491.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._ANDI HARTONO_wa150724_491.zip\n",
      "Converted ._cecilia perubahan no rek_975.rar to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._cecilia perubahan no rek_975.zip\n",
      "Converted ._CHRISTIAN SETIAWAN DOLOKSARIBU_150224_924.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._CHRISTIAN SETIAWAN DOLOKSARIBU_150224_924.zip\n",
      "Converted ._CHRISTIAN SETIAWAN DOLOKSARIBU_150324_925.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._CHRISTIAN SETIAWAN DOLOKSARIBU_150324_925.zip\n",
      "Converted ._CHRISTIAN SETIAWAN DOLOKSARIBU_150424_926.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._CHRISTIAN SETIAWAN DOLOKSARIBU_150424_926.zip\n",
      "Converted ._DEA JANE SUNGKONO_150624_576.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._DEA JANE SUNGKONO_150624_576.zip\n",
      "Converted ._DEA JANE SUNGKONO_150724_577.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._DEA JANE SUNGKONO_150724_577.zip\n",
      "Converted ._DESSY ROSMARIA_100724_517.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._DESSY ROSMARIA_100724_517.zip\n",
      "Converted ._ERI SOSIALISMAN_150624_574.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._ERI SOSIALISMAN_150624_574.zip\n",
      "Converted ._ERIK AZARYA ERLAN_200724_561.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._ERIK AZARYA ERLAN_200724_561.zip\n",
      "Converted ._FADLI HAQIQI MAGHRIBI_150724_600.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._FADLI HAQIQI MAGHRIBI_150724_600.zip\n",
      "Converted ._faf6b70c-8af8-441a-9864-8d8c1b336bae_641.jfif to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._faf6b70c-8af8-441a-9864-8d8c1b336bae_641..zip\n",
      "Converted ._FANI SYAHRIAL R_250624_550.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._FANI SYAHRIAL R_250624_550.zip\n",
      "Converted ._FANI SYAHRIAL R_250724_551.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._FANI SYAHRIAL R_250724_551.zip\n",
      "Converted ._GONAWAN EFENDY_020724_542.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._GONAWAN EFENDY_020724_542.zip\n",
      "Converted ._HANDY PRADHITYA TJHAN_150724_883.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._HANDY PRADHITYA TJHAN_150724_883.zip\n",
      "Converted ._hartono golam autodebet_987.rar to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._hartono golam autodebet_987.zip\n",
      "Converted ._haru no hp_990.rar to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._haru no hp_990.zip\n",
      "Converted ._JENNI MONALISA NABABAN_150724_899.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._JENNI MONALISA NABABAN_150724_899.zip\n",
      "Converted ._KRISTIANA SATIO_100724_572.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._KRISTIANA SATIO_100724_572.zip\n",
      "Converted ._MARGANDATB SITUMORANG_150724_904.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._MARGANDATB SITUMORANG_150724_904.zip\n",
      "Converted ._NEW_REKENING_KORAN_ONLINE_323401019590537_2024-07-28_2024-07-29_00343994.pdf_429.crdownload to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._NEW_REKENING_KORAN_ONLINE_323401019590537_2024-07-28_2024-07-29_00343994.pdf_429.crdown.zip\n",
      "Converted ._NEW_REKENING_KORAN_ONLINE_738701011779538_2024-07-25_2024-07-25_00362082.pdf_820.crdownload to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._NEW_REKENING_KORAN_ONLINE_738701011779538_2024-07-25_2024-07-25_00362082.pdf_820.crdown.zip\n",
      "Converted ._NIKEN NOVIANTY RISDIANASARI_020624_543.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._NIKEN NOVIANTY RISDIANASARI_020624_543.zip\n",
      "Converted ._RIDO TANAIR_100724_888.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._RIDO TANAIR_100724_888.zip\n",
      "Converted ._RIKA ANDINI_150524_940.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._RIKA ANDINI_150524_940.zip\n",
      "Converted ._RIKA ANDINI_150624_941.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._RIKA ANDINI_150624_941.zip\n",
      "Converted ._RIKA ANDINI_150724_945.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._RIKA ANDINI_150724_945.zip\n",
      "Converted ._RMOCHAMAD GANDHANI_150724_534.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._RMOCHAMAD GANDHANI_150724_534.zip\n",
      "Converted ._RUDY YUWONO_plt150724_514.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._RUDY YUWONO_plt150724_514.zip\n",
      "Converted ._RUDY YUWONO_visa150724_513.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._RUDY YUWONO_visa150724_513.zip\n",
      "Converted ._SURYA PURNAMA TJENG_100724_485.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._SURYA PURNAMA TJENG_100724_485.zip\n",
      "Converted ._SYIFA ALAINA MARICAR_150624_563.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._SYIFA ALAINA MARICAR_150624_563.zip\n",
      "Converted ._SYIFA ALAINA MARICAR_150724_571.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._SYIFA ALAINA MARICAR_150724_571.zip\n",
      "Converted ._TEK SUYANTO_200724_508.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._TEK SUYANTO_200724_508.zip\n",
      "Converted ._THERESIA_150724_532.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._THERESIA_150724_532.zip\n",
      "Converted ._TITIN NURLAILA_170724_670.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._TITIN NURLAILA_170724_670.zip\n",
      "Converted ._TRI PANGESTU ADI NAGARA_170724_591.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._TRI PANGESTU ADI NAGARA_170724_591.zip\n",
      "Converted ._YUNITA SARI DEWI_150424_906.htm to D:\\Python2\\Attachment\\Report(6)\\__MACOSX\\Report(6)\\._YUNITA SARI DEWI_150424_906.zip\n",
      "Data from D:\\Python2\\Attachment\\Report(6)\\Report(6)\\Report.arx saved to CSV at D:\\Python2\\Attachment\\Report(6)_files_info.csv\n",
      "Extracted Report(7).zip to D:\\Python2\\Attachment\\Report(7)\n",
      "Converted 54204961a.jpeg_168.crdownload to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\54204961a.jpeg_168.crdown.zip\n",
      "Converted BBKHIT SULAWESI TENGGARA_0724_381.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\BBKHIT SULAWESI TENGGARA_0724_381.zip\n",
      "Converted DEWI CORRY_150724_403.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\DEWI CORRY_150724_403.zip\n",
      "Converted DEWI CORRY_150724_404.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\DEWI CORRY_150724_404.zip\n",
      "Converted IRVAN PANJAITAN_ptlt100724_405.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\IRVAN PANJAITAN_ptlt100724_405.zip\n",
      "Converted JUSRI_080724_398.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\JUSRI_080724_398.zip\n",
      "Converted LEDYANA CHRISTINE SUMITRO_170724_380.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\LEDYANA CHRISTINE SUMITRO_170724_380.zip\n",
      "Converted NENO RIANA_080724_378.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\NENO RIANA_080724_378.zip\n",
      "Converted OENTOENG SUBAGIO_jcb100724_410.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\OENTOENG SUBAGIO_jcb100724_410.zip\n",
      "Converted OENTOENG SUBAGIO_PLT100724_406.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\OENTOENG SUBAGIO_PLT100724_406.zip\n",
      "Converted RUSWANTORO WIDJAJA_150724_400.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\RUSWANTORO WIDJAJA_150724_400.zip\n",
      "Converted SITI BAINAH_080724_34.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SITI BAINAH_080724_34.zip\n",
      "Converted SRI HARJA_150724_401.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SRI HARJA_150724_401.zip\n",
      "Converted SUDIRMAN_080724_397.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SUDIRMAN_080724_397.zip\n",
      "Converted SUSANTO_170724_399.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SUSANTO_170724_399.zip\n",
      "Converted TINA ERINDA FEBRIANI_170724_407.htm to D:\\Python2\\Attachment\\Report(7)\\Report(7)\\TINA ERINDA FEBRIANI_170724_407.zip\n",
      "Converted ._54204961a.jpeg_168.crdownload to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._54204961a.jpeg_168.crdown.zip\n",
      "Converted ._BBKHIT SULAWESI TENGGARA_0724_381.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._BBKHIT SULAWESI TENGGARA_0724_381.zip\n",
      "Converted ._DEWI CORRY_150724_403.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._DEWI CORRY_150724_403.zip\n",
      "Converted ._DEWI CORRY_150724_404.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._DEWI CORRY_150724_404.zip\n",
      "Converted ._IRVAN PANJAITAN_ptlt100724_405.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._IRVAN PANJAITAN_ptlt100724_405.zip\n",
      "Converted ._JUSRI_080724_398.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._JUSRI_080724_398.zip\n",
      "Converted ._LEDYANA CHRISTINE SUMITRO_170724_380.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._LEDYANA CHRISTINE SUMITRO_170724_380.zip\n",
      "Converted ._NENO RIANA_080724_378.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._NENO RIANA_080724_378.zip\n",
      "Converted ._OENTOENG SUBAGIO_jcb100724_410.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._OENTOENG SUBAGIO_jcb100724_410.zip\n",
      "Converted ._OENTOENG SUBAGIO_PLT100724_406.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._OENTOENG SUBAGIO_PLT100724_406.zip\n",
      "Converted ._RUSWANTORO WIDJAJA_150724_400.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._RUSWANTORO WIDJAJA_150724_400.zip\n",
      "Converted ._SITI BAINAH_080724_34.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._SITI BAINAH_080724_34.zip\n",
      "Converted ._SRI HARJA_150724_401.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._SRI HARJA_150724_401.zip\n",
      "Converted ._SUDIRMAN_080724_397.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._SUDIRMAN_080724_397.zip\n",
      "Converted ._SUSANTO_170724_399.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._SUSANTO_170724_399.zip\n",
      "Converted ._TINA ERINDA FEBRIANI_170724_407.htm to D:\\Python2\\Attachment\\Report(7)\\__MACOSX\\Report(7)\\._TINA ERINDA FEBRIANI_170724_407.zip\n",
      "Data from D:\\Python2\\Attachment\\Report(7)\\Report(7)\\Report.arx saved to CSV at D:\\Python2\\Attachment\\Report(7)_files_info.csv\n",
      "Extracted to upload on bridrive_8 aug.zip to D:\\Python2\\Attachment\\to upload on bridrive_8 aug\n",
      "No 'ReportTest.arx' found in D:\\Python2\\Attachment\\to upload on bridrive_8 aug\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import rarfile  # pip install rarfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_process_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # Extract the big zip file\n",
    "        if filename.endswith(\".zip\"):\n",
    "            extract_path = os.path.join(directory, filename[:-4])\n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "\n",
    "            # Convert specific file types to zip within the extracted folder\n",
    "            convert_files_to_zip(extract_path)\n",
    "\n",
    "            # Process the extracted folder to find 'ReportTest.arx' and create the DataFrame\n",
    "            report_file_path = find_report_file(extract_path)\n",
    "            if report_file_path:\n",
    "                df = create_dataframe_from_file(report_file_path, extract_path)\n",
    "                csv_path = os.path.join(directory, filename[:-4] + '_files_info.csv')\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n",
    "            else:\n",
    "                print(f\"No 'ReportTest.arx' found in {extract_path}\")\n",
    "\n",
    "def convert_files_to_zip(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".txt\", \".rtp\", \".tif\", \".rar\",\".rptdesign\",\".js\",\".jfif\",\".crdownload\",\".heic\",\".htm\")):\n",
    "                file_path = os.path.join(root, file)\n",
    "                new_zip_path = file_path[:-4] + \".zip\"\n",
    "                with zipfile.ZipFile(new_zip_path, 'w') as zip_ref:\n",
    "                    zip_ref.write(file_path, file)\n",
    "                os.remove(file_path)  # Optionally remove the original file\n",
    "                print(f\"Converted {file} to {new_zip_path}\")\n",
    "\n",
    "def find_report_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'Report.arx':\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    path_on_client = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            # Check if the file was converted to zip\n",
    "            original_file_path = os.path.join(extract_path, file_name)\n",
    "            zip_file_path = original_file_path[:-4] + \".zip\"\n",
    "            if os.path.exists(zip_file_path):\n",
    "                full_path = zip_file_path\n",
    "            else:\n",
    "                full_path = original_file_path\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            path_on_client.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'PathOnClient': path_on_client,\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "directory_path = r\"D:\\Python2\\Attachment\"\n",
    "extract_and_process_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ReportTest.zip to D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\n",
      "Data from D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest\\ReportTest.arx saved to CSV at D:\\Salesforce\\archive\\dataquality\\Tool for attachments\\ReportTest_files_info.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import rarfile  # pip install rarfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_process_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        extract_path = os.path.join(directory, filename[:-4]) \n",
    "\n",
    "        if filename.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        elif filename.endswith(\".rar\"):\n",
    "            with rarfile.RarFile(filepath, 'r') as rar_ref:\n",
    "                rar_ref.extractall(extract_path)\n",
    "                print(f\"Extracted {filename} to {extract_path}\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        report_file_path = find_report_file(extract_path)\n",
    "        if report_file_path:\n",
    "            df = create_dataframe_from_file(report_file_path, extract_path)\n",
    "            csv_path = os.path.join(directory, filename[:-4] + '_files_info.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n",
    "        else:\n",
    "            print(f\"No 'ReportTest.arx' found in {extract_path}\")\n",
    "\n",
    "def find_report_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'ReportTest.arx':\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            full_path = os.path.join(extract_path, file_name)\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'PathOnClient': version_data,  # New column added here\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "directory_path = r'D:\\Salesforce\\archive\\dataquality\\Tool for attachments'\n",
    "extract_and_process_files(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.arx saved to CSV at C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_dataframe_from_file(file_path, extract_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    version_data = []\n",
    "    first_publish_location_ids = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DATA\"):\n",
    "            parts = line.strip().split('\"')\n",
    "            ticket_number = parts[1].strip()\n",
    "            file_name = parts[3].strip().partition(' ')[2]  # Extracting everything after \"1 \"\n",
    "\n",
    "            full_path = os.path.join(r'E:\\Attachment\\report1', file_name)\n",
    "\n",
    "            titles.append(file_name)\n",
    "            descriptions.append(\"\")\n",
    "            version_data.append(full_path)\n",
    "            first_publish_location_ids.append(ticket_number)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'VersionData': version_data,\n",
    "        'PathOnClient': version_data,  # New column added here\n",
    "        'FirstPublishLocationId': first_publish_location_ids\n",
    "    })\n",
    "\n",
    "# Assume this is the path to the report file you uploaded\n",
    "report_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.arx\"\n",
    "# Create dataframe\n",
    "df = create_dataframe_from_file(report_file_path, 'E:\\\\Attachment\\\\report1')\n",
    "# Save to CSV\n",
    "csv_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\attachment\\Report.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Data from {report_file_path} saved to CSV at {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create copied 1000 ine\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path=r\"C:\\Users\\maste\\Downloads\\bricare\\file_for_attachment.csv\"\n",
    "df=pd.read_csv(path)\n",
    "\n",
    "df = pd.concat([df] * 1000, ignore_index=True)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to change the column FirstPublishLocationId with all values in ID column taken from case ID in salesforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'FirstPublishLocationId' column has been replaced with the 'ID' column values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>VersionData</th>\n",
       "      <th>PathOnClient</th>\n",
       "      <th>FirstPublishLocationId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KLYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KMYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KNYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KOYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449KPYAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LtYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LuYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LvYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LwYAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>attachment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf</td>\n",
       "      <td>/Users/ikawahyuni/dataquality/large_dummy_file...</td>\n",
       "      <td>500MR00000449LxYAI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Title  Description                              VersionData  \\\n",
       "900  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "901  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "902  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "903  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "904  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "..          ...          ...                                      ...   \n",
       "995  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "996  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "997  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "998  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "999  attachment          NaN  C:\\Users\\lenovo\\Downloads\\dummy15mb.pdf   \n",
       "\n",
       "                                          PathOnClient FirstPublishLocationId  \n",
       "900  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KLYAY  \n",
       "901  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KMYAY  \n",
       "902  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KNYAY  \n",
       "903  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KOYAY  \n",
       "904  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449KPYAY  \n",
       "..                                                 ...                    ...  \n",
       "995  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LtYAI  \n",
       "996  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LuYAI  \n",
       "997  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LvYAI  \n",
       "998  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LwYAI  \n",
       "999  /Users/ikawahyuni/dataquality/large_dummy_file...     500MR00000449LxYAI  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df1 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\bricare\\file_for_attachment.csv\")  # Assuming this file contains the columns Title, Description, VersionData, PathOnClient, FirstPublishLocationId\n",
    "df2 = pd.read_csv(r\"C:\\Users\\maste\\Downloads\\bricare\\extract_case_id_1000.csv\")  # Assuming this file contains the column ID\n",
    "\n",
    "# Check if both DataFrames have the same number of rows\n",
    "if len(df1) != len(df2):\n",
    "    raise ValueError(\"The number of rows in both files must be the same\")\n",
    "\n",
    "# Replace the 'FirstPublishLocationId' column in df1 with the 'ID' column from df2\n",
    "df1['FirstPublishLocationId'] = df2['ID']\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "# df1.to_csv('modified_file.csv', index=False)\n",
    "\n",
    "print(\"The 'FirstPublishLocationId' column has been replaced with the 'ID' column values.\")\n",
    "\n",
    "df1.iloc[900:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File split_file_1.csv created with rows from 0 to 100\n",
      "File split_file_2.csv created with rows from 100 to 200\n",
      "File split_file_3.csv created with rows from 200 to 300\n",
      "File split_file_4.csv created with rows from 300 to 400\n",
      "File split_file_5.csv created with rows from 400 to 500\n",
      "Files created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Slice into 5 files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\bricare\\file_for_attachment.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Number of rows per split file\n",
    "rows_per_file = 100\n",
    "\n",
    "# Create 5 files with 200 rows each\n",
    "for i in range(5):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = (i + 1) * rows_per_file\n",
    "    split_df = df.iloc[start_row:end_row]\n",
    "    split_file_path = f'split_file_{i + 1}.csv'\n",
    "    split_df.to_csv(split_file_path, index=False)\n",
    "    print(f'File {split_file_path} created with rows from {start_row} to {end_row}')\n",
    "\n",
    "print('Files created successfully.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Report8.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\report7.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace 'report1' with 'Report2' in the 'VersionData' and 'PathOnClient' columns\n",
    "data['VersionData'] = data['VersionData'].str.replace('report1', 'Report8')\n",
    "data['PathOnClient'] = data['PathOnClient'].str.replace('report1', 'Report8')\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "modified_file_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Report8.csv\"\n",
    "data.to_csv(modified_file_path, index=False)\n",
    "\n",
    "# Provide the path to the modified file\n",
    "print(f'Modified file saved at: {modified_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report2.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report3.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report4.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report5.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report6.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report7.csv\n",
      "Modified file saved at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\\Modified_Report8.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "folder_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\server\\Report csv\\Report csv\"  \n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Replace drive 'D' with 'E' in the 'VersionData' and 'PathOnClient' columns\n",
    "        data['VersionData'] = data['VersionData'].str.replace('D:', 'E:')\n",
    "        data['PathOnClient'] = data['PathOnClient'].str.replace('D:', 'E:')\n",
    "        \n",
    "        # Save the modified DataFrame to a new CSV file\n",
    "        modified_file_path = os.path.join(folder_path, f'Modified_{filename}')\n",
    "        data.to_csv(modified_file_path, index=False)\n",
    "\n",
    "        print(f'Modified file saved at: {modified_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python2\\\\Attachment\\\\attachments_log\\\\log_report8\\\\error080524022045731_done.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Attachment\\attachments_log\\log_report8\\error080524022045731.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the new folder path\n",
    "new_folder_path = \"D:\\Python2\\Attachment\\Report(7)\\Report(7)\"\n",
    "\n",
    "# Update the VERSIONDATA and PATHONCLIENT columns\n",
    "# df['VERSIONDATA'] = df['VERSIONDATA'].apply(lambda x: new_folder_path + x.split('Report(4)')[-1].replace('/', '\\\\'))\n",
    "# df['PATHONCLIENT'] = df['PATHONCLIENT'].apply(lambda x: new_folder_path + x.split('Report(4)')[-1].replace('/', '\\\\'))\n",
    "\n",
    "df['VersionData'] = df['VersionData'].apply(lambda x: os.path.join(new_folder_path, os.path.basename(x)).replace('/', '\\\\'))\n",
    "df['PathOnClient'] = df['PathOnClient'].apply(lambda x: os.path.join(new_folder_path, os.path.basename(x)).replace('/', '\\\\'))\n",
    "\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_path = r\"D:\\Python2\\Attachment\\attachments_log\\log_report8\\error080524022045731_done.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Provide the path to the updated file\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting into ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SITI BAINAH_080724_34.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\ABSENSI MANUAL NINIK_127.odt\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\Invoice Blibli I Wayan_138.csv\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\54204961a.jpeg_168.crdownload\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\NENO RIANA_080724_378.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\LEDYANA CHRISTINE SUMITRO_170724_380.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\BBKHIT SULAWESI TENGGARA_0724_381.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SUDIRMAN_080724_397.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\JUSRI_080724_398.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SUSANTO_170724_399.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\RUSWANTORO WIDJAJA_150724_400.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\SRI HARJA_150724_401.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\TTB000054221593_402.doc\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\DEWI CORRY_150724_403.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\DEWI CORRY_150724_404.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\IRVAN PANJAITAN_ptlt100724_405.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\OENTOENG SUBAGIO_PLT100724_406.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\TINA ERINDA FEBRIANI_170724_407.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(7)\\Report(7)\\OENTOENG SUBAGIO_jcb100724_410.htm\n",
      "Updated CSV file saved as: D:\\Python2\\Attachment\\attachments_log\\log_report8\\error081224021733461_done2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Attachment\\attachments_log\\log_report8\\error081224021733461.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the file title and the directory from VERSIONDATA\n",
    "    title = row['TITLE']\n",
    "    version_data_path = row['VERSIONDATA']\n",
    "    \n",
    "    # Ensure both TITLE and VERSIONDATA are not null\n",
    "    if pd.notnull(title) and pd.notnull(version_data_path):\n",
    "        # Get the directory path from VERSIONDATA\n",
    "        dir_path = os.path.dirname(version_data_path)\n",
    "        # Define the full file path\n",
    "        full_file_path = os.path.join(dir_path, title)\n",
    "        \n",
    "        # Define the ZIP file path\n",
    "        zip_file_path = full_file_path.replace(os.path.splitext(title)[1], '.zip')\n",
    "        \n",
    "        try:\n",
    "            # Create the ZIP file in the same directory without deleting the original file\n",
    "            with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "                zipf.write(full_file_path, title)\n",
    "            print(f\"Successfully zipped: {full_file_path}\")\n",
    "            \n",
    "            # Update the VERSIONDATA and PATHONCLIENT columns with the new ZIP file path\n",
    "            df.at[index, 'VERSIONDATA'] = zip_file_path\n",
    "            df.at[index, 'PATHONCLIENT'] = zip_file_path\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {full_file_path}\")\n",
    "\n",
    "# Define the new file name by adding \"_done\" to the original file name\n",
    "done_file_path = file_path.replace('.csv', '_done2.csv')\n",
    "\n",
    "# Save the updated DataFrame to the new CSV file\n",
    "df.to_csv(done_file_path, index=False)\n",
    "\n",
    "# Output the path to the new CSV file\n",
    "print(f\"Updated CSV file saved as: {done_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting into ZIP for files without extentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\20240727_231535_160.heic\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\NEW_REKENING_KORAN_ONLINE_323401019590537_2024-07-28_2024-07-29_00343994.pdf_429.crdownload\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\SURYA PURNAMA TJENG_100724_485.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ANDI HARTONO_wa150724_491.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\TEK SUYANTO_200724_508.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RUDY YUWONO_visa150724_513.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RUDY YUWONO_plt150724_514.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\DESSY ROSMARIA_100724_517.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\THERESIA_150724_532.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RMOCHAMAD GANDHANI_150724_534.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\GONAWAN EFENDY_020724_542.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\NIKEN NOVIANTY RISDIANASARI_020624_543.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\FANI SYAHRIAL R_250624_550.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\FANI SYAHRIAL R_250724_551.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ERIK AZARYA ERLAN_200724_561.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\SYIFA ALAINA MARICAR_150624_563.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\SYIFA ALAINA MARICAR_150724_571.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\KRISTIANA SATIO_100724_572.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ERI SOSIALISMAN_150624_574.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\DEA JANE SUNGKONO_150624_576.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\DEA JANE SUNGKONO_150724_577.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\TRI PANGESTU ADI NAGARA_170724_591.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\FADLI HAQIQI MAGHRIBI_150724_600.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\6e929343-c3a9-4de4-b96f-ba7c8f45cf51_640.jfif\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\faf6b70c-8af8-441a-9864-8d8c1b336bae_641.jfif\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\AGUSTINUS TANNOS_150724_665.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\TITIN NURLAILA_170724_670.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\a20983e5-4925-427f-a525-d2739548a59e+-+M.+Abdan+Alkhasbi_759.jfif\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\NEW_REKENING_KORAN_ONLINE_738701011779538_2024-07-25_2024-07-25_00362082.pdf_820.crdownload\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\HANDY PRADHITYA TJHAN_150724_883.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ANDANG TJAHJANTOK_150724_885.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIDO TANAIR_100724_888.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\ADI REMANTO_100724_897.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\JENNI MONALISA NABABAN_150724_899.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\MARGANDATB SITUMORANG_150724_904.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\YUNITA SARI DEWI_150424_906.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\CHRISTIAN SETIAWAN DOLOKSARIBU_150224_924.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\CHRISTIAN SETIAWAN DOLOKSARIBU_150324_925.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\CHRISTIAN SETIAWAN DOLOKSARIBU_150424_926.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIKA ANDINI_150524_940.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIKA ANDINI_150624_941.htm\n",
      "File not found: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\RIKA ANDINI_150724_945.htm\n",
      "Successfully zipped: D:\\Python2\\Attachment\\Report(6)\\Report(6)\\image-1545708299631195_974.tmp\n",
      "Updated CSV file saved as: D:\\Python2\\Attachment\\attachments_log\\log_report7\\error081224023217824_done4.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Python2\\Attachment\\attachments_log\\log_report7\\error081224023217824.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    title = row.get('TITLE', None)\n",
    "    version_data_path = row.get('VERSIONDATA', None)\n",
    "    \n",
    "    if pd.notnull(title) and pd.notnull(version_data_path):\n",
    "        dir_path = os.path.dirname(version_data_path)\n",
    "        full_file_path = os.path.join(dir_path, title)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if os.path.exists(full_file_path):\n",
    "            # Handle files without an extension\n",
    "            if not os.path.splitext(title)[1]:\n",
    "                temp_file_path = full_file_path + '.tmp'\n",
    "                shutil.copy(full_file_path, temp_file_path)\n",
    "                \n",
    "                # Zip the file with the temporary extension\n",
    "                zip_file_path = temp_file_path.replace('.tmp', '.zip')\n",
    "                \n",
    "                try:\n",
    "                    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "                        zipf.write(temp_file_path, os.path.basename(temp_file_path))\n",
    "                    print(f\"Successfully zipped: {temp_file_path}\")\n",
    "                    \n",
    "                    # Update the DataFrame with the new ZIP file path\n",
    "                    df.at[index, 'VERSIONDATA'] = zip_file_path\n",
    "                    df.at[index, 'PATHONCLIENT'] = zip_file_path\n",
    "                    \n",
    "                    # Clean up the temporary file\n",
    "                    os.remove(temp_file_path)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error zipping file {temp_file_path}: {str(e)}\")\n",
    "            else:\n",
    "                # If the file has an extension, handle it normally\n",
    "                zip_file_path = full_file_path.replace(os.path.splitext(title)[1], '.zip')\n",
    "                \n",
    "                try:\n",
    "                    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n",
    "                        zipf.write(full_file_path, title)\n",
    "                    print(f\"Successfully zipped: {full_file_path}\")\n",
    "                    \n",
    "                    # Update the DataFrame with the new ZIP file path\n",
    "                    df.at[index, 'VERSIONDATA'] = zip_file_path\n",
    "                    df.at[index, 'PATHONCLIENT'] = zip_file_path\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error zipping file {full_file_path}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"File not found: {full_file_path}\")\n",
    "\n",
    "# Define the new file name by adding \"_done\" to the original file name\n",
    "done_file_path = file_path.replace('.csv', '_done4.csv')\n",
    "\n",
    "# Save the updated DataFrame to the new CSV file\n",
    "df.to_csv(done_file_path, index=False)\n",
    "\n",
    "# Output the path to the new CSV file\n",
    "print(f\"Updated CSV file saved as: {done_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To attach files to cases aftyer closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at: C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extractfile_alex7_done.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_1_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extractfile_alex7_from_file.csv\"\n",
    "file_2_path = r\"C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extract_content_doc_link.csv\"\n",
    "\n",
    "\n",
    "df_extract = pd.read_csv(file_1_path)\n",
    "df_content_doc_link = pd.read_csv(file_2_path)\n",
    "\n",
    "# Merging the two dataframes based on the 'CONTENTDOCUMENTID' column\n",
    "df_merged = pd.merge(df_extract, df_content_doc_link[['CONTENTDOCUMENTID', 'ID', 'LINKEDENTITYID']], on='CONTENTDOCUMENTID', how='left')\n",
    "\n",
    "\n",
    "output_file_path = r'C:\\Users\\maste\\Downloads\\dataloader_v60.0.2\\extractfile_alex7_done.csv'\n",
    "df_merged.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"File saved successfully at:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the extract closed tickets in Prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 1622715    TTB000053954652\n",
       "1730066    TTB000053979039\n",
       "1734478    TTB000054028106\n",
       "1772286    TTB000053835402\n",
       "1772287    TTB000053835650\n",
       "                ...       \n",
       "3567070    TTB000054110279\n",
       "3567074    TTB000054174800\n",
       "3567080    TTB000053921665\n",
       "3567085    TTB000054156889\n",
       "3567086    TTB000054179864\n",
       "Name: SCC_LEGACY_TICKET_ID__C, Length: 3087, dtype: object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined report file\n",
    "combined_file_path = r\"D:\\Python2\\attachment_for_closedcases\\combined_report.csv\"\n",
    "combined_df = pd.read_csv(combined_file_path)\n",
    "\n",
    "# Load the extract file\n",
    "extract_file_path = r\"D:\\Python2\\attachment_for_closedcases\\extract_case_closed_24.csv\"\n",
    "extract_df = pd.read_csv(extract_file_path)\n",
    "\n",
    "# Filter the extract file based on the SCC_LEGACY_TICKET_ID__C in the combined file\n",
    "filtered_extract_df = extract_df[extract_df['SCC_LEGACY_TICKET_ID__C'].isin(combined_df['SCC_LEGACY_TICKET_ID__C'])]\n",
    "filtered_extract_df=filtered_extract_df[filtered_extract_df['STATUS']=='Closed']\n",
    "\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "# filtered_output_path = r'D:\\Python2\\attachment_for_closedcases\\filtered_extract.csv'\n",
    "# filtered_extract_df.to_csv(filtered_output_path, index=False)\n",
    "\n",
    "# Display the filtered dataframe to confirm (optional)\n",
    "# print(filtered_extract_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all Report Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved as D:\\Python2\\cekcek\\Report csv\\combined_report.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder_path = r\"D:\\Python2\\cekcek\\Report csv\"\n",
    "\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "       \n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "       \n",
    "        df['source_file'] = filename\n",
    "        \n",
    "       \n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes in the list into one dataframe\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "output_file = 'D:\\Python2\\cekcek\\Report csv\\combined_report.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Combined file saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TTB000049954663', 'TTB000054264095')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\cekcek\\Report csv\\combined_report.csv\"\n",
    "\n",
    "df=pd.read_csv(path)\n",
    "min_value = df['FirstPublishLocationId'].min()\n",
    "max_value = df['FirstPublishLocationId'].max()\n",
    "min_value, max_value\n",
    "\n",
    "\n",
    "\n",
    "# FirstPublishLocationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r\"D:\\Python2\\attachment_for_closedcases\\content_version_prod - Copy.xlsx\"\n",
    "\n",
    "df=pd.read_excel(path)\n",
    "df=df.iloc[:100]\n",
    "# df.to_csv(\"D:\\Python2\\attachment_for_closedcases\\content_version_done.csv\",index=False)\n",
    "df=df.drop(columns=['_'])\n",
    "df.to_csv('content_version_done.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter content version based on combined report files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data with 'SCC_LEGACY_TICKET_ID__C' has been saved to 'filtered_content_version_prod.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file and CSV file\n",
    "excel_file_path = r\"D:\\Python2\\attachment_for_closedcases\\content_version_prod.xlsx\"\n",
    "csv_file_path = r\"D:\\Python2\\attachment_for_closedcases\\combined_report.csv\"\n",
    "\n",
    "# Read the Excel and CSV files\n",
    "excel_df = pd.read_excel(excel_file_path)\n",
    "csv_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Clean the 'Title' columns by converting them to lowercase and stripping any extra spaces\n",
    "excel_df['Title_cleaned'] = excel_df['Title'].str.lower().str.strip()\n",
    "csv_df['Title_cleaned'] = csv_df['Title'].str.lower().str.strip()\n",
    "\n",
    "# Filter the Excel DataFrame based on the titles in the CSV DataFrame\n",
    "filtered_excel_df = excel_df[excel_df['Title_cleaned'].isin(csv_df['Title_cleaned'])]\n",
    "\n",
    "# Merge the filtered Excel DataFrame with the CSV DataFrame to include the 'FirstPublishLocationId'\n",
    "merged_df = pd.merge(\n",
    "    filtered_excel_df,\n",
    "    csv_df[['Title_cleaned', 'FirstPublishLocationId']],\n",
    "    on='Title_cleaned',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the helper 'Title_cleaned' column used for merging\n",
    "merged_df.drop(columns=['Title_cleaned'], inplace=True)\n",
    "\n",
    "# Rename the 'FirstPublishLocationId_y' column to 'SCC_LEGACY_TICKET_ID__C'\n",
    "merged_df.rename(columns={'FirstPublishLocationId_y': 'SCC_LEGACY_TICKET_ID__C'}, inplace=True)\n",
    "\n",
    "# Save the merged data to a new Excel file\n",
    "merged_df.to_excel(r'D:\\Python2\\attachment_for_closedcases\\filtered_content_version_prod2.xlsx', index=False)\n",
    "merged_df.to_csv(r'D:\\Python2\\attachment_for_closedcases\\filtered_content_version_prod2.csv', index=False)\n",
    "\n",
    "print(\"Filtered data with 'SCC_LEGACY_TICKET_ID__C' has been saved to 'filtered_content_version_prod.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Attachments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to D:\\attachment\\attachment_batch2\\combined_success_files_batch2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder where the files are located\n",
    "folder_path = r\"D:\\attachment\\attachment_batch2\"\n",
    "\n",
    "# List to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if 'success' in filename:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Check if the file is empty\n",
    "        if os.path.getsize(file_path) > 0:\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Check if the dataframe is not empty\n",
    "                if not df.empty:\n",
    "                    # Append the dataframe to the list\n",
    "                    dataframes.append(df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"EmptyDataError: {filename} is empty or has no valid data.\")\n",
    "        else:\n",
    "            print(f\"Skipping {filename} as it is empty.\")\n",
    "\n",
    "# Check if any dataframes were added\n",
    "if dataframes:\n",
    "    # Combine all dataframes into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file_path = os.path.join(folder_path, 'combined_success_files_batch2.csv')\n",
    "\n",
    "    # Save the combined dataframe to a CSV file\n",
    "    combined_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f'Combined file saved to {output_file_path}')\n",
    "else:\n",
    "    print(\"No valid data to combine.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>VersionData</th>\n",
       "      <th>PathOnClient</th>\n",
       "      <th>FirstPublishLocationId</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>VERSIONDATA</th>\n",
       "      <th>PATHONCLIENT</th>\n",
       "      <th>FIRSTPUBLISHLOCATIONID</th>\n",
       "      <th>ERROR</th>\n",
       "      <th>ERROR.1</th>\n",
       "      <th>ERROR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>068Mg0000012T3lIAE</td>\n",
       "      <td>b275766a-c815-47d1-9c2b-c28c22f9e4c1_0.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\b275766a-c815-47d1-9c2b-...</td>\n",
       "      <td>E:\\Attachment\\report1\\b275766a-c815-47d1-9c2b-...</td>\n",
       "      <td>TTB000053312948</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>068Mg0000012T5NIAU</td>\n",
       "      <td>5cd8977b-66d6-4a03-a8a1-460483bc48fc_1.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\5cd8977b-66d6-4a03-a8a1-...</td>\n",
       "      <td>E:\\Attachment\\report1\\5cd8977b-66d6-4a03-a8a1-...</td>\n",
       "      <td>TTB000053312948</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>068Mg0000012T6zIAE</td>\n",
       "      <td>ec2cd765-7d6d-452a-beac-bcf7ceae2360_2.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\ec2cd765-7d6d-452a-beac-...</td>\n",
       "      <td>E:\\Attachment\\report1\\ec2cd765-7d6d-452a-beac-...</td>\n",
       "      <td>TTB000053312948</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>068Mg0000012T8bIAE</td>\n",
       "      <td>8fe5b013-f1ac-455c-83d0-288aa809ade7_3.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\8fe5b013-f1ac-455c-83d0-...</td>\n",
       "      <td>E:\\Attachment\\report1\\8fe5b013-f1ac-455c-83d0-...</td>\n",
       "      <td>TTB000053312948</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>068Mg0000012TADIA2</td>\n",
       "      <td>8c7142fd-2c3e-4d8b-b184-f49a12ce6373_4.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\8c7142fd-2c3e-4d8b-b184-...</td>\n",
       "      <td>E:\\Attachment\\report1\\8c7142fd-2c3e-4d8b-b184-...</td>\n",
       "      <td>TTB000053312948</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>068Mg0000012WEaIAM</td>\n",
       "      <td>TTB000053745871(2)_897.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053745871(2)_897.png</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053745871(2)_897.png</td>\n",
       "      <td>TTB000053745871</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>068Mg0000012TTbIAM</td>\n",
       "      <td>TTB000053719011 INVOICE_898.xlsx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053719011 INVOICE_...</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053719011 INVOICE_...</td>\n",
       "      <td>TTB000053719011</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>068Mg0000012UEOIA2</td>\n",
       "      <td>TTB000053719011(1)_899.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053719011(1)_899.png</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053719011(1)_899.png</td>\n",
       "      <td>TTB000053719011</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>068Mg0000012UrCIAU</td>\n",
       "      <td>TTB000053719011(2)_900.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053719011(2)_900.png</td>\n",
       "      <td>E:\\Attachment\\report1\\TTB000053719011(2)_900.png</td>\n",
       "      <td>TTB000053719011</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>068Mg0000016hcnIAA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Item Created</td>\n",
       "      <td>15139979-1_60.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D:\\Python2\\Attachment\\Report(1)\\Report(1)\\1513...</td>\n",
       "      <td>D:\\Python2\\Attachment\\Report(1)\\Report(1)\\1513...</td>\n",
       "      <td>TTB000053359228</td>\n",
       "      <td>Access from current IP address is not allowed</td>\n",
       "      <td>Required fields are missing: [VersionData]</td>\n",
       "      <td>Access from current IP address is not allowed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>702 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                       Title  \\\n",
       "0    068Mg0000012T3lIAE  b275766a-c815-47d1-9c2b-c28c22f9e4c1_0.png   \n",
       "1    068Mg0000012T5NIAU  5cd8977b-66d6-4a03-a8a1-460483bc48fc_1.png   \n",
       "2    068Mg0000012T6zIAE  ec2cd765-7d6d-452a-beac-bcf7ceae2360_2.png   \n",
       "3    068Mg0000012T8bIAE  8fe5b013-f1ac-455c-83d0-288aa809ade7_3.png   \n",
       "4    068Mg0000012TADIA2  8c7142fd-2c3e-4d8b-b184-f49a12ce6373_4.png   \n",
       "..                  ...                                         ...   \n",
       "697  068Mg0000012WEaIAM                  TTB000053745871(2)_897.png   \n",
       "698  068Mg0000012TTbIAM            TTB000053719011 INVOICE_898.xlsx   \n",
       "699  068Mg0000012UEOIA2                  TTB000053719011(1)_899.png   \n",
       "700  068Mg0000012UrCIAU                  TTB000053719011(2)_900.png   \n",
       "701  068Mg0000016hcnIAA                                         NaN   \n",
       "\n",
       "     Description                                        VersionData  \\\n",
       "0            NaN  E:\\Attachment\\report1\\b275766a-c815-47d1-9c2b-...   \n",
       "1            NaN  E:\\Attachment\\report1\\5cd8977b-66d6-4a03-a8a1-...   \n",
       "2            NaN  E:\\Attachment\\report1\\ec2cd765-7d6d-452a-beac-...   \n",
       "3            NaN  E:\\Attachment\\report1\\8fe5b013-f1ac-455c-83d0-...   \n",
       "4            NaN  E:\\Attachment\\report1\\8c7142fd-2c3e-4d8b-b184-...   \n",
       "..           ...                                                ...   \n",
       "697          NaN   E:\\Attachment\\report1\\TTB000053745871(2)_897.png   \n",
       "698          NaN  E:\\Attachment\\report1\\TTB000053719011 INVOICE_...   \n",
       "699          NaN   E:\\Attachment\\report1\\TTB000053719011(1)_899.png   \n",
       "700          NaN   E:\\Attachment\\report1\\TTB000053719011(2)_900.png   \n",
       "701          NaN                                                NaN   \n",
       "\n",
       "                                          PathOnClient FirstPublishLocationId  \\\n",
       "0    E:\\Attachment\\report1\\b275766a-c815-47d1-9c2b-...        TTB000053312948   \n",
       "1    E:\\Attachment\\report1\\5cd8977b-66d6-4a03-a8a1-...        TTB000053312948   \n",
       "2    E:\\Attachment\\report1\\ec2cd765-7d6d-452a-beac-...        TTB000053312948   \n",
       "3    E:\\Attachment\\report1\\8fe5b013-f1ac-455c-83d0-...        TTB000053312948   \n",
       "4    E:\\Attachment\\report1\\8c7142fd-2c3e-4d8b-b184-...        TTB000053312948   \n",
       "..                                                 ...                    ...   \n",
       "697   E:\\Attachment\\report1\\TTB000053745871(2)_897.png        TTB000053745871   \n",
       "698  E:\\Attachment\\report1\\TTB000053719011 INVOICE_...        TTB000053719011   \n",
       "699   E:\\Attachment\\report1\\TTB000053719011(1)_899.png        TTB000053719011   \n",
       "700   E:\\Attachment\\report1\\TTB000053719011(2)_900.png        TTB000053719011   \n",
       "701                                                NaN                    NaN   \n",
       "\n",
       "           STATUS              TITLE  DESCRIPTION  \\\n",
       "0    Item Created                NaN          NaN   \n",
       "1    Item Created                NaN          NaN   \n",
       "2    Item Created                NaN          NaN   \n",
       "3    Item Created                NaN          NaN   \n",
       "4    Item Created                NaN          NaN   \n",
       "..            ...                ...          ...   \n",
       "697  Item Created                NaN          NaN   \n",
       "698  Item Created                NaN          NaN   \n",
       "699  Item Created                NaN          NaN   \n",
       "700  Item Created                NaN          NaN   \n",
       "701  Item Created  15139979-1_60.pdf          NaN   \n",
       "\n",
       "                                           VERSIONDATA  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "697                                                NaN   \n",
       "698                                                NaN   \n",
       "699                                                NaN   \n",
       "700                                                NaN   \n",
       "701  D:\\Python2\\Attachment\\Report(1)\\Report(1)\\1513...   \n",
       "\n",
       "                                          PATHONCLIENT FIRSTPUBLISHLOCATIONID  \\\n",
       "0                                                  NaN                    NaN   \n",
       "1                                                  NaN                    NaN   \n",
       "2                                                  NaN                    NaN   \n",
       "3                                                  NaN                    NaN   \n",
       "4                                                  NaN                    NaN   \n",
       "..                                                 ...                    ...   \n",
       "697                                                NaN                    NaN   \n",
       "698                                                NaN                    NaN   \n",
       "699                                                NaN                    NaN   \n",
       "700                                                NaN                    NaN   \n",
       "701  D:\\Python2\\Attachment\\Report(1)\\Report(1)\\1513...        TTB000053359228   \n",
       "\n",
       "                                             ERROR  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "..                                             ...   \n",
       "697                                            NaN   \n",
       "698                                            NaN   \n",
       "699                                            NaN   \n",
       "700                                            NaN   \n",
       "701  Access from current IP address is not allowed   \n",
       "\n",
       "                                        ERROR.1  \\\n",
       "0                                           NaN   \n",
       "1                                           NaN   \n",
       "2                                           NaN   \n",
       "3                                           NaN   \n",
       "4                                           NaN   \n",
       "..                                          ...   \n",
       "697                                         NaN   \n",
       "698                                         NaN   \n",
       "699                                         NaN   \n",
       "700                                         NaN   \n",
       "701  Required fields are missing: [VersionData]   \n",
       "\n",
       "                                            ERROR2  \n",
       "0                                              NaN  \n",
       "1                                              NaN  \n",
       "2                                              NaN  \n",
       "3                                              NaN  \n",
       "4                                              NaN  \n",
       "..                                             ...  \n",
       "697                                            NaN  \n",
       "698                                            NaN  \n",
       "699                                            NaN  \n",
       "700                                            NaN  \n",
       "701  Access from current IP address is not allowed  \n",
       "\n",
       "[702 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\attachment\\attachment_batch1\\report1and2\\combined_success_files2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove duplicate rows based on the 'Title' column\n",
    "df_unique = df.drop_duplicates(subset='Title')\n",
    "\n",
    "# Get the number of rows and columns in the resulting dataframe\n",
    "num_rows, num_columns = df_unique.shape\n",
    "\n",
    "num_rows, num_columns\n",
    "df_unique\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
